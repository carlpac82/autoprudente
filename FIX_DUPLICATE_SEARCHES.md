# üîß Corre√ß√£o: Duplica√ß√£o de Pesquisas no Auto-Save

## üêõ Problema Identificado

**Sintomas**:
- Mais de 100 vers√µes da mesma pesquisa (ex: Dezembro, Aeroporto de Faro)
- Para meses com v√°rias pesquisas (ex: Novembro), s√≥ aparece 1 vers√£o
- Visuais do hist√≥rico n√£o aparecem corretamente

**Causa**:
O endpoint `/api/automated-search/save` estava sempre fazendo `INSERT`, criando uma nova entrada cada vez que um pre√ßo era editado, em vez de atualizar a pesquisa existente do mesmo dia.

---

## ‚úÖ Solu√ß√£o Implementada

### 1. **UPSERT Logic**

O sistema agora verifica se j√° existe uma pesquisa para:
- Mesma **localiza√ß√£o** (location)
- Mesmo **tipo de pesquisa** (search_type: automated/current)
- Mesma **data de pickup** (pickup_date)
- Mesmo **dia** (DATE(search_date))

Se existir: **UPDATE** a pesquisa existente  
Se n√£o existir: **INSERT** nova pesquisa

### 2. **Implementa√ß√£o**

#### PostgreSQL (main.py, linha ~34210):
```python
# Check if search already exists
cur.execute("""
    SELECT id FROM automated_search_history
    WHERE location = %s 
      AND search_type = %s
      AND pickup_date = %s
      AND DATE(search_date) = %s
    ORDER BY search_date DESC
    LIMIT 1
""", (location, search_type, pickup_date, today))

existing = cur.fetchone()

if existing:
    # UPDATE existing search
    search_id = existing[0]
    cur.execute("""
        UPDATE automated_search_history
        SET prices_data = %s::jsonb,
            supplier_data = %s::jsonb,
            dias = %s,
            price_count = %s,
            search_date = CURRENT_TIMESTAMP
        WHERE id = %s
    """, (prices_json, supplier_data_json, dias_json, price_count, search_id))
    logging.info(f"[UPSERT] Updated existing search ID: {search_id}")
else:
    # INSERT new search
    cur.execute("""
        INSERT INTO automated_search_history 
        (location, search_type, month_key, prices_data, dias, price_count, user_email, supplier_data, pickup_date)
        VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s, %s::jsonb, %s)
        RETURNING id
    """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email, supplier_data_json, pickup_date))
    result = cur.fetchone()
    search_id = result[0] if result else 0
    logging.info(f"[UPSERT] Inserted new search ID: {search_id}")
```

#### SQLite (main.py, linha ~34350):
- Mesma l√≥gica adaptada para SQLite
- Fallbacks para schemas antigos (sem `pickup_date` ou `supplier_data`)

### 3. **Logs de Debug**

Novos logs para identificar a√ß√£o:
- `[UPSERT] Updated existing search ID: 123` - Pesquisa atualizada
- `[UPSERT] Inserted new search ID: 456` - Nova pesquisa criada
- `[FALLBACK-1 UPSERT]` / `[FALLBACK-2 UPSERT]` - Fallbacks com UPSERT
- `[SQLITE UPSERT]` - UPSERT em SQLite

---

## üßπ Limpeza de Duplicados Existentes

### Script: `cleanup_duplicate_searches.py`

**Funcionalidade**:
- Identifica grupos de pesquisas duplicadas (mesma location, pickup_date, search_type, dia)
- Mant√©m apenas a vers√£o mais recente de cada grupo
- Deleta todas as vers√µes antigas

**Como usar**:

1. **Definir DATABASE_URL**:
   ```bash
   export DATABASE_URL="postgresql://user:pass@host:5432/dbname"
   ```

2. **Executar script**:
   ```bash
   python cleanup_duplicate_searches.py
   ```

3. **Confirmar limpeza**:
   ```
   üîç Found 150 groups with duplicates:
     ‚Ä¢ Aeroporto de Faro | 2025-12-27 | automated | 2025-11-20: 102 versions
     ‚Ä¢ Albufeira | 2025-11-15 | automated | 2025-11-20: 5 versions
     ...
   
   ‚ö†Ô∏è  Total entries to delete: 300
   
   ‚ùì Proceed with cleanup? (yes/no): yes
   
   ‚úÖ Cleanup complete!
      ‚Ä¢ Before: 500 entries
      ‚Ä¢ After: 200 entries
      ‚Ä¢ Deleted: 300 duplicates
   ```

---

## üìä Resultados Esperados

### Antes da Corre√ß√£o:
```
2025-12: Total=199 versions (1 auto, 198 current)  ‚ùå 198 duplicadas!
2025-11: Total=1 versions (1 auto, 0 current)      ‚ùå S√≥ 1 quando deviam ser v√°rias
```

### Depois da Corre√ß√£o:
```
2025-12: Total=1 version (1 auto, 0 current)       ‚úÖ Uma √∫nica vers√£o atualizada
2025-11: Total=5 versions (5 auto, 0 current)      ‚úÖ Vers√µes √∫nicas por pesquisa
```

### Comportamento:
1. ‚úÖ **Primeira pesquisa do dia**: Cria nova entrada (INSERT)
2. ‚úÖ **Editar pre√ßos**: Atualiza entrada existente (UPDATE)
3. ‚úÖ **Nova pesquisa (dia diferente)**: Cria nova entrada (INSERT)
4. ‚úÖ **Hist√≥rico visual**: Mostra apenas vers√µes √∫nicas

---

## üîç Logs do Render

Ap√≥s deploy, ao editar pre√ßos, procurar:

### Logs de Sucesso (UPDATE):
```
[UPSERT] Updated existing search ID: 123
‚úÖ Automated search saved: ID=123, Type=automated, Prices=56, Month=2025-12
```

### Logs de Sucesso (INSERT - Nova Pesquisa):
```
[UPSERT] Inserted new search ID: 456
‚úÖ Automated search saved: ID=456, Type=automated, Prices=56, Month=2025-12
```

### Logs de Fallback:
```
[FALLBACK-1 UPSERT] Updated ID: 123
[FALLBACK-2 UPSERT] Inserted ID: 456
[SQLITE UPSERT] Updated ID: 123
```

---

## üß™ Como Testar

### 1. **Fazer Nova Pesquisa**:
   - Ir para "Automated Prices"
   - Selecionar: Aeroporto de Faro, Dezembro 2025, 31 dias
   - Fazer pesquisa
   - **Verificar logs**: `[UPSERT] Inserted new search ID: X`

### 2. **Editar Pre√ßos**:
   - Editar um pre√ßo em qualquer card (ex: M1 / 31 dias)
   - Aguardar auto-save (~2 segundos)
   - **Verificar logs**: `[UPSERT] Updated existing search ID: X` (mesmo ID)

### 3. **Verificar Hist√≥rico**:
   - Ir para aba "History"
   - Filtrar por "Aeroporto de Faro"
   - **Verificar**: Deve aparecer apenas 1 vers√£o para Dezembro
   - **Verificar**: Card deve mostrar o pre√ßo editado

### 4. **Fazer Pesquisa para Outro M√™s**:
   - Fazer nova pesquisa para Janeiro 2026
   - **Verificar logs**: `[UPSERT] Inserted new search ID: Y` (ID diferente)
   - **Verificar hist√≥rico**: Deve ter 2 vers√µes (Dezembro + Janeiro)

---

## üöÄ Deploy e A√ß√µes

### Deploy Status:
- ‚úÖ **Commit**: `02573ce`
- ‚úÖ **Push**: Conclu√≠do
- üîÑ **Render Deploy**: Aguardar ~5 minutos

### A√ß√µes Imediatas:

1. ‚úÖ **Testar auto-save** ap√≥s deploy
2. ‚ö†Ô∏è  **Executar cleanup script** (remover duplicados)
3. ‚úÖ **Verificar hist√≥rico visual** est√° correto

### A√ß√µes Opcionais:

- Executar `cleanup_duplicate_searches.py` no Render (via shell)
- Verificar tabela `automated_search_history` via SQL:
  ```sql
  SELECT location, pickup_date, search_type, DATE(search_date), COUNT(*)
  FROM automated_search_history
  GROUP BY location, pickup_date, search_type, DATE(search_date)
  HAVING COUNT(*) > 1;
  ```

---

## üìù Notas T√©cnicas

### Crit√©rios de Identifica√ß√£o de Duplicados:
- **location**: Aeroporto de Faro, Albufeira, etc.
- **search_type**: automated ou current
- **pickup_date**: Data da pesquisa (ex: 2025-12-27)
- **DATE(search_date)**: Dia em que foi salva (ex: 2025-11-20)

### Campos Atualizados no UPDATE:
- `prices_data` (JSONB com pre√ßos de todos os grupos)
- `supplier_data` (JSONB com dados dos suppliers)
- `dias` (JSON array com dias selecionados)
- `price_count` (n√∫mero total de pre√ßos)
- `search_date` (timestamp atualizado)

### Campos N√ÉO Atualizados:
- `id` (mant√©m o mesmo)
- `location` (n√£o muda)
- `search_type` (n√£o muda)
- `month_key` (n√£o muda)
- `user_email` (n√£o muda)
- `pickup_date` (n√£o muda)

---

## ‚ö†Ô∏è Outros Erros a Corrigir

### 1. **AI Adjustment**: "Missing required fields" (400)
```
[POST] 400 /api/ai/save-adjustment
```
**Status**: Pendente an√°lise dos logs do backend

### 2. **Export History**: "column period_start does not exist"
```
Failed to save export to history: 
"column \"period_start\" of relation \"export_history\" does not exist"
```
**Solu√ß√£o**:
```sql
ALTER TABLE export_history ADD COLUMN IF NOT EXISTS period_start DATE;
ALTER TABLE export_history ADD COLUMN IF NOT EXISTS period_end DATE;
```

---

## üéØ Resumo

| Problema | Causa | Solu√ß√£o |
|----------|-------|---------|
| 100+ vers√µes duplicadas | INSERT sempre | UPSERT (UPDATE se existe) |
| Hist√≥rico visual errado | Muitas vers√µes | Apenas 1 vers√£o por pesquisa/dia |
| Novembro s√≥ 1 vers√£o | Bug visual? | Corrigido com UPSERT |

---

**Status Final**: ‚úÖ Corre√ß√£o implementada e deployed  
**Deploy**: Commit `02573ce`  
**Data**: 2025-11-20 13:00 UTC  
**Pr√≥ximos Passos**: Testar e executar cleanup script
