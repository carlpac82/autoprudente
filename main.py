from __future__ import annotations

def _no_store_json(payload: Dict[str, Any], status_code: int = 200) -> JSONResponse:
    try:
        return JSONResponse(
            payload,
            status_code=status_code,
            headers={
                "Cache-Control": "no-store, no-cache, must-revalidate, max-age=0",
                "Pragma": "no-cache",
                "Expires": "0",
            },
        )
    except Exception:
        return JSONResponse(payload, status_code=status_code)
def render_with_playwright(url: str) -> str:
    if not _HAS_PLAYWRIGHT:
        return ""

def _is_carjet(u: str) -> bool:
    try:
        from urllib.parse import urlparse as _parse
        return _parse(u).netloc.endswith("carjet.com")
    except Exception:
        return False

def _ensure_settings_table():
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute(
                    "CREATE TABLE IF NOT EXISTS app_settings (key TEXT PRIMARY KEY, value TEXT)"
                )
                con.commit()
            finally:
                con.close()
    except Exception:
        pass

def _get_setting(key: str, default: str = "") -> str:
    try:
        _ensure_settings_table()
        with _db_lock:
            con = _db_connect()
            try:
                cur = con.execute("SELECT value FROM app_settings WHERE key=?", (key,))
                r = cur.fetchone()
                return (r[0] if r and r[0] is not None else default)
            finally:
                con.close()
    except Exception:
        return default

def _set_setting(key: str, value: str) -> None:
    try:
        _ensure_settings_table()
        with _db_lock:
            con = _db_connect()
            try:
                con.execute(
                    "INSERT INTO app_settings (key, value) VALUES (?,?) ON CONFLICT(key) DO UPDATE SET value=excluded.value",
                    (key, value),
                )
                con.commit()
            finally:
                con.close()
    except Exception:
        pass

def _get_carjet_adjustment() -> Tuple[float, float]:
    """
    CRITICAL: Carjet Adjustment must ALWAYS be 0% and 0‚Ç¨
    User requirement: Never apply price adjustments to Carjet
    """
    return 0.0, 0.0

def _get_abbycar_adjustment() -> float:
    """
    Get Abbycar Excel export price adjustment percentage
    Returns: Percentage adjustment (e.g., 5.0 for +5%, -3.0 for -3%)
    Default: 3.0%
    """
    try:
        val = _get_setting("abbycar_pct")
        return float(val) if val else 3.0
    except Exception:
        return 3.0

def _get_abbycar_low_deposit_enabled() -> bool:
    """
    Check if Low Deposit adjustment is enabled
    Returns: True if enabled, False otherwise
    Default: False
    """
    try:
        val = _get_setting("abbycar_low_deposit_enabled")
        return val == "1" or val == "true" or val == True
    except Exception:
        return False

def _get_abbycar_low_deposit_adjustment() -> float:
    """
    Get Low Deposit groups additional adjustment percentage
    Returns: Additional percentage adjustment for Low Deposit groups
    Default: 0.0%
    """
    try:
        val = _get_setting("abbycar_low_deposit_pct")
        return float(val) if val else 0.0
    except Exception:
        return 0.0

def apply_price_adjustments(items: List[Dict[str, Any]], base_url: str) -> List[Dict[str, Any]]:
    try:
        if not items:
            return items
        if not _is_carjet(base_url):
            return items
        pct, off = _get_carjet_adjustment()
        if pct == 0 and off == 0:
            return items
        out: List[Dict[str, Any]] = []
        for it in items:
            ptxt = str(it.get("price") or "")
            amt = _parse_amount(ptxt)
            if amt is None:
                out.append(it)
                continue
            adj = amt * (1.0 + (pct/100.0)) + off
            it2 = dict(it)
            it2.setdefault("original_price", ptxt)
            it2["price"] = _format_eur(adj)
            it2["currency"] = "EUR"
            out.append(it2)
        return out
    except Exception:
        return items

def scrape_with_playwright(url: str) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    if not _HAS_PLAYWRIGHT:
        return items
    try:
        with sync_playwright() as pw:
            browser = pw.chromium.launch(headless=True)
            context = browser.new_context(locale="pt-PT", user_agent="Mozilla/5.0 (compatible; PriceTracker/1.0)")
            try:
                context.add_cookies([
                    {"name": "monedaForzada", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                    {"name": "moneda", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                    {"name": "currency", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                    {"name": "country", "value": "PT", "domain": ".carjet.com", "path": "/"},
                    {"name": "idioma", "value": "PT", "domain": ".carjet.com", "path": "/"},
                    {"name": "lang", "value": "pt", "domain": ".carjet.com", "path": "/"},
                ])
            except Exception:
                pass
            page = context.new_page()
            try:
                page.set_extra_http_headers({"Accept-Language": "pt-PT,pt;q=0.9,en;q=0.8"})
            except Exception:
                pass
            page.goto(url, wait_until="networkidle", timeout=35000)
            
            # ===== FILTRAR APENAS AUTOPRUDENTE =====
            try:
                # Aguardar filtros carregarem
                page.wait_for_selector('#chkAUP', timeout=5000)
                print("[PLAYWRIGHT] Checkbox AUTOPRUDENTE encontrado", file=sys.stderr, flush=True)
                
                # IMPORTANTE: Aceitar cookies primeiro se aparecer
                try:
                    page.evaluate("""
                        // Procurar e clicar no bot√£o de cookies
                        const buttons = document.querySelectorAll('button');
                        for (let btn of buttons) {
                            const text = btn.textContent.toLowerCase().trim();
                            if (text.includes('aceitar todos') || text.includes('aceitar tudo')) {
                                btn.click();
                                console.log('‚úì Cookies aceitos:', btn.textContent);
                                break;
                            }
                        }
                        // Remover banners de cookies
                        document.querySelectorAll('[id*=cookie], [class*=cookie], [id*=didomi], [class*=didomi]').forEach(el => {
                            el.remove();
                        });
                        document.body.style.overflow = 'auto';
                    """)
                    print("[PLAYWRIGHT] Cookies aceites via JavaScript", file=sys.stderr, flush=True)
                    page.wait_for_timeout(1000)
                except Exception as e:
                    print(f"[PLAYWRIGHT] Erro ao aceitar cookies: {e}", file=sys.stderr, flush=True)
                    pass
                
                # Desmarcar todos os checkboxes de suppliers primeiro
                print("[PLAYWRIGHT] Desmarcando todos os suppliers...", file=sys.stderr, flush=True)
                page.evaluate("""
                    const checkboxes = document.querySelectorAll('input[name="frmPrv"]:checked');
                    checkboxes.forEach(cb => cb.click());
                """)
                
                # Aguardar um pouco
                page.wait_for_timeout(1000)
                
                # AUTODETECTAR COOKIES ap√≥s desmarcar
                try:
                    page.evaluate("""
                        const buttons = document.querySelectorAll('button');
                        for (let btn of buttons) {
                            const text = btn.textContent.toLowerCase().trim();
                            if (text.includes('aceitar todos') || text.includes('aceitar tudo')) {
                                btn.click();
                                console.log('‚úì Cookies aceitos ap√≥s desmarcar');
                                break;
                            }
                        }
                    """)
                except:
                    pass
                
                # Marcar apenas AUTOPRUDENTE
                print("[PLAYWRIGHT] Marcando apenas AUTOPRUDENTE...", file=sys.stderr, flush=True)
                page.evaluate("""
                    const aupCheckbox = document.querySelector('#chkAUP');
                    if (aupCheckbox && !aupCheckbox.checked) {
                        aupCheckbox.click();
                    }
                """)
                
                print("[PLAYWRIGHT] Filtro AUTOPRUDENTE ativado", file=sys.stderr, flush=True)
                
                # Aguardar p√°gina recarregar com filtro
                page.wait_for_load_state("networkidle", timeout=15000)
                page.wait_for_timeout(2000)
                
                # AUTODETECTAR COOKIES ap√≥s marcar AUTOPRUDENTE
                try:
                    page.evaluate("""
                        const buttons = document.querySelectorAll('button');
                        for (let btn of buttons) {
                            const text = btn.textContent.toLowerCase().trim();
                            if (text.includes('aceitar todos') || text.includes('aceitar tudo')) {
                                btn.click();
                                console.log('‚úì Cookies aceitos ap√≥s AUTOPRUDENTE');
                                break;
                            }
                        }
                    """)
                except:
                    pass
                    
            except Exception as e:
                print(f"[PLAYWRIGHT] Erro ao filtrar AUTOPRUDENTE: {e}", file=sys.stderr, flush=True)
                # Continuar mesmo se falhar o filtro
                pass
            # ===== FIM FILTRO AUTOPRUDENTE =====
            
            # Try clicking the primary search/submit button if the page requires it to load results
            try:
                btn = None
                # Prefer role-based lookup, then fall back to text and CSS selectors
                try:
                    btn = page.get_by_role("button", name=re.compile(r"(Pesquisar|Buscar|Search)", re.I))
                except Exception:
                    btn = None
                if btn and btn.is_visible():
                    btn.click(timeout=3000)
                else:
                    cand = page.locator("button:has-text('Pesquisar'), button:has-text('Buscar'), button:has-text('Search'), input[type=submit], button[type=submit]")
                    if cand and (cand.count() or 0) > 0:
                        try:
                            cand.first.click(timeout=3000)
                        except Exception:
                            pass
                # After clicking, wait for network to settle and results to appear
                try:
                    page.wait_for_load_state("networkidle", timeout=10000)
                except Exception:
                    pass
            except Exception:
                pass
            # Incremental scroll to trigger lazy loading
            try:
                for _ in range(5):
                    try:
                        page.mouse.wheel(0, 2000)
                    except Exception:
                        pass
                    page.wait_for_timeout(400)
            except Exception:
                pass
            try:
                page.wait_for_selector("section.newcarlist article, .newcarlist article, article.car, li.result, li.car, .car-item, .result-row", timeout=30000)
            except Exception:
                pass

            # Query all cards - SELETORES ESPEC√çFICOS CARJET
            handles = page.query_selector_all("section.newcarlist article")
            idx = 0
            print(f"[PLAYWRIGHT] Encontrados {len(handles)} artigos", file=sys.stderr, flush=True)
            
            for h in handles:
                try:
                    card_text = (h.inner_text() or "").strip()
                except Exception:
                    card_text = ""
                
                # === PRE√áO TOTAL (CARJET ESPEC√çFICO) ===
                price_text = ""
                try:
                    # Prioridade 1: .pr-euros (pre√ßo em euros - TESTADO E FUNCIONA)
                    price_el = h.query_selector(".pr-euros")
                    if price_el:
                        price_text = (price_el.inner_text() or "").strip()
                    
                    # Prioridade 2: .price.pr-euros (alternativa)
                    if not price_text:
                        price_el = h.query_selector(".price.pr-euros")
                        if price_el:
                            price_text = (price_el.inner_text() or "").strip()
                    
                    # Prioridade 3: Procurar "Pre√ßo por X dias: XX,XX ‚Ç¨" no texto
                    if not price_text:
                        # Procurar padr√£o: "Pre√ßo por 5 dias:\n8,80 ‚Ç¨"
                        m = re.search(r"pre√ßo\s*por\s*\d+\s*dias:\s*([0-9]+[,\.][0-9]{2})\s*‚Ç¨", card_text, re.I)
                        if m:
                            price_text = m.group(1) + " ‚Ç¨"
                    
                    # Limpar pre√ßo
                    if price_text:
                        # Remover tudo exceto n√∫meros, v√≠rgula, ponto e ‚Ç¨
                        price_text = re.sub(r'[^\d,\.‚Ç¨\s]', '', price_text).strip()
                        if '‚Ç¨' not in price_text:
                            price_text += ' ‚Ç¨'
                        
                except Exception as e:
                    print(f"[PLAYWRIGHT] Erro ao extrair pre√ßo: {e}", file=sys.stderr, flush=True)
                    price_text = ""
                
                # === NOME DO CARRO (CARJET ESPEC√çFICO) ===
                car = ""
                try:
                    # Prioridade 1: h2 (TESTADO E FUNCIONA)
                    name_el = h.query_selector("h2")
                    if name_el:
                        car = (name_el.inner_text() or "").strip()
                    
                    # Fallback: outros seletores
                    if not car:
                        name_el = h.query_selector(".titleCar, .veh-name, .vehicle-name, .model, .title, h3")
                        if name_el:
                            car = (name_el.inner_text() or "").strip()
                except Exception:
                    pass
                
                # === SUPPLIER (CARJET ESPEC√çFICO) ===
                supplier = ""
                try:
                    # Prioridade 1: Logo do supplier
                    im = h.query_selector("img[src*='/prv/'], img[src*='logo_']")
                    if im:
                        src = im.get_attribute("src") or ""
                        # Extrair c√≥digo do supplier da URL: /logo_AUP.png ‚Üí AUP
                        match = re.search(r'logo_([A-Z0-9]+)', src)
                        if match:
                            supplier = match.group(1)
                        else:
                            supplier = (im.get_attribute("alt") or "").strip()
                    
                    # Fallback: texto do supplier
                    if not supplier:
                        sup_el = h.query_selector(".supplier, .vendor, .partner, [class*='supplier']")
                        supplier = (sup_el.inner_text() or "").strip() if sup_el else ""
                except Exception:
                    pass
                
                # === CATEGORIA/GRUPO (CARJET ESPEC√çFICO) ===
                category = ""
                try:
                    # Prioridade 1: .category
                    cat_el = h.query_selector(".category, .grupo, [class*='category'], [class*='grupo']")
                    if cat_el:
                        category = (cat_el.inner_text() or "").strip()
                    
                    # Prioridade 2: Extrair do texto (ex: "Grupo B1")
                    if not category:
                        match = re.search(r'grupo\s+([A-Z][0-9]?)', card_text, re.I)
                        if match:
                            category = match.group(1)
                except Exception:
                    pass
                # link
                link = ""
                try:
                    a = h.query_selector("a[href]")
                    if a:
                        href = a.get_attribute("href") or ""
                        if href and not href.lower().startswith("javascript"):
                            from urllib.parse import urljoin as _urljoin
                            link = _urljoin(url, href)
                except Exception:
                    pass
                # Only add if we have a price
                if price_text:
                    # Mapear categoria para c√≥digo de grupo
                    group_code = map_category_to_group(category, car)
                    
                    # Log detalhado
                    print(f"[PLAYWRIGHT] Carro #{idx+1}:", file=sys.stderr, flush=True)
                    print(f"  Nome: {car}", file=sys.stderr, flush=True)
                    print(f"  Supplier: {supplier}", file=sys.stderr, flush=True)
                    print(f"  Pre√ßo: {price_text}", file=sys.stderr, flush=True)
                    print(f"  Categoria: {category}", file=sys.stderr, flush=True)
                    print(f"  Grupo: {group_code}", file=sys.stderr, flush=True)
                    
                    items.append({
                        "id": idx,
                        "car": car,
                        "supplier": supplier,
                        "price": price_text,
                        "currency": "",
                        "category": category,
                        "group": group_code,
                        "transmission": "",
                        "photo": "",
                        "link": link or url,
                    })
                    idx += 1
                else:
                    print(f"[PLAYWRIGHT] ‚ö†Ô∏è  Carro sem pre√ßo ignorado: {car}", file=sys.stderr, flush=True)
            # If no items collected via card scanning, try parsing the full rendered HTML
            try:
                if not items:
                    html_full = page.content()
                    try:
                        # Best-effort: save debug HTML for inspection
                        stamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
                        (DEBUG_DIR / f"playwright-capture-{stamp}.html").write_text(html_full or "", encoding="utf-8")
                    except Exception:
                        pass
                    try:
                        items2 = parse_prices(html_full, url)
                        if items2:
                            items = items2
                    except Exception:
                        pass
            except Exception:
                pass
            context.close()
            browser.close()
    except Exception:
        return items
    return items

import os
import sys
import secrets
import re
from urllib.parse import urljoin
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta
import traceback as _tb
import logging
import json
import base64

from fastapi import FastAPI, Request, Form, Depends, HTTPException, UploadFile, File
from fastapi.responses import RedirectResponse, JSONResponse, HTMLResponse, Response, StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path

# ============================================================
# MONITORING & ERROR TRACKING (Sentry)
# ============================================================
try:
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    SENTRY_DSN = os.getenv("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="url"),
                FastApiIntegration(transaction_style="url"),
            ],
            traces_sample_rate=0.1,  # 10% das transa√ß√µes
            profiles_sample_rate=0.1,  # 10% dos profiles
            environment=os.getenv("ENVIRONMENT", "production"),
            release=os.getenv("RENDER_GIT_COMMIT", "unknown"),
        )
        logging.info("‚úÖ Sentry monitoring enabled")
    else:
        logging.info("‚ÑπÔ∏è  Sentry DSN not configured - monitoring disabled")
except ImportError:
    logging.warning("‚ö†Ô∏è  Sentry SDK not installed - monitoring disabled")
except Exception as e:
    logging.error(f"‚ùå Failed to initialize Sentry: {e}")
from urllib.parse import urlencode, quote_plus
from fastapi.templating import Jinja2Templates
from starlette.middleware.sessions import SessionMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from starlette.status import HTTP_303_SEE_OTHER
from dotenv import load_dotenv

# ‚ö†Ô∏è CRITICAL: Load environment variables FIRST before importing database module
load_dotenv()

import requests
import asyncio
from bs4 import BeautifulSoup
import sqlite3
from threading import Lock
import random

# Import database module for PostgreSQL/SQLite hybrid support
try:
    from database import _db_connect as _db_connect_new, USE_POSTGRES, PostgreSQLConnectionWrapper as DBPostgreSQLWrapper
    _USE_NEW_DB = True
    # Usar a classe do database.py em vez da local
    PostgreSQLConnectionWrapper = DBPostgreSQLWrapper
    if USE_POSTGRES:
        logging.info("üêò PostgreSQL mode enabled")
    else:
        logging.info("üìÅ SQLite mode (local development)")
except ImportError:
    _USE_NEW_DB = False
    logging.info("üìÅ Using legacy SQLite connection")
import time
import io
import hashlib
import smtplib
from email.message import EmailMessage
from fastapi import Query
try:
    import httpx  # type: ignore
    _HTTPX_CLIENT = httpx.Client(timeout=httpx.Timeout(10.0, connect=4.0), headers={"Connection": "keep-alive"})
    _HTTPX_ASYNC: Optional["httpx.AsyncClient"] = httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=4.0), headers={"Connection": "keep-alive"})
except Exception:
    _HTTPX_CLIENT = None
    _HTTPX_ASYNC = None

# Import VEHICLES dictionary from carjet_direct
try:
    from carjet_direct import VEHICLES
except ImportError:
    logging.warning("‚ö†Ô∏è  Could not import VEHICLES from carjet_direct")
    VEHICLES = {}

# Import match helper
try:
    from match_helper import match_vehicle_group_by_characteristics
except ImportError:
    logging.warning("‚ö†Ô∏è  Could not import match_helper")
    match_vehicle_group_by_characteristics = None

# Cache global para ve√≠culos do Admin
_admin_vehicles_cache = None
_admin_vehicles_cache_time = 0
ADMIN_VEHICLES_CACHE_TTL = 300  # 5 minutos

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:     %(message)s'
)

try:
    from playwright.sync_api import sync_playwright  # type: ignore
    _HAS_PLAYWRIGHT = True
except Exception:
    _HAS_PLAYWRIGHT = False

# Environment variables
USE_PLAYWRIGHT = str(os.getenv("USE_PLAYWRIGHT", "")).strip().lower() in ("1","true","yes","on")
_test_mode_val = os.getenv("TEST_MODE_LOCAL", "0").strip()
TEST_MODE_LOCAL = int(_test_mode_val) if _test_mode_val.isdigit() else (1 if _test_mode_val.lower() in ("true", "yes") else 0)
TEST_FARO_URL = os.getenv("TEST_FARO_URL", "")
TEST_ALBUFEIRA_URL = os.getenv("TEST_ALBUFEIRA_URL", "")
APP_PASSWORD = os.getenv("APP_PASSWORD", "change_me")
SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(32))
TARGET_URL = os.getenv("TARGET_URL", "https://example.com")
SCRAPER_SERVICE = os.getenv("SCRAPER_SERVICE", "")
SCRAPER_API_KEY = os.getenv("SCRAPER_API_KEY", "")
SCRAPER_COUNTRY = os.getenv("SCRAPER_COUNTRY", "").strip()
APP_USERNAME = os.getenv("APP_USERNAME", "user")
CARJET_PRICE_ADJUSTMENT_PCT = float(os.getenv("CARJET_PRICE_ADJUSTMENT_PCT", "0") or 0)
CARJET_PRICE_OFFSET_EUR = float(os.getenv("CARJET_PRICE_OFFSET_EUR", "0") or 0)
AUDIT_RETENTION_DAYS = int(os.getenv("AUDIT_RETENTION_DAYS", "90") or 90)
IMAGE_CACHE_DAYS = int(os.getenv("IMAGE_CACHE_DAYS", "365") or 365)
PRICES_CACHE_TTL_SECONDS = int(os.getenv("PRICES_CACHE_TTL_SECONDS", "300") or 300)
BULK_CONCURRENCY = int(os.getenv("BULK_CONCURRENCY", "6") or 6)
BULK_MAX_RETRIES = int(os.getenv("BULK_MAX_RETRIES", "2") or 2)
GLOBAL_FETCH_RPS = float(os.getenv("GLOBAL_FETCH_RPS", "5") or 5.0)

# --- Precompiled regexes for parser performance ---
AUTO_RX = re.compile(r"\b(auto|automatic|automatico|autom√°tico|automatik|aut\.|a/t|at|dsg|cvt|bva|tiptronic|steptronic|s\s*tronic|multidrive|multitronic|eat|eat6|eat8)\b", re.I)
BG_IMAGE_RX = re.compile(r"background-image\s*:\s*url\(([^)]+)\)", re.I)
LOGO_CODE_RX = re.compile(r"/logo_([A-Za-z0-9]+)\.", re.I)
CAR_CODE_RX = re.compile(r"car_([A-Za-z0-9]+)\.jpg", re.I)
OBJ_RX = re.compile(r"\{[^{}]*\"priceStr\"\s*:\s*\"[^\"]+\"[^{}]*\"id\"\s*:\s*\"[^\"]+\"[^{}]*\}", re.S)
DATAMAP_RX = re.compile(r"var\s+dataMap\s*=\s*(\[.*?\]);", re.S)

# Aumentar limite de payload para permitir dados completos (284 carros √ó ~2KB = ~568KB)
app = FastAPI(
    title="Rental Price Tracker",
    # Aumentar limite de upload para 10MB (dados completos de pesquisas)
    # Default √© 1MB - insuficiente para 284 carros completos
)
app.add_middleware(SessionMiddleware, secret_key=SECRET_KEY, same_site="lax")
app.add_middleware(GZipMiddleware, minimum_size=500)

def _ensure_damage_reports_tables():
    """Criar tabelas de Damage Reports no PostgreSQL"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if not is_postgres:
                    print("   ‚ö†Ô∏è  SQLite detected, skipping DR tables", flush=True)
                    return
                
                print("   üîß Creating PostgreSQL DR tables...", flush=True)
                with conn.cursor() as cur:
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_reports (
                            id SERIAL PRIMARY KEY,
                            dr_number TEXT UNIQUE,
                            ra_number TEXT,
                            contract_number TEXT,
                            date DATE,
                            client_name TEXT,
                            client_email TEXT,
                            client_phone TEXT,
                            client_address TEXT,
                            client_city TEXT,
                            client_postal_code TEXT,
                            client_country TEXT,
                            vehicle_plate TEXT,
                            vehicle_model TEXT,
                            vehicle_brand TEXT,
                            pickup_date TIMESTAMP,
                            pickup_time TEXT,
                            pickup_location TEXT,
                            return_date TIMESTAMP,
                            return_time TEXT,
                            return_location TEXT,
                            issued_by TEXT,
                            inspection_type TEXT,
                            inspector_name TEXT,
                            mileage INTEGER,
                            fuel_level TEXT,
                            damage_description TEXT,
                            observations TEXT,
                            damage_diagram_data TEXT,
                            repair_items TEXT,
                            damage_images TEXT,
                            total_amount REAL,
                            status TEXT DEFAULT 'draft',
                            pdf_data BYTEA,
                            pdf_filename TEXT,
                            is_protected INTEGER DEFAULT 0,
                            is_deleted INTEGER DEFAULT 0,
                            deleted_at TIMESTAMP,
                            deleted_by TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            created_by TEXT,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_coordinates (
                            id SERIAL PRIMARY KEY,
                            field_id TEXT NOT NULL,
                            x REAL NOT NULL,
                            y REAL NOT NULL,
                            width REAL NOT NULL,
                            height REAL NOT NULL,
                            page INTEGER DEFAULT 1,
                            field_type TEXT,
                            template_version INTEGER DEFAULT 1,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_mapping_history (
                            id SERIAL PRIMARY KEY,
                            template_version INTEGER NOT NULL,
                            field_id TEXT NOT NULL,
                            x REAL NOT NULL,
                            y REAL NOT NULL,
                            width REAL NOT NULL,
                            height REAL NOT NULL,
                            page INTEGER DEFAULT 1,
                            field_type TEXT,
                            mapped_by TEXT,
                            mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_templates (
                            id SERIAL PRIMARY KEY,
                            version INTEGER NOT NULL,
                            filename TEXT NOT NULL,
                            file_data BYTEA NOT NULL,
                            num_pages INTEGER,
                            uploaded_by TEXT,
                            uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            is_active INTEGER DEFAULT 0,
                            notes TEXT
                        )
                    """)
                    
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_numbering (
                            id INTEGER PRIMARY KEY,
                            current_year INTEGER NOT NULL,
                            current_number INTEGER NOT NULL,
                            prefix TEXT DEFAULT 'DR',
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # Inserir valores iniciais APENAS se n√£o existir nenhum registro
                    cur.execute("""
                        INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                        SELECT 1, 2025, 1, 'DR'
                        WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                    """)
                    
                    # Tabela de templates de e-mail (multi-idioma)
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS dr_email_templates (
                            id SERIAL PRIMARY KEY,
                            language_code TEXT NOT NULL UNIQUE,
                            language_name TEXT NOT NULL,
                            subject_template TEXT NOT NULL,
                            body_template TEXT NOT NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # Inserir templates padr√£o (PT, EN, FR, DE)
                    cur.execute("""
                        INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template)
                        SELECT 'pt', 'Portugu√™s',
                            'Relat√≥rio de Danos {drNumber} - {vehiclePlate}',
                            E'<!DOCTYPE html><html><head><meta charset="UTF-8"><style>body{font-family:Arial,sans-serif;margin:0;padding:0;background-color:#f5f5f5}.email-container{max-width:600px;margin:0 auto;background-color:#fff}.header{background-color:#009cb6;padding:20px;text-align:left}.header-content{display:flex;justify-content:space-between;align-items:center}.logo img{height:50px}.header-info{text-align:right;color:#fff;font-size:14px;font-weight:bold}.content{padding:30px 20px;color:#333;line-height:1.6}.footer{background-color:#f0f0f0;padding:15px 20px;text-align:center;font-size:12px;color:#666}</style></head><body><div class="email-container"><div class="header"><div class="header-content"><div class="logo"><img src="https://carrental-api-5f8q.onrender.com/static/ap-heather.png" alt="Auto Prudente" style="height:50px"/></div><div class="header-info"><div>DR: {drNumber}</div><div>RA: {raNumber}</div></div></div></div><div class="content"><p>Ol√° {firstName},</p><p>Thank you for choosing Auto Prudente Rent a Car!</p><p>Due to unforeseen circumstances that occurred during the above-mentioned rental contract, the vehicle you rented has been damaged.</p><p>Your rental contract reflects that you declined our premium insurance, choosing to accept full responsibility for any loss or damage that occurs, regardless of who was at fault.</p></div></div></body></html>'
                        WHERE NOT EXISTS (SELECT 1 FROM dr_email_templates WHERE language_code = 'pt')
                    """)
                    
                    cur.execute("""
                        INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template)
                        SELECT 'en', 'English',
                            'Damage Report {drNumber} - {vehiclePlate}',
                            E'Hello {firstName},\n\nPlease find attached the Damage Report n¬∫ {drNumber} for contract {raNumber}.\n\n**Details:**\n- Vehicle: {vehiclePlate}\n- Date: {date}\n\nBest regards,\nAuto Prudente'
                        WHERE NOT EXISTS (SELECT 1 FROM dr_email_templates WHERE language_code = 'en')
                    """)
                    
                    cur.execute("""
                        INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template)
                        SELECT 'fr', 'Fran√ßais',
                            'Rapport de Dommages {drNumber} - {vehiclePlate}',
                            E'Bonjour {firstName},\n\nVeuillez trouver ci-joint le Rapport de Dommages n¬∫ {drNumber} pour le contrat {raNumber}.\n\n**D√©tails:**\n- V√©hicule: {vehiclePlate}\n- Date: {date}\n\nCordialement,\nAuto Prudente'
                        WHERE NOT EXISTS (SELECT 1 FROM dr_email_templates WHERE language_code = 'fr')
                    """)
                    
                    cur.execute("""
                        INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template)
                        SELECT 'de', 'Deutsch',
                            'Schadensbericht {drNumber} - {vehiclePlate}',
                            E'Hallo {firstName},\n\nAnbei finden Sie den Schadensbericht Nr. {drNumber} f√ºr Vertrag {raNumber}.\n\n**Details:**\n- Fahrzeug: {vehiclePlate}\n- Datum: {date}\n\nMit freundlichen Gr√º√üen,\nAuto Prudente'
                        WHERE NOT EXISTS (SELECT 1 FROM dr_email_templates WHERE language_code = 'de')
                    """)
                    
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_dr_number ON damage_reports(dr_number)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_dr_created_at ON damage_reports(created_at DESC)")
                    
                    print("   ‚úÖ damage_reports", flush=True)
                    print("   ‚úÖ damage_report_coordinates", flush=True)
                    print("   ‚úÖ damage_report_mapping_history", flush=True)
                    print("   ‚úÖ damage_report_templates", flush=True)
                    print("   ‚úÖ damage_report_numbering", flush=True)
                    print("   ‚úÖ dr_email_templates (PT, EN, FR, DE)", flush=True)
                    
                conn.commit()
            finally:
                conn.close()
    except Exception as e:
        print(f"   ‚ùå Error creating DR tables: {e}", flush=True)
        import traceback
        traceback.print_exc()

def _ensure_rental_agreement_tables():
    """Criar tabelas de Rental Agreement no PostgreSQL"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if not is_postgres:
                    print("   ‚ö†Ô∏è  SQLite detected, skipping RA tables", flush=True)
                    return
                
                print("   üîß Creating PostgreSQL RA tables...", flush=True)
                cur = conn.cursor()
                # Tabela de templates do RA
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS rental_agreement_templates (
                        id SERIAL PRIMARY KEY,
                        version INTEGER NOT NULL,
                        filename TEXT NOT NULL,
                        file_data BYTEA NOT NULL,
                        num_pages INTEGER,
                        uploaded_by TEXT,
                        uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        is_active INTEGER DEFAULT 0,
                        notes TEXT
                    )
                """)
                
                # Tabela de coordenadas do RA
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS rental_agreement_coordinates (
                        id SERIAL PRIMARY KEY,
                        field_id TEXT NOT NULL,
                        x REAL NOT NULL,
                        y REAL NOT NULL,
                        width REAL NOT NULL,
                        height REAL NOT NULL,
                        page INTEGER DEFAULT 1,
                        field_type TEXT,
                        template_version INTEGER DEFAULT 1,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Tabela de hist√≥rico de mapeamentos do RA
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS rental_agreement_mapping_history (
                        id SERIAL PRIMARY KEY,
                        template_version INTEGER NOT NULL,
                        field_id TEXT NOT NULL,
                        x REAL NOT NULL,
                        y REAL NOT NULL,
                        width REAL NOT NULL,
                        height REAL NOT NULL,
                        page INTEGER DEFAULT 1,
                        field_type TEXT,
                        mapped_by TEXT,
                        mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                cur.execute("CREATE INDEX IF NOT EXISTS idx_ra_field ON rental_agreement_coordinates(field_id)")
                cur.close()
                
                print("   ‚úÖ rental_agreement_templates", flush=True)
                print("   ‚úÖ rental_agreement_coordinates", flush=True)
                print("   ‚úÖ rental_agreement_mapping_history", flush=True)
                
                conn.commit()
            finally:
                conn.close()
    except Exception as e:
        print(f"   ‚ùå Error creating RA tables: {e}", flush=True)
        import traceback
        traceback.print_exc()

@app.on_event("startup")
async def startup_event():
    """Initialize database and create default users on startup"""
    print(f"========================================", flush=True)
    print(f"üöÄ APP STARTUP - Rental Price Tracker", flush=True)
    print(f"üîç Testing data persistence after deploy", flush=True)
    print(f"========================================", flush=True)
    
    # Initialize database tables FIRST
    try:
        print(f"üìä Initializing database tables...", flush=True)
        _ensure_users_table()
        print(f"   ‚úÖ users table created/exists", flush=True)
        
        # Initialize ALL other tables (price_snapshots, ai_learning_data, etc.)
        init_db()
        print(f"   ‚úÖ All tables created/verified (19 tables total)", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Database initialization error: {e}", flush=True)
        import traceback
        traceback.print_exc()
    
    # Fix PostgreSQL schema AFTER tables exist
    if _USE_NEW_DB and USE_POSTGRES:
        try:
            print(f"üîß Fixing PostgreSQL schema...", flush=True)
            # Run inline instead of subprocess for better error handling
            from fix_postgres_schema import fix_users_table, fix_system_logs_table
            
            if fix_users_table():
                print(f"   ‚úÖ users schema fixed", flush=True)
            else:
                print(f"   ‚ö†Ô∏è  users schema warnings", flush=True)
            
            if fix_system_logs_table():
                print(f"   ‚úÖ system_logs schema fixed", flush=True)
            else:
                print(f"   ‚ö†Ô∏è  system_logs schema warnings", flush=True)
        except Exception as e:
            print(f"‚ö†Ô∏è  Schema fix error: {e}", flush=True)
            import traceback
            traceback.print_exc()
    
    # Create default users AFTER schema is fixed
    try:
        print(f"üë• Creating default users...", flush=True)
        _ensure_default_users()
        print(f"‚úÖ Default users ready (admin/admin)", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Default users error: {e}", flush=True)
    
    # Create Damage Reports tables
    try:
        print(f"üìã Creating Damage Reports tables...", flush=True)
        _ensure_damage_reports_tables()
        print(f"‚úÖ Damage Reports tables ready", flush=True)
    except Exception as e:
        print(f"   ‚ùå Error with DR tables: {e}", flush=True)
    
    # Create Rental Agreement tables
    try:
        print(f"üìã Creating Rental Agreement tables...", flush=True)
        _ensure_rental_agreement_tables()
        print(f"‚úÖ Rental Agreement tables ready", flush=True)
    except Exception as e:
        print(f"   ‚ùå Error with RA tables: {e}", flush=True)
    
    # Migrations
    try:
        # Migration: Add vehicle_damage_image column
        try:
            if USE_POSTGRES:
                import psycopg2
                import os
                database_url = os.getenv("DATABASE_URL")
                if not database_url:
                    print(f"   ‚ö†Ô∏è  DATABASE_URL not found, skipping PostgreSQL migration", flush=True)
                else:
                    conn = psycopg2.connect(database_url)
                    try:
                        with conn.cursor() as cur:
                            cur.execute("ALTER TABLE damage_reports ADD COLUMN vehicle_damage_image BYTEA")
                            conn.commit()
                        print(f"   ‚úÖ Added vehicle_damage_image column (PostgreSQL)", flush=True)
                    except Exception as e:
                        conn.rollback()  # CRITICAL for PostgreSQL
                        error_msg = str(e).lower()
                        if 'already exists' in error_msg or 'duplicate column' in error_msg:
                            print(f"   ‚ÑπÔ∏è  vehicle_damage_image column already exists", flush=True)
                        else:
                            print(f"   ‚ö†Ô∏è  Could not add vehicle_damage_image: {e}", flush=True)
                    finally:
                        conn.close()
            else:
                conn = _db_connect()
                try:
                    conn.execute("ALTER TABLE damage_reports ADD COLUMN vehicle_damage_image BLOB")
                    conn.commit()
                    print(f"   ‚úÖ Added vehicle_damage_image column (SQLite)", flush=True)
                except Exception as e:
                    conn.rollback()  # CRITICAL for SQLite
                    error_msg = str(e).lower()
                    if 'already exists' in error_msg or 'duplicate column' in error_msg:
                        print(f"   ‚ÑπÔ∏è  vehicle_damage_image column already exists", flush=True)
                    else:
                        print(f"   ‚ö†Ô∏è  Could not add vehicle_damage_image: {e}", flush=True)
                finally:
                    conn.close()
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Migration error: {e}", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Damage Reports tables error: {e}", flush=True)
    
    # Create Recent Searches table
    try:
        print(f"üîç Creating Recent Searches table...", flush=True)
        _ensure_recent_searches_table()
        print(f"‚úÖ Recent Searches table ready", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Recent Searches table error: {e}", flush=True)
    
    # Ensure all missing tables/columns
    try:
        print(f"üîß Ensuring missing tables/columns...", flush=True)
        _ensure_missing_tables()
        print(f"‚úÖ All missing tables/columns ready", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Missing tables error: {e}", flush=True)
    
    print(f"========================================", flush=True)

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    # Redirect to login on unauthorized/forbidden
    if exc.status_code in (401, 403):
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)

# --- Admin: Test email ---
@app.get("/admin/price-validation", response_class=HTMLResponse)
async def admin_price_validation(request: Request):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    return templates.TemplateResponse("price_validation_rules.html", {"request": request})

@app.get("/admin/export-db")
async def admin_export_db(request: Request):
    """Temporary endpoint to export database"""
    try:
        require_admin(request)
    except HTTPException:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    
    try:
        return FileResponse(
            path=str(DB_PATH),
            filename="data_backup.db",
            media_type="application/octet-stream"
        )
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/admin/export-vehicles-json")
async def admin_export_vehicles_json(request: Request):
    """Export vehicles as JSON"""
    try:
        require_admin(request)
    except HTTPException:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    
    try:
        vehicles = []
        with _db_lock:
            con = _db_connect()
            try:
                cur = con.execute("SELECT id, brand, model, code, category, doors, seats, transmission, luggage, photo_url, enabled FROM car_groups")
                for r in cur.fetchall():
                    vehicles.append({
                        "id": r[0], "brand": r[1], "model": r[2], "code": r[3],
                        "category": r[4], "doors": r[5], "seats": r[6],
                        "transmission": r[7], "luggage": r[8], "photo_url": r[9], "enabled": r[10]
                    })
            finally:
                con.close()
        return JSONResponse({"vehicles": vehicles, "count": len(vehicles)})
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/admin/test-email", response_class=HTMLResponse)
async def admin_test_email_send(request: Request, to: str = Form("")):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    err = None
    try:
        _send_creds_email((to or "").strip(), "test.user", "Temp1234!")
    except Exception as e:
        err = str(e)
    ok = err is None
    return templates.TemplateResponse("admin_test_email.html", {"request": request, "error": err, "ok": ok})

@app.exception_handler(Exception)
async def unhandled_exception_handler(request: Request, exc: Exception):
    try:
        (DEBUG_DIR / "last_exception.txt").write_text(_tb.format_exc(), encoding="utf-8")
    except Exception:
        pass
    # Ensure a valid response is always returned to Starlette
    return JSONResponse({"ok": False, "error": "Server error"}, status_code=500)

# --- Prices response cache (memory) ---
_PRICES_CACHE: Dict[str, Tuple[float, Dict[str, Any]]] = {}

async def _compute_prices_for(url: str) -> Dict[str, Any]:
    headers = {"User-Agent": "Mozilla/5.0 (compatible; PriceTracker/1.0)"}
    # Use async fetch to avoid blocking and improve concurrency
    r = await async_fetch_with_optional_proxy(url, headers=headers)
    r.raise_for_status()
    html = r.text
    # Parse HTML off the main loop
    items = await asyncio.to_thread(parse_prices, html, url)
    items = convert_items_gbp_to_eur(items)
    items = apply_price_adjustments(items, url)
    # schedule image prefetch (best-effort)
    try:
        img_urls: List[str] = []
        for it in items:
            u = (it.get("photo") or "").strip()
            if u and (u.startswith("http://") or u.startswith("https://")):
                img_urls.append(u)
        if img_urls:
            asyncio.create_task(_prefetch_many(img_urls[:12]))
            asyncio.create_task(_delayed_prefetch(img_urls[12:64], 1.5))
    except Exception:
        pass
    return {"ok": True, "count": len(items), "items": items}

def _cache_get(url: str) -> Optional[Dict[str, Any]]:
    try:
        ts, payload = _PRICES_CACHE.get(url, (0.0, None))
        if not payload:
            return None
        age = time.time() - ts
        if age <= PRICES_CACHE_TTL_SECONDS:
            return payload
        return None
    except Exception:
        return None

def _cache_set(url: str, payload: Dict[str, Any]):
    try:
        _PRICES_CACHE[url] = (time.time(), payload)
    except Exception:
        pass

async def _refresh_prices_background(url: str):
    try:
        data = await _compute_prices_for(url)
        _cache_set(url, data)
    except Exception:
        pass
    return JSONResponse({"ok": False, "error": "Server error"}, status_code=500)

# --- Image cache proxy and retention ---
def _ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "jpeg" in ct: return ".jpg"
    if "png" in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "gif" in ct: return ".gif"
    if "svg" in ct: return ".svg"
    return ".bin"

def _guess_ext_from_url(u: str) -> str:
    try:
        p = u.split("?")[0]
        for ext in (".jpg", ".jpeg", ".png", ".webp", ".gif", ".svg"):
            if p.lower().endswith(ext):
                return ".jpg" if ext == ".jpeg" else ext
    except Exception:
        pass
    return ""

def _cache_path_for(url: str) -> Path:
    import hashlib
    h = hashlib.sha256(url.encode("utf-8")).hexdigest()
    return CACHE_CARS_DIR / h

def _serve_file(fp: Path, content_type: str = "application/octet-stream"):
    try:
        data = fp.read_bytes()
    except Exception:
        raise HTTPException(status_code=404, detail="Not found")
    headers = {"Cache-Control": f"public, max-age={IMAGE_CACHE_DAYS*86400}"}
    return Response(content=data, media_type=content_type or "application/octet-stream", headers=headers)

@app.get("/img")
async def img_proxy(request: Request, src: str):
    try:
        if not src or not (src.startswith("http://") or src.startswith("https://")):
            raise HTTPException(status_code=400, detail="Invalid src")
        key = _cache_path_for(src)
        meta = key.with_suffix(".meta")
        # Serve from cache if present
        if key.exists():
            try:
                now = time.time(); os.utime(key, (now, now));
                if meta.exists(): os.utime(meta, (now, now))
            except Exception:
                pass
            ct = "application/octet-stream"
            try:
                if meta.exists():
                    ct = (meta.read_text(encoding="utf-8").strip() or ct)
            except Exception:
                pass
            return _serve_file(key, ct)

        # On HEAD requests, don't fetch body, just forward and prime headers
        if request.method == "HEAD":
            import httpx
            async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
                hr = await client.head(src)
            if hr.status_code != 200:
                raise HTTPException(status_code=404, detail="Upstream not found")
            headers = {"Cache-Control": f"public, max-age={IMAGE_CACHE_DAYS*86400}"}
            return Response(status_code=200, headers=headers)

        # Fetch from origin using requests for broader SSL compatibility, then cache
        import requests as _rq
        try:
            rr = _rq.get(src, timeout=15, headers={"User-Agent": "PriceTracker/1.0"})
        except Exception as e:
            raise HTTPException(status_code=502, detail=f"Upstream error: {type(e).__name__}")
        if rr.status_code != 200 or not rr.content:
            raise HTTPException(status_code=404, detail="Upstream not found")
        ct = rr.headers.get("content-type", "application/octet-stream")
        try:
            with key.open("wb") as f:
                f.write(rr.content)
            meta.write_text(ct, encoding="utf-8")
        except Exception:
            pass
        headers = {"Cache-Control": f"public, max-age={IMAGE_CACHE_DAYS*86400}"}
        return Response(content=rr.content, media_type=ct or "application/octet-stream", headers=headers)
    except HTTPException:
        raise
    except Exception as e:
        try:
            (DEBUG_DIR / "img_error.txt").write_text(f"{type(e).__name__}: {e}\n", encoding="utf-8")
        except Exception:
            pass
        raise HTTPException(status_code=500, detail="Image fetch error")

def cleanup_image_cache():
    try:
        cutoff = time.time() - IMAGE_CACHE_DAYS*86400
        for fp in CACHE_CARS_DIR.glob("*"):
            try:
                if fp.is_file():
                    st = fp.stat()
                    if max(st.st_mtime, st.st_atime) < cutoff:
                        fp.unlink(missing_ok=True)
            except Exception:
                continue
    except Exception:
        pass

BASE_DIR = Path(__file__).resolve().parent
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))

static_dir = BASE_DIR / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
# Persistent image cache under DATA_DIR
CACHE_CARS_DIR = Path(os.environ.get("CACHE_IMAGES_DIR", str(Path(os.environ.get("DATA_DIR", str(BASE_DIR))) / "cars")))
CACHE_CARS_DIR.mkdir(parents=True, exist_ok=True)
# Persisted uploads live under DATA_DIR and are served at /uploads
UPLOADS_ROOT = Path(os.environ.get("UPLOADS_ROOT", str(Path(os.environ.get("DATA_DIR", str(BASE_DIR))) / "uploads")))
UPLOADS_ROOT.mkdir(parents=True, exist_ok=True)
try:
    app.mount("/uploads", StaticFiles(directory=str(UPLOADS_ROOT)), name="uploads")
except Exception:
    pass
UPLOADS_DIR = UPLOADS_ROOT / "profiles"
UPLOADS_DIR.mkdir(parents=True, exist_ok=True)

# --- Background image prefetch ---
async def _prefetch_image(url: str):
    try:
        if not url or not (url.startswith("http://") or url.startswith("https://")):
            return
        key = _cache_path_for(url)
        if key.exists() and key.stat().st_size > 0:
            # already cached
            try:
                now = time.time(); os.utime(key, (now, now))
            except Exception:
                pass
            return
        import httpx
        async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
            r = await client.get(url)
            if r.status_code != 200 or not r.content:
                return
            try:
                with key.open("wb") as f:
                    f.write(r.content)
            except Exception:
                pass
    except Exception:
        pass

async def _prefetch_many(urls: List[str]):
    try:
        tasks = [asyncio.create_task(_prefetch_image(u)) for u in urls if isinstance(u, str) and u]
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
    except Exception:
        pass

async def _delayed_prefetch(urls: List[str], delay_seconds: float = 1.5):
    try:
        await asyncio.sleep(delay_seconds)
        await _prefetch_many(urls)
    except Exception:
        pass

# --- Icon fallbacks to avoid 404s ---
@app.get("/favicon.ico")
async def favicon_redirect():
    return RedirectResponse(url="/static/autoprudente-favicon.png?v=2", status_code=HTTP_303_SEE_OTHER)

@app.get("/apple-touch-icon.png")
async def apple_touch_icon_redirect():
    return RedirectResponse(url="/static/autoprudente-favicon.png?v=2", status_code=HTTP_303_SEE_OTHER)

@app.get("/apple-touch-icon-precomposed.png")
async def apple_touch_icon_pre_redirect():
    return RedirectResponse(url="/static/autoprudente-favicon.png?v=2", status_code=HTTP_303_SEE_OTHER)

@app.get("/static/ap-favicon.png")
async def static_ap_favicon_redirect():
    return RedirectResponse(url="/static/autoprudente-favicon.png?v=2", status_code=HTTP_303_SEE_OTHER)

DATA_DIR = Path(os.environ.get("DATA_DIR", str(BASE_DIR)))
DATA_DIR.mkdir(parents=True, exist_ok=True)
DB_PATH = DATA_DIR / "data.db"
_db_lock = Lock()
DEBUG_DIR = Path(os.environ.get("DEBUG_DIR", BASE_DIR / "static" / "debug"))
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# --- Admin/Users: DB helpers ---
class PostgreSQLConnectionWrapper:
    """Wrapper para adicionar m√©todo execute() √† conex√£o PostgreSQL"""
    def __init__(self, conn):
        self._conn = conn
        self._cursor = None
    
    def execute(self, query, params=None):
        """Execute query usando cursor"""
        # Convert SQLite ? placeholders to PostgreSQL %s
        if '?' in query:
            # Count number of ? to ensure we have right number of params
            num_placeholders = query.count('?')
            query = query.replace('?', '%s')
            
            # Ensure params is a tuple
            if params is not None:
                if not isinstance(params, (tuple, list)):
                    params = (params,)
                elif isinstance(params, list):
                    params = tuple(params)
        
        # Convert SQLite AUTOINCREMENT to PostgreSQL SERIAL
        if 'AUTOINCREMENT' in query.upper():
            query = query.replace('INTEGER PRIMARY KEY AUTOINCREMENT', 'SERIAL PRIMARY KEY')
            query = query.replace('AUTOINCREMENT', '')
        
        # Convert SQLite datetime('now') to PostgreSQL NOW()
        if "datetime('now')" in query or 'datetime("now")' in query:
            query = query.replace("datetime('now')", "NOW()")
            query = query.replace('datetime("now")', "NOW()")
        
        # Convert INSERT OR REPLACE to INSERT ... ON CONFLICT (automatic PostgreSQL compatibility)
        if 'INSERT OR REPLACE' in query.upper():
            import re
            # Replace INSERT OR REPLACE with INSERT
            query = re.sub(r'INSERT\s+OR\s+REPLACE', 'INSERT', query, flags=re.IGNORECASE)
            # Add ON CONFLICT DO NOTHING at the end if not already there
            if 'ON CONFLICT' not in query.upper():
                query = query.rstrip().rstrip(';') + ' ON CONFLICT DO NOTHING'
            # Debug log only (not warning)
            import logging
            logging.debug(f"‚úÖ AUTO-CONVERTED: INSERT OR REPLACE ‚Üí INSERT ... ON CONFLICT DO NOTHING")
        
        # Convert unquoted 'user' column to quoted "user" (PostgreSQL reserved keyword)
        import re
        # Match 'user' as column name but not in strings
        # Patterns: ", user," -> ", "user","  or "(user)" -> "("user")" or " user " -> " "user" "
        query = re.sub(r'\buser\b(?=\s*[,\)])', '"user"', query, flags=re.IGNORECASE)
        query = re.sub(r'\(\s*user\s*,', '("user",', query, flags=re.IGNORECASE)
        
        self._cursor = self._conn.cursor()
        try:
            if params:
                self._cursor.execute(query, params)
            else:
                self._cursor.execute(query)
        except Exception as e:
            # Log the error with query and params summary (not full content)
            import logging
            logging.error(f"PostgreSQL execute error: {e}")
            logging.error(f"Query: {query}")
            if params:
                # Show only summary to avoid flooding logs with data
                param_summary = []
                for p in params:
                    if isinstance(p, str) and len(p) > 100:
                        param_summary.append(f"<string:{len(p)} chars>")
                    else:
                        param_summary.append(repr(p)[:50])
                logging.error(f"Params summary: {param_summary}")
            raise
        return self._cursor
    
    def cursor(self):
        """Retorna um cursor da conex√£o PostgreSQL"""
        return self._conn.cursor()
    
    def commit(self):
        return self._conn.commit()
    
    def rollback(self):
        return self._conn.rollback()
    
    def close(self):
        if self._cursor:
            self._cursor.close()
        return self._conn.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            self.rollback()
        self.close()

def _db_connect():
    """Database connection - supports both PostgreSQL and SQLite"""
    if _USE_NEW_DB:
        conn = _db_connect_new()
        # Wrap PostgreSQL connection to add execute() method
        if conn.__class__.__module__ == 'psycopg2.extensions' and not hasattr(conn, 'row_factory'):
            return PostgreSQLConnectionWrapper(conn)
        return conn
    else:
        return sqlite3.connect(str(DB_PATH))

def _convert_query_for_db(query, conn):
    """Convert SQLite query to PostgreSQL if needed"""
    is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
    if is_postgres:
        # Convert datetime('now') to NOW()
        query = query.replace("datetime('now')", "NOW()")
        query = query.replace('datetime("now")', "NOW()")
        # Convert ? to %s
        import re
        # Count placeholders
        param_count = query.count('?')
        if param_count > 0:
            parts = query.split('?')
            query = '%s'.join(parts)
    return query

def _ensure_users_table():
    with _db_lock:
        con = _db_connect()
        try:
            con.execute("""
                CREATE TABLE IF NOT EXISTS users (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  username TEXT UNIQUE NOT NULL,
                  password_hash TEXT NOT NULL,
                  first_name TEXT,
                  last_name TEXT,
                  mobile TEXT,
                  email TEXT,
                  profile_picture_path TEXT,
                  profile_picture_data BLOB,
                  is_admin INTEGER DEFAULT 0,
                  enabled INTEGER DEFAULT 1,
                  created_at TEXT,
                  google_id TEXT UNIQUE
                )
            """)
            
            # Migration: Add google_id column if it doesn't exist
            try:
                con.execute("ALTER TABLE users ADD COLUMN google_id TEXT UNIQUE")
                con.commit()
                logging.info("‚úÖ Added google_id column to users table")
            except Exception as e:
                con.rollback()  # CRITICAL for PostgreSQL - must rollback on error
                # Column already exists, ignore
                pass
            
            # Migration: Add profile_picture_data column if it doesn't exist
            try:
                con.execute("ALTER TABLE users ADD COLUMN profile_picture_data BLOB")
                con.commit()
                logging.info("‚úÖ Added profile_picture_data column to users table")
            except Exception as e:
                con.rollback()  # CRITICAL for PostgreSQL - must rollback on error
                # Column already exists, ignore
                pass
            
            con.commit()
        finally:
            con.close()

def _get_user_by_username(username: str) -> Optional[Dict[str, Any]]:
    try:
        with _db_lock:
            con = _db_connect()
            try:
                cur = con.execute("SELECT id, username, first_name, last_name, email, mobile, profile_picture_path, is_admin, enabled FROM users WHERE username=?", (username,))
                r = cur.fetchone()
                if not r:
                    return None
                return {
                    "id": r[0],
                    "username": r[1],
                    "first_name": r[2] or "",
                    "last_name": r[3] or "",
                    "email": r[4] or "",
                    "mobile": r[5] or "",
                    "profile_picture_path": r[6] or "",
                    "is_admin": bool(r[7]),
                    "enabled": bool(r[8]),
                }
            finally:
                con.close()
    except Exception:
        return None

def _get_current_user_from_session(request: Request):
    """Helper to get current user from session using _db_connect()"""
    user_id = request.session.get("user_id")
    if not user_id:
        return None
    
    try:
        conn = _db_connect()
        try:
            if conn.__class__.__module__ == 'psycopg2.extensions':
                # PostgreSQL
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
                current_user = cursor.fetchone()
                cursor.close()
            else:
                # SQLite
                current_user = conn.execute("SELECT * FROM users WHERE id = ?", (user_id,)).fetchone()
            return current_user
        finally:
            conn.close()
    except Exception:
        return None

# --- Activity Log ---
def _ensure_activity_table():
    with _db_lock:
        con = _db_connect()
        try:
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS activity_log (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  ts_utc TEXT NOT NULL,
                  username TEXT,
                  action TEXT NOT NULL,
                  details TEXT,
                  ip TEXT,
                  user_agent TEXT
                );
                """
            )
            con.commit()
        finally:
            con.close()

def log_activity(request: Request, action: str, details: str = "", username: Optional[str] = None):
    try:
        _ensure_activity_table()
    except Exception:
        pass
    # best-effort metadata
    try:
        ip = request.client.host if request and request.client else None
    except Exception:
        ip = None
    ua = request.headers.get("user-agent", "") if request else ""
    user = username or (request.session.get("username") if request and request.session else None)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute(
                    "INSERT INTO activity_log (ts_utc, username, action, details, ip, user_agent) VALUES (?,?,?,?,?,?)",
                    (datetime.now(timezone.utc).isoformat(), user, action, details, ip or "", ua[:300])
                )
                con.commit()
            finally:
                con.close()
    except Exception:
        pass
    pass

def cleanup_activity_retention():
    try:
        _ensure_activity_table()
        if AUDIT_RETENTION_DAYS <= 0:
            return
        cutoff = datetime.now(timezone.utc).timestamp() - AUDIT_RETENTION_DAYS*86400
        # Compare lexicographically on ISO timestamps by computing a boundary
        cutoff_iso = datetime.utcfromtimestamp(cutoff).replace(tzinfo=timezone.utc).isoformat()
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("DELETE FROM activity_log WHERE ts_utc < ?", (cutoff_iso,))
                con.commit()
            finally:
                con.close()
    except Exception:
        pass

def _hash_password(pw: str, salt: str = ""):  # basic salted sha256
    if not salt:
        salt = secrets.token_hex(8)
    digest = hashlib.sha256((salt + ":" + pw).encode("utf-8")).hexdigest()
    return f"sha256:{salt}:{digest}"

def _verify_password(pw: str, stored: str) -> bool:
    try:
        algo, salt, digest = stored.split(":", 2)
        if algo != "sha256":
            return False
        test = hashlib.sha256((salt + ":" + pw).encode("utf-8")).hexdigest()
        return secrets.compare_digest(test, digest)
    except Exception:
        return False

def clean_car_name(car_name: str) -> str:
    """
    Limpa e normaliza nomes de carros EXATAMENTE como o Vehicle Editor
    - Remove duplica√ß√µes como "Autoautom√°tico" ‚Üí "Autom√°tico"
    - Remove "ou similar"
    - Remove "4p" (4 portas) exceto para 7 e 9 lugares
    - Normaliza espa√ßos
    - Converte para LOWERCASE (igual ao VEHICLES dictionary)
    """
    if not car_name:
        return ""
    
    name = str(car_name).strip()
    
    # Remover duplica√ß√µes comuns
    name = re.sub(r'[Aa]uto[Aa]utom[a√°]tico', 'Autom√°tico', name)
    name = re.sub(r'[Aa]uto[Aa]utomatic', 'Automatic', name)
    
    # Remover "ou similar" e variantes
    name = re.sub(r'\s*ou\s+similar(es)?.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'\s*or\s+similar.*$', '', name, flags=re.IGNORECASE)
    
    # Remover v√≠rgulas e espa√ßos extras (ex: "2008 , Electric" ‚Üí "2008 Electric")
    name = re.sub(r'\s*,\s*', ' ', name)
    
    # Remover "Special Edition" e variantes
    name = re.sub(r'\s+special\s+edition\b', '', name, flags=re.IGNORECASE)
    name = re.sub(r'\s+edition\b', '', name, flags=re.IGNORECASE)
    
    # Remover "4p" (4 portas) EXCETO para 7 e 9 lugares
    # Exemplos: "Fiat 500 4p" ‚Üí "Fiat 500", "Fiat Panda 4p" ‚Üí "Fiat Panda"
    # MAS: "Dacia Lodgy 7 Lugares 4p" ‚Üí mant√©m (n√£o remove)
    name_lower = name.lower()
    if '7' not in name_lower and '9' not in name_lower and 'seater' not in name_lower and 'lugares' not in name_lower:
        # Remover "4p", "4 portas", "4 doors"
        name = re.sub(r'\s+4p\b', '', name, flags=re.IGNORECASE)
        name = re.sub(r'\s+4\s*portas?\b', '', name, flags=re.IGNORECASE)
        name = re.sub(r'\s+4\s*doors?\b', '', name, flags=re.IGNORECASE)
    
    # Normalizar espa√ßos m√∫ltiplos
    name = re.sub(r'\s+', ' ', name).strip()
    
    # N√ÉO converter para lowercase aqui! 
    # Manter capitaliza√ß√£o original para display bonito
    # O lowercase √© feito apenas quando consultar VEHICLES
    
    return name

def capitalize_car_name(car_name: str) -> str:
    """
    Capitaliza nomes de carros para display:
    - Primeira letra de cada palavra em mai√∫scula
    - SW, SUV, 4X4 sempre em mai√∫sculas
    - Exemplos:
      - "peugeot 2008 auto" ‚Üí "Peugeot 2008 Auto"
      - "renault megane sw auto" ‚Üí "Renault Megane SW Auto"
      - "toyota rav4 4x4 auto" ‚Üí "Toyota Rav4 4X4 Auto"
    """
    if not car_name:
        return ""
    
    # Palavras que devem ficar em mai√∫sculas
    uppercase_words = {'sw', 'suv', '4x4', 'gt', 'gti', 'rs', 'st', 'amg', 'bmw', 'vw'}
    
    words = car_name.lower().split()
    capitalized = []
    
    for word in words:
        if word in uppercase_words:
            capitalized.append(word.upper())
        else:
            capitalized.append(word.capitalize())
    
    return ' '.join(capitalized)

def load_admin_vehicles() -> dict:
    """Carrega caracter√≠sticas dos ve√≠culos parametrizados no Admin Vehicles
    
    Retorna dict:
    {
        'B1': {
            'brand': 'Fiat',
            'model': '500',
            'category': 'Mini',
            'transmission': 'Manual',
            'doors': 3,
            'seats': 4,
            'luggage': 1,
            'photo_url': 'https://...'
        },
        ...
    }
    """
    global _admin_vehicles_cache, _admin_vehicles_cache_time
    
    # Usar cache se ainda v√°lido
    now = time.time()
    if _admin_vehicles_cache and (now - _admin_vehicles_cache_time) < ADMIN_VEHICLES_CACHE_TTL:
        return _admin_vehicles_cache
    
    vehicle_groups = {}
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT code, brand, model, category, transmission, 
                                   doors, seats, luggage, photo_url
                            FROM car_groups
                            WHERE enabled = 1
                        """)
                        rows = cur.fetchall()
                else:
                    # SQLite
                    rows = conn.execute("""
                        SELECT code, brand, model, category, transmission,
                               doors, seats, luggage, photo_url
                        FROM car_groups
                        WHERE enabled = 1
                    """).fetchall()
                
                for row in rows:
                    full_code = row[0]  # ex: B1-FIAT500
                    # Extrair apenas o c√≥digo do grupo (B1, D, F, etc)
                    group_code = full_code.split('-')[0] if '-' in full_code else full_code
                    
                    vehicle_groups[group_code] = {
                        'brand': row[1],
                        'model': row[2],
                        'category': row[3],
                        'transmission': row[4],
                        'doors': row[5],
                        'seats': row[6],
                        'luggage': row[7],
                        'photo_url': row[8]
                    }
                    
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"[ADMIN-VEHICLES] Error loading vehicles: {e}")
        return {}
    
    # Atualizar cache
    _admin_vehicles_cache = vehicle_groups
    _admin_vehicles_cache_time = now
    
    logging.info(f"[ADMIN-VEHICLES] Loaded {len(vehicle_groups)} groups from Admin Vehicles")
    return vehicle_groups

def map_category_to_group(category: str, car_name: str = "", transmission: str = "") -> str:
    """
    Mapeia categorias descritivas para c√≥digos de grupos definidos:
    B1, B2, D, E1, E2, F, G, X, J1, J2, L1, L2, M1, M2, N, Others
    
    CASE-INSENSITIVE: Converte para lowercase para compara√ß√£o
    
    B1 vs B2 LOGIC (baseado em LUGARES, n√£o PORTAS):
    - B1 = Mini 4 LUGARES (Fiat 500, Peugeot 108, C1, VW Up, Kia Picanto, Toyota Aygo)
    - B2 = Mini 5 LUGARES (Fiat Panda, Hyundai i10, etc)
    
    REGRAS ESPECIAIS:
    - Cabrio/Cabriolet ‚Üí G (Cabrio)
    - Toyota Aygo X ‚Üí F (SUV)
    - Mini 4 lugares Autom√°tico ‚Üí E1
    - Premium/Luxury ‚Üí X
    
    Args:
        category: Categoria do carro (ex: "Mini", "Economy", "SUV")
        car_name: Nome do carro (ex: "Fiat 500", "VW Golf")
        transmission: Transmiss√£o (ex: "Manual", "Automatic")
    """
    # PRIORIDADE 0: Matching inteligente baseado em Admin Vehicles
    if match_vehicle_group_by_characteristics and car_name:
        vehicle_groups = load_admin_vehicles()
        if vehicle_groups:
            # Criar fun√ß√£o fallback que √© a l√≥gica atual
            def fallback(cat, name):
                return _map_category_fallback(cat, name, transmission)
            
            matched_group = match_vehicle_group_by_characteristics(
                category, car_name, transmission, vehicle_groups, fallback
            )
            
            # Se encontrou match v√°lido (n√£o "Others"), usar
            if matched_group and matched_group != "Others":
                logging.info(f"[SMART-MATCH] {car_name} ({category}) ‚Üí {matched_group} (via Admin characteristics)")
                return matched_group
    
    # Fallback para l√≥gica original
    return _map_category_fallback(category, car_name, transmission)

def _map_category_fallback(category: str, car_name: str = "", transmission: str = "") -> str:
    """L√≥gica de fallback original para mapeamento de categorias"""
    # Converter para lowercase para mapeamento case-insensitive
    cat = category.strip().lower() if category else ""
    car_lower = car_name.lower() if car_name else ""
    trans_lower = transmission.lower() if transmission else ""
    
    # PRIORIDADE -1: CABRIO/CABRIOLET no NOME ‚Üí SEMPRE Grupo G
    # Independente da categoria (Luxury, Mini, SUV, etc), se tem "cabrio" no nome = G
    if any(word in car_lower for word in ['cabrio', 'cabriolet', 'convertible', 'convers√≠vel']):
        return "G"
    
    # PRIORIDADE 0: Categorias expl√≠citas do CarJet (mais confi√°veis que tabela manual)
    # Suporta INGL√äS e PORTUGU√äS
    
    # Mini 4 Seats / Mini 4 Lugares ‚Üí B1 ou E1 (se autom√°tico)
    if cat in ['mini 4 seats', 'mini 4 doors', 'mini 4 portas', 'mini 4 lugares']:
        is_auto = any(word in trans_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico']) or \
                  any(word in car_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico'])
        if is_auto:
            return "E1"
        return "B1"
    
    # Mini 5 Seats / Mini 5 Lugares ‚Üí B2 ou E1 (se autom√°tico)
    if cat in ['mini 5 seats', 'mini 5 doors', 'mini 5 portas', 'mini 5 lugares']:
        is_auto = any(word in trans_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico']) or \
                  any(word in car_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico'])
        if is_auto:
            return "E1"
        return "B2"
    
    # Mini Automatic / Mini Auto ‚Üí E1
    if cat in ['mini automatic', 'mini auto', 'mini autom√°tico']:
        return "E1"
    
    # Economy / Econ√≥mico ‚Üí D
    if cat in ['economy', 'econ√≥mico', 'compact', 'compacto']:
        return "D"
    
    # Economy Automatic / Economy Auto ‚Üí E2
    if cat in ['economy automatic', 'economy auto', 'econ√≥mico automatic', 'econ√≥mico auto',
               'compact automatic', 'compact auto']:
        return "E2"
    
    # SUV ‚Üí F
    if cat in ['suv', 'jeep']:
        return "F"
    
    # SUV Automatic / SUV Auto ‚Üí L1
    if cat in ['suv automatic', 'suv auto', 'jeep automatic', 'jeep auto']:
        return "L1"
    
    # Station Wagon / Estate / Carrinha ‚Üí J2
    if cat in ['station wagon', 'estate', 'carrinha', 'estate/station wagon', 'sw', 'touring']:
        return "J2"
    
    # Station Wagon Automatic ‚Üí L2
    if cat in ['station wagon automatic', 'station wagon auto', 'estate automatic', 'estate auto',
               'carrinha automatic', 'carrinha auto', 'sw automatic', 'sw auto']:
        return "L2"
    
    # Crossover ‚Üí J1
    if cat in ['crossover']:
        return "J1"
    
    # Cabrio / Cabriolet / Convertible ‚Üí G
    if cat in ['cabrio', 'cabriolet', 'convertible', 'convers√≠vel']:
        return "G"
    
    # Luxury / Premium ‚Üí Others (n√£o oferecemos estas categorias)
    if cat in ['luxury', 'premium', 'luxo']:
        return "Others"
    
    # 7 Seater / 7 Seats ‚Üí M1
    if cat in ['7 seater', '7 seats', '7 lugares', 'people carrier', 'mpv']:
        return "M1"
    
    # 7 Seater Automatic / 7 Seats Auto ‚Üí M2
    if cat in ['7 seater automatic', '7 seater auto', '7 seats automatic', '7 seats auto',
               '7 lugares automatic', '7 lugares auto', '7 lugares autom√°tico',
               'mpv automatic', 'mpv auto']:
        return "M2"
    
    # 9 Seater / 9 Seats ‚Üí N
    if cat in ['9 seater', '9 seater automatic', '9 seater auto',
               '9 seats', '9 seats automatic', '9 seats auto',
               '9 lugares', '9 lugares automatic', '9 lugares auto', '9 lugares autom√°tico',
               'minivan', 'van']:
        return "N"
    
    # PRIORIDADE 1: Consultar tabela car_groups (22 grupos categorizados manualmente)
    if car_name:
        try:
            car_clean = clean_car_name(car_name)
            car_clean_lower = car_clean.lower()
            
            # Buscar na tabela car_groups
            with _db_lock:
                conn = _db_connect()
                try:
                    is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                    
                    if is_postgres:
                        # PostgreSQL
                        with conn.cursor() as cur:
                            cur.execute(
                                "SELECT code FROM car_groups WHERE LOWER(name) = %s OR LOWER(model) = %s",
                                (car_clean_lower, car_clean_lower)
                            )
                            row = cur.fetchone()
                    else:
                        # SQLite
                        row = conn.execute(
                            "SELECT code FROM car_groups WHERE LOWER(name) = ? OR LOWER(model) = ?",
                            (car_clean_lower, car_clean_lower)
                        ).fetchone()
                    
                    if row:
                        # Extrair c√≥digo do grupo (ex: B1-FIAT500 -> B1)
                        full_code = row[0]
                        group_code = full_code.split('-')[0] if '-' in full_code else full_code
                        return group_code
                finally:
                    conn.close()
        except Exception as e:
            logging.error(f"Error querying car_groups: {e}")
            pass  # Se falhar, continuar para pr√≥xima prioridade
    
    # PRIORIDADE 2: Consultar dicion√°rio VEHICLES de carjet_direct.py
    # Tentar SEMPRE que tiver car_name, mesmo se category n√£o estiver vazia
    # (mas evitar loop infinito: s√≥ chamar recursivamente se encontrar categoria diferente)
    if car_name:
        try:
            from carjet_direct import VEHICLES
            import re
            
            # Normalizar nome do carro para consulta (lowercase)
            car_clean = clean_car_name(car_name)
            car_clean_lower = car_clean.lower().strip()
            
            # Remover sufixos comuns que impedem match
            # "Peugeot E-208 Electric" ‚Üí "peugeot e-208"
            # "Toyota Chr Auto" ‚Üí "toyota chr auto"
            car_normalized = car_clean_lower
            car_normalized = re.sub(r'\s+(electric|hybrid|diesel|petrol|plug-in|phev)$', '', car_normalized, flags=re.IGNORECASE)
            car_normalized = re.sub(r'\s+4x4$', '', car_normalized, flags=re.IGNORECASE)
            car_normalized = re.sub(r'\s+\d+\s*door(s)?$', '', car_normalized, flags=re.IGNORECASE)
            car_normalized = re.sub(r',\s*electric$', '', car_normalized, flags=re.IGNORECASE)
            car_normalized = re.sub(r',\s*hybrid$', '', car_normalized, flags=re.IGNORECASE)
            car_normalized = car_normalized.strip()
            
            # Tentar match direto
            if car_normalized in VEHICLES:
                category_from_vehicles = VEHICLES[car_normalized]
                # VEHICLES retorna categoria descritiva (ex: "ECONOMY", "SUV Auto")
                # Precisamos mapear para c√≥digo de grupo (B1, D, F, etc)
                # Passar car_name tamb√©m para manter contexto (ex: distinguir autom√°tico)
                # IMPORTANTE: S√≥ chamar recursivamente se a categoria for diferente (evitar loop)
                if category_from_vehicles.lower() != cat:
                    return map_category_to_group(category_from_vehicles, car_name)
            
            # Tentar match parcial (buscar chave que est√° contida no nome ou vice-versa)
            # Ordenar por tamanho decrescente para pegar matches mais espec√≠ficos primeiro
            for vehicle_key in sorted(VEHICLES.keys(), key=len, reverse=True):
                # Match se o nome do carro cont√©m a chave completa
                # Ex: "toyota chr auto" cont√©m "toyota chr"
                if len(vehicle_key) >= 5 and vehicle_key in car_normalized:
                    category_from_vehicles = VEHICLES[vehicle_key]
                    # S√≥ chamar recursivamente se a categoria for diferente
                    if category_from_vehicles.lower() != cat:
                        return map_category_to_group(category_from_vehicles, car_name)
        except ImportError:
            pass  # carjet_direct.py n√£o dispon√≠vel
        except Exception:
            pass  # Se falhar, continuar para pr√≥xima prioridade
    
    # PRIORIDADE 3: CABRIO/CABRIOLET ‚Üí Grupo G (apenas descapot√°veis)
    if any(word in car_lower for word in ['cabrio', 'cabriolet', 'convertible', 'convers√≠vel']):
        return "G"
    
    # PRIORIDADE 4: Toyota Aygo X ‚Üí F (SUV), n√£o confundir com Aygo normal (B1)
    if 'aygo x' in car_lower or 'aygo-x' in car_lower:
        return "F"
    
    # PRIORIDADE 5: Modelos de 4 LUGARES ‚Üí B1
    # (Fiat 500, Peugeot 108, C1, VW Up, Kia Picanto, Toyota Aygo)
    b1_4_lugares_models = [
        'fiat 500', 'fiat500',
        'peugeot 108', 'peugeot108',
        'citroen c1', 'citro√´n c1', 'c1',
        'volkswagen up', 'vw up', 'vwup',
        'kia picanto', 'kiapicanto',
        'toyota aygo', 'toyotaaygo',
    ]
    
    # Se categoria √© "mini" OU cont√©m "mini", verificar modelo espec√≠fico
    # MAS excluir categorias expl√≠citas (j√° tratadas acima)
    if 'mini' in cat and not 'countryman' in car_lower:
        # Excluir categorias expl√≠citas que j√° foram tratadas
        if cat not in ['mini 4 seats', 'mini 4 doors', 'mini 4 portas', 'mini 4 lugares',
                       'mini 5 seats', 'mini 5 doors', 'mini 5 portas', 'mini 5 lugares']:
            # Verificar se √© um modelo de 4 lugares (B1)
            for model in b1_4_lugares_models:
                if model in car_lower:
                    # Se √© autom√°tico de 4 lugares ‚Üí E1 (Mini Automatic)
                    is_auto = any(word in trans_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico']) or \
                              any(word in car_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico'])
                    if is_auto:
                        return "E1"
                    # Se √© manual de 4 lugares ‚Üí B1
                    return "B1"
            # Se n√£o √© B1 espec√≠fico, √© B2 (5 lugares)
            # Modelos B2: Fiat Panda, Hyundai i10, etc
            return "B2"
    
    # Mapeamento direto (TUDO EM LOWERCASE)
    # Suporta INGL√äS (do scraping CarJet) e PORTUGU√äS (do VEHICLES)
    category_map = {
        # B1 - Mini 4 Lugares / Mini 4 Doors
        "mini 4 doors": "B1",
        "mini 4 seats": "B1",  # Ingl√™s do CarJet
        "mini 4 portas": "B1",
        "mini 4 lugares": "B1",
        
        # B2 - Mini 5 Lugares / Mini 5 Doors
        "mini": "B2",
        "mini 5 doors": "B2",
        "mini 5 seats": "B2",  # Ingl√™s do CarJet
        "mini 5 portas": "B2",
        "mini 5 lugares": "B2",
        
        # D - Economy
        "economy": "D",
        "econ√≥mico": "D",
        "compact": "D",
        "compacto": "D",
        
        # E1 - Mini Automatic (do VEHICLES: "MINI Auto")
        "mini automatic": "E1",
        "mini auto": "E1",
        "mini autom√°tico": "E1",
        
        # E2 - Economy Automatic (do VEHICLES: "ECONOMY Auto")
        "economy automatic": "E2",
        "economy auto": "E2",
        "econ√≥mico automatic": "E2",
        "econ√≥mico auto": "E2",
        "compact automatic": "E2",
        "compact auto": "E2",
        
        # F - SUV
        "suv": "F",
        "jeep": "F",
        
        # G - Cabrio APENAS (Premium/Luxury ‚Üí Others)
        "cabrio": "G",
        "cabriolet": "G",
        "convertible": "G",
        "convers√≠vel": "G",
        
        # J1 - Crossover (do VEHICLES: "Crossover")
        "crossover": "J1",
        
        # J2 - Estate/Station Wagon
        "estate/station wagon": "J2",
        "station wagon": "J2",
        "estate": "J2",
        "carrinha": "J2",
        "sw": "J2",
        "touring": "J2",
        
        # L1 - SUV Automatic (do VEHICLES: "SUV Auto")
        "suv automatic": "L1",
        "suv auto": "L1",
        "jeep automatic": "L1",
        "jeep auto": "L1",
        
        # L2 - Station Wagon Automatic (do VEHICLES: "Station Wagon Auto")
        "station wagon automatic": "L2",
        "station wagon auto": "L2",
        "estate automatic": "L2",
        "estate auto": "L2",
        "carrinha automatic": "L2",
        "carrinha auto": "L2",
        "sw automatic": "L2",
        "sw auto": "L2",
        
        # M1 - 7 Seater (do VEHICLES: "7 Lugares")
        "7 seater": "M1",
        "7 seats": "M1",  # Ingl√™s do CarJet
        "7 lugares": "M1",
        "people carrier": "M1",
        "mpv": "M1",
        
        # M2 - 7 Seater Automatic (do VEHICLES: "7 Lugares Auto")
        "7 seater automatic": "M2",
        "7 seater auto": "M2",
        "7 seats automatic": "M2",  # Ingl√™s do CarJet
        "7 seats auto": "M2",  # Ingl√™s do CarJet
        "7 lugares automatic": "M2",
        "7 lugares auto": "M2",
        "7 lugares autom√°tico": "M2",
        "mpv automatic": "M2",
        "mpv auto": "M2",
        
        # N - 9 Seater (do VEHICLES: "9 Lugares")
        "9 seater": "N",
        "9 seater automatic": "N",
        "9 seater auto": "N",
        "9 seats": "N",  # Ingl√™s do CarJet
        "9 seats automatic": "N",  # Ingl√™s do CarJet
        "9 seats auto": "N",  # Ingl√™s do CarJet
        "9 lugares": "N",
        "9 lugares automatic": "N",
        "9 lugares auto": "N",
        "9 lugares autom√°tico": "N",
        "minivan": "N",
        "van": "N",
    }
    
    # Tentar match direto primeiro
    if cat in category_map:
        return category_map[cat]
    
    # FALLBACK: An√°lise inteligente por palavras-chave
    # Verificar se √© autom√°tico (priorizar transmission, depois category, depois car_name)
    is_auto = any(word in trans_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico']) or \
              any(word in cat for word in ['auto', 'automatic', 'autom√°tico', 'automatico']) or \
              any(word in car_lower for word in ['auto', 'automatic', 'autom√°tico', 'automatico'])
    
    # Verificar tipo de ve√≠culo por palavras-chave
    if '9' in cat or 'minivan' in cat or 'van' in cat:
        return "N"  # 9 Seater
    
    if '7' in cat or 'mpv' in cat or 'people carrier' in cat:
        return "M2" if is_auto else "M1"  # 7 Seater
    
    if any(word in cat for word in ['sw', 'station', 'wagon', 'estate', 'carrinha', 'touring']):
        return "L2" if is_auto else "J2"  # Station Wagon
    
    if 'crossover' in cat:
        return "J1"  # Crossover
    
    if any(word in cat for word in ['suv', 'jeep', '4x4', '4wd']):
        return "L1" if is_auto else "F"  # SUV
    
    if any(word in cat for word in ['cabrio', 'cabriolet', 'convertible']):
        return "G"  # Cabrio apenas
    
    if any(word in cat for word in ['premium', 'luxury', 'luxo']):
        return "Others"  # Luxury n√£o oferecido
    
    if any(word in cat for word in ['mini', 'small', 'pequeno']):
        # Verificar se √© 4 ou 5 lugares pelo nome do carro
        if any(model in car_lower for model in ['fiat 500', 'fiat500', 'peugeot 108', 'c1', 'vw up', 'picanto', 'aygo']):
            return "E1" if is_auto else "B1"  # Mini 4 lugares
        return "E1" if is_auto else "B2"  # Mini 5 lugares (default)
    
    if any(word in cat for word in ['economy', 'econom', 'compact', 'compacto']):
        return "E2" if is_auto else "D"  # Economy
    
    # Se chegou aqui, n√£o conseguiu mapear
    return "Others"

def _send_creds_email(to_email: str, username: str, password: str):
    # Ler configura√ß√µes SMTP da base de dados (persistente) em vez de env vars
    host = _get_setting("smtp_host", os.getenv("SMTP_HOST", "")).strip()
    port = int(_get_setting("smtp_port", os.getenv("SMTP_PORT", "587")) or 587)
    user = _get_setting("smtp_username", os.getenv("SMTP_USERNAME", "")).strip()
    pwd = _get_setting("smtp_password", os.getenv("SMTP_PASSWORD", "")).strip()
    from_addr = _get_setting("smtp_from", os.getenv("SMTP_FROM", "no-reply@example.com")).strip()
    use_tls_val = _get_setting("smtp_tls", os.getenv("SMTP_TLS", "true"))
    use_tls = str(use_tls_val).lower() in ("1", "true", "yes", "y", "on")
    if not host or not to_email:
        error_msg = f"Missing SMTP configuration: host={bool(host)}, to_email={bool(to_email)}"
        try:
            (DEBUG_DIR / "mail_error.txt").write_text(error_msg + "\n", encoding="utf-8")
        except Exception:
            pass
        raise Exception(error_msg)
    msg = EmailMessage()
    msg["Subject"] = "Your Car Rental Tracker account"
    msg["From"] = from_addr
    msg["To"] = to_email
    # Plain text
    msg.set_content(
        f"Hello,\n\nYour account was created.\n\nUsername: {username}\nPassword: {password}\n\nLogin: https://cartracker-6twv.onrender.com\n\nPlease change your password after first login."
    )
    # Simple branded HTML
    html = f"""
    <!doctype html>
    <html>
      <body style="margin:0;padding:0;background:#f8fafc;font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;">
        <table role="presentation" width="100%" cellpadding="0" cellspacing="0" style="background:#f8fafc;padding:24px 0;">
          <tr>
            <td align="center">
              <table role="presentation" width="560" cellpadding="0" cellspacing="0" style="background:#ffffff;border-radius:10px;overflow:hidden;border:1px solid #e5e7eb;">
                <tr>
                  <td style="background:#009cb6;padding:16px 20px;">
                    <img src="https://cartracker-6twv.onrender.com/static/ap-heather.png" alt="Car Rental Tracker" style="height:40px;display:block" />
                  </td>
                </tr>
                <tr>
                  <td style="padding:20px 20px 8px 20px;color:#111827;font-size:16px;">Hello,</td>
                </tr>
                <tr>
                  <td style="padding:0 20px 16px 20px;color:#111827;font-size:16px;">Your account was created.</td>
                </tr>
                <tr>
                  <td style="padding:0 20px 16px 20px;color:#111827;font-size:14px;line-height:1.6;">
                    <div><strong>Username:</strong> {username}</div>
                    <div><strong>Password:</strong> {password}</div>
                  </td>
                </tr>
                <tr>
                  <td align="center" style="padding:8px 20px 24px 20px;">
                    <a href="https://cartracker-6twv.onrender.com/login" style="display:inline-block;background:#009cb6;color:#ffffff;text-decoration:none;padding:10px 16px;border-radius:8px;font-size:14px;">Login</a>
                  </td>
                </tr>
                <tr>
                  <td style="padding:0 20px 24px 20px;color:#6b7280;font-size:12px;">Please change your password after first login.</td>
                </tr>
              </table>
            </td>
          </tr>
        </table>
      </body>
    </html>
    """
    msg.add_alternative(html, subtype="html")
    try:
        if use_tls:
            with smtplib.SMTP(host, port, timeout=15) as s:
                s.starttls()
                if user and pwd:
                    s.login(user, pwd)
                s.send_message(msg)
        else:
            with smtplib.SMTP_SSL(host, port, timeout=15) as s:
                if user and pwd:
                    s.login(user, pwd)
                s.send_message(msg)
    except Exception as e:
        error_details = f"{type(e).__name__}: {e}\nHost: {host}\nPort: {port}\nUser: {user}\nFrom: {from_addr}\nTo: {to_email}"
        try:
            (DEBUG_DIR / "mail_error.txt").write_text(error_details + "\n", encoding="utf-8")
        except Exception:
            pass
        raise  # Re-raise para mostrar erro ao utilizador

# Simple FX cache to avoid repeated HTTP calls
_FX_CACHE: Dict[str, Tuple[float, float]] = {}  # key "GBP->EUR" -> (rate, ts)
_URL_CACHE: Dict[str, Tuple[float, Dict[str, Any]]] = {}  # key normalized URL -> (ts, response payload)

# Ensure users table and seed initial admin on startup
try:
    # === Ensure default admin users exist ===
    def _ensure_default_users():
        """Create default users if they don't exist"""
        default_users = [
            {
                "username": "admin",
                "password": APP_PASSWORD,
                "first_name": "Filipe",
                "last_name": "Pacheco",
                "email": "carlpac82@hotmail.com",
                "mobile": "+351 964 805 750",
                "profile_picture": "/static/profiles/carlpac82.png",
                "is_admin": True
            },
            {
                "username": "carlpac82",
                "password": "Frederico.2025",
                "first_name": "Filipe",
                "last_name": "Pacheco",
                "email": "carlpac82@hotmail.com",
                "mobile": "+351 964 805 750",
                "profile_picture": "/static/profiles/carlpac82.png",
                "is_admin": True
            },
            {
                "username": "dprudente",
                "password": "dprudente",
                "first_name": "Daniell",
                "last_name": "Prudente",
                "email": "comercial.autoprudente@gmail.com",
                "mobile": "+351 911 747 478",
                "profile_picture": "/static/profiles/dprudente.jpg",
                "is_admin": False
            }
        ]
        
        try:
            with _db_lock:
                con = _db_connect()
                try:
                    for user in default_users:
                        cur = con.execute("SELECT id FROM users WHERE username=?", (user["username"],))
                        row = cur.fetchone()
                        if not row:
                            pw_hash = _hash_password(user["password"])
                            # Convert integers to boolean for PostgreSQL
                            is_admin_val = True if user.get("is_admin", 0) == 1 else False
                            enabled_val = True if user.get("enabled", 1) == 1 else False
                            con.execute(
                                "INSERT INTO users (username, password_hash, first_name, last_name, email, mobile, profile_picture_path, is_admin, enabled, created_at) VALUES (?,?,?,?,?,?,?,?,?,?)",
                                (
                                    user["username"],
                                    pw_hash,
                                    user["first_name"],
                                    user["last_name"],
                                    user["email"],
                                    user["mobile"],
                                    user["profile_picture"],
                                    is_admin_val,
                                    enabled_val,
                                    time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
                                )
                            )
                            print(f"[INIT] Created user: {user['username']}", file=sys.stderr)
                    con.commit()
                finally:
                    con.close()
        except Exception as e:
            print(f"[INIT] Error creating default users: {e}", file=sys.stderr)
    
    _ensure_default_users()
except Exception:
    pass

def _fx_rate_gbp_eur(timeout: float = 5.0) -> float:
    key = "GBP->EUR"
    now = time.time()
    cached = _FX_CACHE.get(key)
    if cached and now - cached[1] < 3600:
        return cached[0]
    try:
        r = requests.get(
            "https://api.exchangerate.host/latest",
            params={"base": "GBP", "symbols": "EUR"},
            timeout=timeout,
        )
        if r.status_code == 200:
            data = r.json()
            rate = float(data.get("rates", {}).get("EUR") or 0)
            if rate > 0:
                _FX_CACHE[key] = (rate, now)
                return rate
    except Exception:
        pass
    # conservative fallback
    return cached[0] if cached else 1.16

def _parse_amount(s: str) -> Optional[float]:
    try:
        m = re.search(r"([0-9][0-9\.,\s]*)", s or "")
        if not m:
            return None
        num = m.group(1).replace("\u00a0", "").replace(" ", "")
        has_comma = "," in num
        has_dot = "." in num
        
        # Formato europeu: 1.234,56 (ponto = milhares, v√≠rgula = decimais)
        if has_comma and has_dot:
            num = num.replace(".", "").replace(",", ".")
        # S√≥ v√≠rgula: 1234,56 ‚Üí 1234.56
        elif has_comma and not has_dot:
            num = num.replace(",", ".")
        # S√≥ ponto: pode ser milhares (1.234) ou decimais (12.34)
        # Se tem mais de 1 ponto OU ponto est√° a 3 d√≠gitos do fim ‚Üí √© separador de milhares
        elif has_dot and not has_comma:
            parts = num.split(".")
            # M√∫ltiplos pontos ‚Üí separador de milhares (1.234.567)
            if len(parts) > 2:
                num = "".join(parts)
            # Um ponto a 3 d√≠gitos do fim ‚Üí separador de milhares (1.234)
            elif len(parts) == 2 and len(parts[1]) == 3:
                num = "".join(parts)
            # Caso contr√°rio, √© decimal (12.34)
            # num j√° est√° correto
        
        v = float(num)
        return v
    except Exception:
        return None

def _format_eur(v: float) -> str:
    try:
        s = f"{v:,.2f}"
        s = s.replace(",", "_").replace(".", ",").replace("_", ".")
        return f"{s} ‚Ç¨"
    except Exception:
        return f"{v:.2f} ‚Ç¨"

def convert_items_gbp_to_eur(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    rate = _fx_rate_gbp_eur()
    out = []
    for it in items or []:
        price_txt = it.get("price") or ""
        if "¬£" in price_txt or re.search(r"\bGBP\b", price_txt, re.I):
            amt = _parse_amount(price_txt)
            if amt is not None:
                eur = amt * rate
                it = dict(it)
                it["price"] = _format_eur(eur)
                it["currency"] = "EUR"
        out.append(it)
    return out

# CarJet destination codes we target  
LOCATION_CODES = {
    # Albufeira: deixar vazio para o CarJet descobrir automaticamente
    # "albufeira": "ABF01",  # ABF01 n√£o funciona - testar sem c√≥digo
    # "albufeira cidade": "ABF01",
    "faro": "FAO01",
    "faro airport": "FAO01",
    "faro aeroporto": "FAO01",
    "faro aeroporto (fao)": "FAO01",
    "aeroporto de faro": "FAO01",
}

def init_db():
    """Initialize all database tables (works with both SQLite and PostgreSQL)"""
    
    def safe_create_index(conn, index_sql, index_name="index"):
        """Safely create index - won't crash if columns don't exist"""
        try:
            conn.execute(index_sql)
        except Exception as idx_err:
            logging.warning(f"Could not create {index_name}: {idx_err}")
            try:
                conn.rollback()
            except:
                pass
    
    with _db_lock:
        conn = _db_connect()  # Use _db_connect() instead of direct sqlite3.connect()
        try:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS price_snapshots (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  ts TEXT NOT NULL,
                  location TEXT NOT NULL,
                  pickup_date TEXT NOT NULL,
                  pickup_time TEXT NOT NULL,
                  days INTEGER NOT NULL,
                  supplier TEXT,
                  car TEXT,
                  price_text TEXT,
                  price_num REAL,
                  currency TEXT,
                  link TEXT
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_snapshots_q ON price_snapshots(location, days, ts)", "idx_snapshots_q")
            
            # Tabela para regras automatizadas de pre√ßos
            logging.info("üìã Creating automated_price_rules table (if not exists)...")
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS automated_price_rules (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  grupo TEXT NOT NULL,
                  month INTEGER,
                  day INTEGER,
                  config TEXT NOT NULL,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  UNIQUE(location, grupo, month, day)
                )
                """
            )
            
            # Check if table has data
            try:
                cursor = conn.execute("SELECT COUNT(*) FROM automated_price_rules")
                count = cursor.fetchone()[0]
                logging.info(f"‚úÖ automated_price_rules table ready - {count} existing rules")
            except Exception as check_err:
                logging.warning(f"‚ö†Ô∏è Could not count rules: {check_err}")
            
            # Tabela para estrat√©gias de pricing
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS pricing_strategies (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  grupo TEXT NOT NULL,
                  month INTEGER,
                  day INTEGER,
                  priority INTEGER NOT NULL DEFAULT 1,
                  strategy_type TEXT NOT NULL,
                  config TEXT NOT NULL,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_strategies ON pricing_strategies(location, grupo, month, day, priority)", "idx_strategies")
            
            # Tabela para hist√≥rico de pre√ßos automatizados
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS automated_prices_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  grupo TEXT NOT NULL,
                  dias INTEGER NOT NULL,
                  pickup_date TEXT NOT NULL,
                  auto_price REAL NOT NULL,
                  real_price REAL NOT NULL,
                  strategy_used TEXT,
                  strategy_details TEXT,
                  min_price_applied REAL,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  created_by TEXT,
                  source TEXT DEFAULT 'manual'
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_auto_prices_history ON automated_prices_history(location, grupo, pickup_date, created_at)", "idx_auto_prices_history")
            
            # Add source column to automated_prices_history if it doesn't exist (migration)
            try:
                conn.execute("ALTER TABLE automated_prices_history ADD COLUMN source TEXT DEFAULT 'manual'")
                conn.commit()
                logging.info("‚úÖ Added 'source' column to automated_prices_history table")
            except Exception as e:
                conn.rollback()  # CRITICAL for PostgreSQL - must rollback on error
                error_msg = str(e).lower()
                if 'duplicate column' in error_msg or 'already exists' in error_msg:
                    logging.info("‚ÑπÔ∏è Column 'source' already exists in automated_prices_history")
                else:
                    logging.error(f"‚ùå Failed to add 'source' column to automated_prices_history: {e}")
                pass
            
            # Tabela para logs do sistema (evitar perda em disco ef√™mero)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS system_logs (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  level TEXT NOT NULL,
                  message TEXT NOT NULL,
                  module TEXT,
                  function TEXT,
                  line_number INTEGER,
                  exception TEXT,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_system_logs ON system_logs(level, created_at)", "idx_system_logs")
            
            # Tabela para cache de dados (evitar perda em disco ef√™mero)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS cache_data (
                  key TEXT PRIMARY KEY,
                  value TEXT NOT NULL,
                  expires_at TEXT,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            
            # Tabela para uploads/ficheiros (evitar perda em disco ef√™mero)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS file_storage (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  filename TEXT NOT NULL,
                  filepath TEXT NOT NULL UNIQUE,
                  file_data BLOB NOT NULL,
                  content_type TEXT,
                  file_size INTEGER,
                  uploaded_by TEXT,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_file_storage ON file_storage(filepath, uploaded_by)", "idx_file_storage")
            
            # Tabela para hist√≥rico de exports (Way2Rentals, Abbycar, etc.)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS export_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  filename TEXT NOT NULL,
                  broker TEXT NOT NULL,
                  location TEXT NOT NULL,
                  period_start INTEGER,
                  period_end INTEGER,
                  month INTEGER NOT NULL,
                  year INTEGER NOT NULL,
                  month_name TEXT NOT NULL,
                  file_content TEXT NOT NULL,
                  file_size INTEGER,
                  exported_by TEXT,
                  export_date TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  last_downloaded TEXT
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_export_history ON export_history(broker, location, year, month, export_date)", "idx_export_history")
            
            # Tabela para AI Learning Data (substituir localStorage)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS ai_learning_data (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  grupo TEXT NOT NULL,
                  days INTEGER NOT NULL,
                  location TEXT NOT NULL,
                  original_price REAL,
                  new_price REAL NOT NULL,
                  timestamp TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  "user" TEXT DEFAULT 'admin'
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_ai_learning ON ai_learning_data(grupo, days, location, timestamp DESC)", "idx_ai_learning")
            
            # Tabela para User Settings (localStorage persistente)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS user_settings (
                  user_key TEXT NOT NULL,
                  setting_key TEXT NOT NULL,
                  setting_value TEXT NOT NULL,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  PRIMARY KEY (user_key, setting_key)
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_user_settings ON user_settings(user_key, updated_at DESC)", "idx_user_settings")
            
            # Tabela para Commercial Vans Pricing (C3, C4, C5)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS vans_pricing (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  c3_1day REAL DEFAULT 112.00,
                  c3_2days REAL DEFAULT 144.00,
                  c3_3days REAL DEFAULT 180.00,
                  c4_1day REAL DEFAULT 152.00,
                  c4_2days REAL DEFAULT 170.00,
                  c4_3days REAL DEFAULT 210.00,
                  c5_1day REAL DEFAULT 175.00,
                  c5_2days REAL DEFAULT 190.00,
                  c5_3days REAL DEFAULT 240.00,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_by TEXT DEFAULT 'admin'
                )
                """
            )
            
            # Tabela para Automated Price Rules
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS automated_price_rules (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  grupo TEXT NOT NULL,
                  month INTEGER NOT NULL,
                  day INTEGER NOT NULL,
                  strategy_type TEXT NOT NULL,
                  config TEXT NOT NULL,
                  priority INTEGER DEFAULT 1,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  UNIQUE(location, grupo, month, day, priority)
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_automated_rules ON automated_price_rules(location, grupo, month, day)", "idx_automated_rules")
            
            # Tabela para Price Automation Settings
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS price_automation_settings (
                  setting_key TEXT PRIMARY KEY,
                  setting_value TEXT NOT NULL,
                  setting_type TEXT DEFAULT 'string',
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            
            # Tabela para Custom Days Configuration
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS custom_days (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  days_array TEXT NOT NULL,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            
            # Tabela para OAuth Tokens (Gmail, etc)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS oauth_tokens (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  provider TEXT NOT NULL,
                  user_email TEXT NOT NULL,
                  access_token TEXT NOT NULL,
                  refresh_token TEXT,
                  expires_at INTEGER,
                  google_id TEXT,
                  user_name TEXT,
                  user_picture TEXT,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  UNIQUE(provider, user_email)
                )
                """
            )
            
            # Tabela para Price Validation Rules
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS price_validation_rules (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  rules_json TEXT NOT NULL,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_by TEXT DEFAULT 'admin'
                )
                """
            )
            
            # Tabela para Price History (vers√µes salvas)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS price_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  history_type TEXT NOT NULL,
                  year INTEGER NOT NULL,
                  month INTEGER NOT NULL,
                  location TEXT NOT NULL,
                  prices_data TEXT NOT NULL,
                  saved_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  saved_by TEXT DEFAULT 'admin'
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_price_history ON price_history(history_type, year, month, location, saved_at DESC)", "idx_price_history")
            
            # Tabela para Search History (hist√≥rico de pesquisas)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS search_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  start_date TEXT NOT NULL,
                  end_date TEXT NOT NULL,
                  days INTEGER NOT NULL,
                  results_count INTEGER,
                  min_price REAL,
                  max_price REAL,
                  avg_price REAL,
                  search_timestamp TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  "user" TEXT DEFAULT 'admin',
                  search_params TEXT
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_search_history ON search_history(location, start_date, search_timestamp DESC)", "idx_search_history")
            
            # Tabela para Automated Search History (hist√≥rico de pesquisas automatizadas)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS automated_search_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  location TEXT NOT NULL,
                  search_type TEXT NOT NULL,
                  search_date TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  month_key TEXT NOT NULL,
                  prices_data TEXT NOT NULL,
                  dias TEXT NOT NULL,
                  price_count INTEGER DEFAULT 0,
                  user_email TEXT,
                  supplier_data TEXT,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_automated_search_month ON automated_search_history(month_key, search_type, search_date DESC)", "idx_automated_search_month")
            
            # Tabela para Car Groups (grupos de carros parametrizados)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS car_groups (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  code TEXT UNIQUE NOT NULL,
                  name TEXT,
                  model TEXT,
                  brand TEXT,
                  category TEXT,
                  doors INTEGER,
                  seats INTEGER,
                  transmission TEXT,
                  luggage INTEGER,
                  photo_url TEXT,
                  enabled INTEGER DEFAULT 1,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_car_groups_name ON car_groups(LOWER(name))", "idx_car_groups_name")
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_car_groups_model ON car_groups(LOWER(model))", "idx_car_groups_model")
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_car_groups_code ON car_groups(code)", "idx_car_groups_code")
            
            # Tabela para Notification Rules (regras de notifica√ß√£o)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS notification_rules (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  rule_name TEXT NOT NULL,
                  rule_type TEXT NOT NULL,
                  condition_json TEXT NOT NULL,
                  action_json TEXT NOT NULL,
                  enabled INTEGER DEFAULT 1,
                  priority INTEGER DEFAULT 1,
                  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  created_by TEXT DEFAULT 'admin'
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_notification_rules ON notification_rules(enabled, priority, rule_type)", "idx_notification_rules")
            
            # Tabela para Notification History (hist√≥rico de notifica√ß√µes enviadas)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS notification_history (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  rule_id INTEGER,
                  notification_type TEXT NOT NULL,
                  recipient TEXT NOT NULL,
                  subject TEXT,
                  message TEXT NOT NULL,
                  sent_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                  status TEXT DEFAULT 'sent',
                  error_message TEXT
                )
                """
            )
            safe_create_index(conn, "CREATE INDEX IF NOT EXISTS idx_notification_history ON notification_history(sent_at DESC, status)", "idx_notification_history")
            
        finally:
            conn.commit()
            conn.close()

init_db()

# ============================================================
# HELPER FUNCTIONS - PERSIST√äNCIA EM DB (EVITAR DISCO EF√äMERO)
# ============================================================

def log_to_db(level: str, message: str, module: str = None, function: str = None, line_number: int = None, exception: str = None):
    """Salvar logs na base de dados em vez de ficheiros"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT INTO system_logs (level, message, module, function, line_number, exception)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (level, message, module, function, line_number, exception)
                )
                conn.commit()
            finally:
                conn.close()
    except Exception as e:
        # Fallback para print se DB falhar
        print(f"[{level}] {message}", file=sys.stderr, flush=True)

def save_to_cache(key: str, value: str, expires_in_seconds: int = None):
    """Salvar dados em cache na DB em vez de filesystem"""
    try:
        expires_at = None
        if expires_in_seconds:
            from datetime import datetime, timedelta, timezone
            expires_at = (datetime.now(timezone.utc) + timedelta(seconds=expires_in_seconds)).isoformat()
        
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO cache_data (key, value, expires_at, updated_at)
                    VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                    """,
                    (key, value, expires_at)
                )
                conn.commit()
            finally:
                conn.close()
    except Exception as e:
        log_to_db("ERROR", f"Failed to save cache: {str(e)}", "main", "save_to_cache")

def get_from_cache(key: str):
    """Obter dados do cache na DB"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT value, expires_at FROM cache_data 
                    WHERE key = ?
                    """,
                    (key,)
                )
                row = cursor.fetchone()
                
                if row:
                    value, expires_at = row
                    
                    # Verificar expira√ß√£o
                    if expires_at:
                        from datetime import datetime, timezone
                        expires_dt = datetime.fromisoformat(expires_at)
                        if datetime.now(timezone.utc) > expires_dt:
                            # Expirado, deletar
                            conn.execute("DELETE FROM cache_data WHERE key = ?", (key,))
                            conn.commit()
                            return None
                    
                    return value
                
                return None
            finally:
                conn.close()
    except Exception as e:
        log_to_db("ERROR", f"Failed to get cache: {str(e)}", "main", "get_from_cache")
        return None

def save_file_to_db(filename: str, filepath: str, file_data: bytes, content_type: str = None, uploaded_by: str = None):
    """Salvar ficheiro na base de dados em vez de filesystem"""
    try:
        file_size = len(file_data)
        
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO file_storage 
                    (filename, filepath, file_data, content_type, file_size, uploaded_by)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (filename, filepath, file_data, content_type, file_size, uploaded_by)
                )
                conn.commit()
                log_to_db("INFO", f"File saved to DB: {filepath} ({file_size} bytes)", "main", "save_file_to_db")
            finally:
                conn.close()
    except Exception as e:
        log_to_db("ERROR", f"Failed to save file to DB: {str(e)}", "main", "save_file_to_db", exception=str(e))
        raise

def save_search_to_history(location: str, start_date: str, end_date: str, days: int, results_count: int = 0, 
                           min_price: float = None, max_price: float = None, avg_price: float = None, 
                           user: str = "admin", search_params: str = None):
    """Salvar pesquisa no hist√≥rico"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # PostgreSQL e SQLite compatibility
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL - usar %s e "user" com aspas
                    cursor = conn.cursor()
                    cursor.execute(
                        """
                        INSERT INTO search_history 
                        (location, start_date, end_date, days, results_count, min_price, max_price, avg_price, "user", search_params)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """,
                        (location, start_date, end_date, days, results_count, min_price, max_price, avg_price, user, search_params)
                    )
                    cursor.close()
                else:
                    # SQLite - usar ?
                    conn.execute(
                        """
                        INSERT INTO search_history 
                        (location, start_date, end_date, days, results_count, min_price, max_price, avg_price, user, search_params)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (location, start_date, end_date, days, results_count, min_price, max_price, avg_price, user, search_params)
                    )
                conn.commit()
                logging.info(f"‚úÖ Search saved to history: {location}, {start_date}-{end_date}, {results_count} results")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Failed to save search history: {str(e)}")
        import traceback
        traceback.print_exc()

def send_notification(rule_id: int, notification_type: str, recipient: str, subject: str, message: str):
    """Enviar notifica√ß√£o e salvar no hist√≥rico"""
    try:
        # Enviar email
        if notification_type == "email":
            _send_notification_email(recipient, subject, message)
        
        # Salvar no hist√≥rico
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT INTO notification_history 
                    (rule_id, notification_type, recipient, subject, message, status)
                    VALUES (?, ?, ?, ?, ?, 'sent')
                    """,
                    (rule_id, notification_type, recipient, subject, message)
                )
                conn.commit()
                log_to_db("INFO", f"Notification sent: {notification_type} to {recipient}", "main", "send_notification")
            finally:
                conn.close()
    except Exception as e:
        # Salvar erro no hist√≥rico
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT INTO notification_history 
                    (rule_id, notification_type, recipient, subject, message, status, error_message)
                    VALUES (?, ?, ?, ?, ?, 'failed', ?)
                    """,
                    (rule_id, notification_type, recipient, subject, message, str(e))
                )
                conn.commit()
            finally:
                conn.close()
        log_to_db("ERROR", f"Failed to send notification: {str(e)}", "main", "send_notification")

def _get_gmail_credentials():
    """Load complete Gmail OAuth credentials from database with refresh capability"""
    from google.oauth2.credentials import Credentials
    
    # Load token from database
    access_token = None
    refresh_token = None
    
    with _db_lock:
        conn = _db_connect()
        try:
            cursor = conn.execute(
                "SELECT access_token, refresh_token FROM oauth_tokens WHERE provider = 'google' ORDER BY updated_at DESC LIMIT 1"
            )
            row = cursor.fetchone()
            if row:
                access_token = row[0]
                refresh_token = row[1] if len(row) > 1 else None
                logging.info("‚úÖ OAuth tokens loaded from database")
        finally:
            conn.close()
    
    if not access_token:
        logging.error("‚ùå No OAuth access token found in database")
        return None
    
    # CRITICAL: Check if refresh_token exists
    if not refresh_token or refresh_token.strip() == '':
        logging.error("‚ùå No refresh_token found in database - USER MUST RECONNECT GMAIL")
        logging.error("   ‚Üí Go to Admin Settings ‚Üí Email ‚Üí Click 'Connect Gmail' again")
        return None
    
    # Load OAuth client credentials from environment
    client_id = os.getenv('GOOGLE_CLIENT_ID')
    client_secret = os.getenv('GOOGLE_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        logging.error("‚ùå GOOGLE_CLIENT_ID or GOOGLE_CLIENT_SECRET not configured")
        return None
    
    # Create complete credentials object with refresh capability
    credentials = Credentials(
        token=access_token,
        refresh_token=refresh_token,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=client_id,
        client_secret=client_secret,
        scopes=['https://www.googleapis.com/auth/gmail.send']
    )
    
    logging.info("‚úÖ Complete Gmail credentials created with refresh capability")
    return credentials

def _send_notification_email(to_email: str, subject: str, message: str):
    """Enviar email de notifica√ß√£o via Gmail OAuth"""
    try:
        from googleapiclient.discovery import build
        import base64
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        
        # Get complete Gmail credentials
        credentials = _get_gmail_credentials()
        
        if not credentials:
            # Fallback para SMTP se n√£o houver credenciais OAuth
            logging.warning("‚ö†Ô∏è No OAuth credentials available, trying SMTP fallback")
            _send_notification_email_smtp(to_email, subject, message)
            return
        
        # Criar email HTML
        email_message = MIMEMultipart('alternative')
        email_message['to'] = to_email
        email_message['subject'] = subject
        
        # Se a mensagem j√° √© HTML, usar diretamente
        if message.strip().startswith('<!DOCTYPE') or message.strip().startswith('<html'):
            html_part = MIMEText(message, 'html')
        else:
            # Converter texto simples para HTML
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
            </head>
            <body style="font-family: 'Segoe UI', sans-serif; background: #f8fafc; padding: 20px;">
                <div style="max-width: 600px; margin: 0 auto; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                    <div style="background: linear-gradient(135deg, #009cb6 0%, #007a91 100%); padding: 30px; text-align: center;">
                        <h1 style="margin: 0; color: white; font-size: 24px;">üîî Notifica√ß√£o Auto Prudente</h1>
                    </div>
                    <div style="padding: 30px;">
                        <div style="color: #1e293b; font-size: 16px; line-height: 1.6; white-space: pre-wrap;">{message}</div>
                    </div>
                    <div style="background: #f8fafc; padding: 20px; text-align: center; border-top: 1px solid #e2e8f0;">
                        <p style="margin: 0; font-size: 12px; color: #94a3b8;">
                            Auto Prudente ¬© 2025 - Sistema de Monitoriza√ß√£o de Pre√ßos
                        </p>
                    </div>
                </div>
            </body>
            </html>
            """
            html_part = MIMEText(html_content, 'html')
        
        email_message.attach(html_part)
        
        # Enviar via Gmail API (credentials j√° carregadas com refresh capability)
        service = build('gmail', 'v1', credentials=credentials)
        
        raw_message = base64.urlsafe_b64encode(email_message.as_bytes()).decode()
        send_message = service.users().messages().send(
            userId='me',
            body={'raw': raw_message}
        ).execute()
        
        logging.info(f"‚úÖ Notification email sent via Gmail OAuth to {to_email}")
        
    except Exception as e:
        logging.error(f"‚ùå Failed to send notification email via OAuth: {str(e)}")
        # Tentar SMTP como fallback
        try:
            _send_notification_email_smtp(to_email, subject, message)
        except Exception as smtp_error:
            logging.error(f"‚ùå SMTP fallback also failed: {str(smtp_error)}")
            raise

def _send_notification_email_smtp(to_email: str, subject: str, message: str):
    """Enviar email de notifica√ß√£o via SMTP (fallback)"""
    host = _get_setting("smtp_host", "").strip()
    port = int(_get_setting("smtp_port", "587") or 587)
    user = _get_setting("smtp_username", "").strip()
    pwd = _get_setting("smtp_password", "").strip()
    from_addr = _get_setting("smtp_from", "no-reply@example.com").strip()
    use_tls_val = _get_setting("smtp_tls", "true")
    use_tls = str(use_tls_val).lower() in ("1", "true", "yes", "y", "on")
    
    if not host or not to_email:
        raise Exception("Missing SMTP_HOST or recipient")
    
    msg = EmailMessage()
    msg["Subject"] = subject
    msg["From"] = from_addr
    msg["To"] = to_email
    msg.set_content(message)
    
    if use_tls:
        with smtplib.SMTP(host, port, timeout=15) as s:
            s.starttls()
            if user and pwd:
                s.login(user, pwd)
            s.send_message(msg)
    else:
        with smtplib.SMTP_SSL(host, port, timeout=15) as s:
            if user and pwd:
                s.login(user, pwd)
            s.send_message(msg)

def get_file_from_db(filepath: str):
    """Obter ficheiro da base de dados"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT filename, file_data, content_type, file_size FROM file_storage 
                    WHERE filepath = ?
                    """,
                    (filepath,)
                )
                row = cursor.fetchone()
                
                if row:
                    return {
                        "filename": row[0],
                        "data": row[1],
                        "content_type": row[2],
                        "size": row[3]
                    }
                
                return None
            finally:
                conn.close()
    except Exception as e:
        log_to_db("ERROR", f"Failed to get file from DB: {str(e)}", "main", "get_file_from_db")
        return None

def cleanup_expired_cache():
    """Limpar cache expirado"""
    try:
        from datetime import datetime, timezone
        now = datetime.now(timezone.utc).isoformat()
        
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "DELETE FROM cache_data WHERE expires_at IS NOT NULL AND expires_at < ?",
                    (now,)
                )
                deleted = cursor.rowcount
                conn.commit()
                
                if deleted > 0:
                    log_to_db("INFO", f"Cleaned up {deleted} expired cache entries", "main", "cleanup_expired_cache")
            finally:
                conn.close()
    except Exception as e:
        log_to_db("ERROR", f"Failed to cleanup cache: {str(e)}", "main", "cleanup_expired_cache")


IDLE_TIMEOUT_SECONDS = 30 * 60  # 30 minutes

@app.get("/healthz")
async def healthz():
    return JSONResponse({"ok": True})

@app.get("/debug/test-group")
async def debug_test_group():
    """Endpoint de teste para verificar se campo group funciona"""
    test_items = [
        {"car": "Test Car 1", "category": "MINI 5 Portas", "price": "10 ‚Ç¨", "supplier": "Test", "transmission": "Manual", "photo": "", "link": ""},
        {"car": "Test Car 2", "category": "7 lugares", "price": "20 ‚Ç¨", "supplier": "Test", "transmission": "Manual", "photo": "", "link": ""},
        {"car": "Test Car 3", "category": "9 Seater", "price": "30 ‚Ç¨", "supplier": "Test", "transmission": "Manual", "photo": "", "link": ""},
    ]
    result = normalize_and_sort(test_items, None)
    return JSONResponse({"ok": True, "items": result})

def require_auth(request: Request):
    # Allow internal requests from scheduler
    if request.headers.get("X-Internal-Request") == "scheduler":
        return  # Bypass authentication for internal scheduler requests
    
    if not request.session.get("auth", False):
        raise HTTPException(status_code=401, detail="Unauthorized")
    # Enforce inactivity timeout
    try:
        now = int(datetime.now(timezone.utc).timestamp())
        last = int(request.session.get("last_active_ts", 0))
        if last and now - last > IDLE_TIMEOUT_SECONDS:
            request.session.clear()
            raise HTTPException(status_code=401, detail="Session expired")
        # update last activity timestamp
        request.session["last_active_ts"] = now
    except Exception:
        # if any parsing error, refresh the timestamp anyway
        request.session["last_active_ts"] = int(datetime.now(timezone.utc).timestamp())

def require_admin(request: Request):
    require_auth(request)
    if not request.session.get("is_admin", False):
        raise HTTPException(status_code=403, detail="Forbidden")

@app.get("/api/current-user")
async def get_current_user(request: Request):
    """Retorna informa√ß√µes do utilizador logado"""
    require_auth(request)
    
    username = request.session.get("username", "")
    
    # Buscar nome completo do utilizador na base de dados
    with _db_lock:
        conn = _db_connect()
        try:
            cursor = conn.execute(
                "SELECT first_name, last_name FROM users WHERE username = ?",
                (username,)
            )
            row = cursor.fetchone()
            
            if row and row[0] and row[1]:
                full_name = f"{row[0]} {row[1]}".strip()
            elif row and row[0]:
                full_name = row[0].strip()
            else:
                full_name = username
        except:
            full_name = username
        finally:
            conn.close()
    
    return {
        "username": username,
        "full_name": full_name,
        "is_admin": request.session.get("is_admin", False)
    }

@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    if request.session.get("auth"):
        return RedirectResponse(url="/", status_code=HTTP_303_SEE_OTHER)
    return templates.TemplateResponse("login.html", {"request": request, "error": None})

@app.post("/login")
async def login_action(request: Request, username: str = Form(...), password: str = Form(...)):
    try:
        u = (username or "").strip()
        p = (password or "").strip()
        try:
            with (DEBUG_DIR / "login_trace.txt").open("a", encoding="utf-8") as f:
                f.write(f"attempt {datetime.now(timezone.utc).isoformat()} user={u}\n")
        except Exception:
            pass
        # Check DB users
        is_admin_flag = False
        ok = False
        try:
            with _db_lock:
                con = _db_connect()
                try:
                    cur = con.execute("SELECT id, password_hash, is_admin, enabled FROM users WHERE username=?", (u,))
                    row = cur.fetchone()
                    if row and row[3]:
                        ok = _verify_password(p, row[1])
                        is_admin_flag = bool(row[2])
                finally:
                    con.close()
        except Exception:
            ok = False
        # Fallback to env user for safety
        if not ok and u == APP_USERNAME and p == APP_PASSWORD:
            ok = True
            is_admin_flag = True
        if ok:
            try:
                request.session["auth"] = True
                request.session["username"] = u
                request.session["is_admin"] = bool(is_admin_flag)
                request.session["last_active_ts"] = int(datetime.now(timezone.utc).timestamp())
                log_activity(request, "login_success", details="", username=u)
                try:
                    with (DEBUG_DIR / "login_trace.txt").open("a", encoding="utf-8") as f:
                        f.write(f"success {datetime.now(timezone.utc).isoformat()} user={u}\n")
                except Exception:
                    pass
                return RedirectResponse(url="/", status_code=HTTP_303_SEE_OTHER)
            except Exception as e:
                import sys
                print(f"[LOGIN ERROR] Session error: {e}", file=sys.stderr, flush=True)
                return templates.TemplateResponse("login.html", {"request": request, "error": f"Login session error: {str(e)}"})
        try:
            with (DEBUG_DIR / "login_trace.txt").open("a", encoding="utf-8") as f:
                f.write(f"invalid {datetime.now(timezone.utc).isoformat()} user={u}\n")
        except Exception:
            pass
        log_activity(request, "login_failure", details="", username=u)
        return templates.TemplateResponse("login.html", {"request": request, "error": "Invalid credentials"})
    except Exception:
        try:
            (DEBUG_DIR / "login_error.txt").write_text(_tb.format_exc(), encoding="utf-8")
        except Exception:
            pass
        log_activity(request, "login_exception", details="see login_error.txt")
        return templates.TemplateResponse("login.html", {"request": request, "error": "Login failed. Please try again."})

@app.post("/logout")
async def logout_action(request: Request):
    try:
        log_activity(request, "logout")
    except Exception:
        pass
    request.session.clear()
    return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)


@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    # load current user profile for greeting
    user_ctx = None
    try:
        uname = request.session.get("username")
        if uname:
            user_ctx = _get_user_by_username(uname)
    except Exception:
        user_ctx = None
    
    # Load supplier logos for preloading
    supplier_logos = []
    try:
        conn = _db_connect()
        try:
            rows = conn.execute("SELECT DISTINCT logo_path FROM suppliers WHERE logo_path IS NOT NULL AND active = 1").fetchall()
            supplier_logos = [row[0] for row in rows if row[0]]
        finally:
            conn.close()
    except Exception:
        # Suppliers table doesn't exist yet, skip logo preloading
        pass
    
    # FORCE NO CACHE - prevent browser from caching HTML/JS
    response = templates.TemplateResponse("index.html", {
        "request": request, 
        "current_user": user_ctx,
        "supplier_logos": supplier_logos
    })
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response

@app.get("/admin", response_class=HTMLResponse)
async def admin_root(request: Request, section: str = None):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # Get current user
    current_user = _get_current_user_from_session(request)
    
    return templates.TemplateResponse("settings_dashboard.html", {
        "request": request,
        "current_user": current_user,
        "section": section or "users"
    })

@app.get("/price-history", response_class=HTMLResponse)
async def price_history(request: Request):
    """P√°gina de hist√≥rico e gr√°ficos de pre√ßos"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # Get current user
    current_user = _get_current_user_from_session(request)
    
    return templates.TemplateResponse("price_history.html", {
        "request": request,
        "current_user": current_user
    })

@app.get("/test_abbycar_download.html", response_class=HTMLResponse)
async def test_abbycar_download(request: Request):
    """P√°gina de teste para debug do download Abbycar"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("test_abbycar_download.html", {
        "request": request
    })

@app.get("/price-automation", response_class=HTMLResponse)
async def price_automation(request: Request):
    """P√°gina de automa√ß√£o de pre√ßos com upload de Excel"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # Get current user
    current_user = _get_current_user_from_session(request)
    
    return templates.TemplateResponse("price_automation.html", {
        "request": request,
        "current_user": current_user
    })

@app.get("/price-automation/fill", response_class=HTMLResponse)
async def price_automation_fill(request: Request):
    """P√°gina para preencher pre√ßos do CarJet automaticamente"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # Get current user
    current_user = _get_current_user_from_session(request)
    
    return templates.TemplateResponse("price_automation_fill.html", {
        "request": request,
        "current_user": current_user
    })

@app.get("/damage-report", response_class=HTMLResponse)
async def damage_report_page(request: Request):
    """P√°gina de Damage Report"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # Get current user
    current_user = _get_current_user_from_session(request)
    
    return templates.TemplateResponse("damage_report.html", {
        "request": request,
        "current_user": current_user
    })

@app.get("/damage-report-mapper", response_class=HTMLResponse)
async def damage_report_mapper_page(request: Request):
    """P√°gina dedicada para mapeamento de campos PDF"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("damage_report_mapper_only.html", {
        "request": request
    })

@app.get("/rental-agreement-mapper", response_class=HTMLResponse)
async def rental_agreement_mapper_page(request: Request):
    """P√°gina dedicada para mapeamento de campos do Rental Agreement"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("rental_agreement_mapper.html", {
        "request": request
    })

@app.get("/rental-agreement-debug", response_class=HTMLResponse)
async def rental_agreement_debug_page(request: Request):
    """P√°gina de debug para ver linhas numeradas do PDF"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("ra_debug_lines.html", {
        "request": request
    })

@app.get("/vehicle-damage-test", response_class=HTMLResponse)
async def vehicle_damage_test_page(request: Request):
    """üöó AI Vehicle Damage Detection Test Page - FREE (No API costs)"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("vehicle_damage_test.html", {
        "request": request
    })

# --- Admin: environment summary and adjustment preview ---
@app.get("/admin/env-summary")
async def admin_env_summary(request: Request):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    try:
        cj_pct, cj_off = _get_carjet_adjustment()
        website_pct = float(_get_setting("website_pct", "14"))
        data = {
            "CARJET_PRICE_ADJUSTMENT_PCT": cj_pct,
            "CARJET_PRICE_OFFSET_EUR": cj_off,
            "website_pct": website_pct,
            "PRICES_CACHE_TTL_SECONDS": PRICES_CACHE_TTL_SECONDS,
            "BULK_CONCURRENCY": BULK_CONCURRENCY,
            "BULK_MAX_RETRIES": BULK_MAX_RETRIES,
            "GLOBAL_FETCH_RPS": GLOBAL_FETCH_RPS,
        }
        return JSONResponse({"ok": True, "env": data})
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/admin/files")
async def admin_list_files(request: Request):
    """Listar todos os ficheiros guardados na base de dados"""
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT id, filename, filepath, content_type, file_size, uploaded_by, created_at
                    FROM file_storage
                    ORDER BY created_at DESC
                    LIMIT 100
                    """
                )
                files = []
                for row in cursor.fetchall():
                    files.append({
                        "id": row[0],
                        "filename": row[1],
                        "filepath": row[2],
                        "content_type": row[3],
                        "file_size": row[4],
                        "uploaded_by": row[5],
                        "created_at": row[6]
                    })
                return JSONResponse({"ok": True, "files": files, "count": len(files)})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/admin/files/{file_id}")
async def admin_get_file(request: Request, file_id: int):
    """Recuperar ficheiro da base de dados"""
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT filename, file_data, content_type
                    FROM file_storage
                    WHERE id = ?
                    """,
                    (file_id,)
                )
                row = cursor.fetchone()
                if not row:
                    return JSONResponse({"ok": False, "error": "File not found"}, status_code=404)
                
                from starlette.responses import Response
                return Response(
                    content=row[1],
                    media_type=row[2] or "application/octet-stream",
                    headers={"Content-Disposition": f'attachment; filename="{row[0]}"'}
                )
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/admin/adjust-preview")
async def admin_adjust_preview(request: Request, price: str, url: str):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    try:
        # determine if adjustment applies
        is_cj = False
        try:
            from urllib.parse import urlparse as _parse
            is_cj = _parse(url).netloc.endswith("carjet.com")
        except Exception:
            is_cj = False
        pct, off = _get_carjet_adjustment()
        amt = _parse_amount(price)
        if amt is None:
            return JSONResponse({"ok": False, "error": "Invalid price format"}, status_code=400)
        adjusted = amt
        if is_cj and (pct != 0 or off != 0):
            adjusted = amt * (1.0 + (pct/100.0)) + off
        return _no_store_json({
            "ok": True,
            "input": {"price": price, "url": url},
            "env": {"pct": pct, "offset": off},
            "is_carjet": is_cj,
            "amount": amt,
            "adjusted_amount": adjusted,
            "adjusted_price": _format_eur(adjusted),
        })
    except Exception as e:
        return _no_store_json({"ok": False, "error": str(e)}, status_code=500)

@app.get("/admin/settings", response_class=HTMLResponse)
async def admin_settings_page(request: Request):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    cj_pct, cj_off = _get_carjet_adjustment()
    abbycar_pct = _get_abbycar_adjustment()
    abbycar_low_deposit_pct = _get_abbycar_low_deposit_adjustment()
    abbycar_low_deposit_enabled = _get_abbycar_low_deposit_enabled()
    website_pct = float(_get_setting("website_pct", "14"))
    
    # Carregar configura√ß√µes SMTP da base de dados
    smtp_host = _get_setting("smtp_host", "")
    smtp_port = _get_setting("smtp_port", "587")
    smtp_username = _get_setting("smtp_username", "")
    smtp_password = _get_setting("smtp_password", "")
    smtp_from = _get_setting("smtp_from", "")
    smtp_tls = _get_setting("smtp_tls", "1") == "1"
    
    return templates.TemplateResponse("admin_settings.html", {
        "request": request, 
        "carjet_pct": cj_pct, 
        "carjet_off": cj_off, 
        "abbycar_pct": abbycar_pct,
        "abbycar_low_deposit_pct": abbycar_low_deposit_pct,
        "abbycar_low_deposit_enabled": abbycar_low_deposit_enabled,
        "website_pct": website_pct,
        "smtp_host": smtp_host,
        "smtp_port": smtp_port,
        "smtp_username": smtp_username,
        "smtp_password": smtp_password,
        "smtp_from": smtp_from,
        "smtp_tls": smtp_tls,
        "saved": False, 
        "error": None
    })

@app.post("/admin/settings", response_class=HTMLResponse)
async def admin_settings_save(
    request: Request, 
    carjet_pct: str = Form(""), 
    carjet_off: str = Form(""), 
    abbycar_pct: str = Form(""), 
    abbycar_low_deposit_pct: str = Form(""), 
    abbycar_low_deposit_enabled: str = Form(""),
    website_pct: str = Form(""),
    smtp_host: str = Form(""),
    smtp_port: str = Form("587"),
    smtp_username: str = Form(""),
    smtp_password: str = Form(""),
    smtp_from: str = Form(""),
    smtp_tls: str = Form("")
):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    err = None
    try:
        pct_val = float((carjet_pct or "0").replace(",", "."))
        off_val = float((carjet_off or "0").replace(",", "."))
        abbycar_pct_val = float((abbycar_pct or "3").replace(",", "."))
        abbycar_low_deposit_pct_val = float((abbycar_low_deposit_pct or "0").replace(",", "."))
        abbycar_low_deposit_enabled_val = "1" if abbycar_low_deposit_enabled == "1" else "0"
        website_pct_val = float((website_pct or "0").replace(",", "."))
        
        # Guardar configura√ß√µes de pre√ßos
        _set_setting("carjet_pct", str(pct_val))
        _set_setting("carjet_off", str(off_val))
        _set_setting("abbycar_pct", str(abbycar_pct_val))
        _set_setting("abbycar_low_deposit_pct", str(abbycar_low_deposit_pct_val))
        _set_setting("abbycar_low_deposit_enabled", abbycar_low_deposit_enabled_val)
        _set_setting("website_pct", str(website_pct_val))
        
        # Guardar configura√ß√µes SMTP na base de dados (persistente)
        _set_setting("smtp_host", smtp_host.strip())
        _set_setting("smtp_port", smtp_port.strip())
        _set_setting("smtp_username", smtp_username.strip())
        _set_setting("smtp_password", smtp_password.strip())
        _set_setting("smtp_from", smtp_from.strip())
        _set_setting("smtp_tls", "1" if smtp_tls == "1" else "0")
        
        cj_pct, cj_off = pct_val, off_val
        abbycar_pct_result = abbycar_pct_val
        abbycar_low_deposit_pct_result = abbycar_low_deposit_pct_val
        abbycar_low_deposit_enabled_result = abbycar_low_deposit_enabled_val == "1"
        website_pct_result = website_pct_val
        smtp_host_result = smtp_host.strip()
        smtp_port_result = smtp_port.strip()
        smtp_username_result = smtp_username.strip()
        smtp_password_result = smtp_password.strip()
        smtp_from_result = smtp_from.strip()
        smtp_tls_result = smtp_tls == "1"
    except Exception as e:
        err = str(e)
        cj_pct, cj_off = _get_carjet_adjustment()
        abbycar_pct_result = _get_abbycar_adjustment()
        abbycar_low_deposit_pct_result = _get_abbycar_low_deposit_adjustment()
        abbycar_low_deposit_enabled_result = _get_abbycar_low_deposit_enabled()
        website_pct_result = float(_get_setting("website_pct", "14"))
        smtp_host_result = _get_setting("smtp_host", "")
        smtp_port_result = _get_setting("smtp_port", "587")
        smtp_username_result = _get_setting("smtp_username", "")
        smtp_password_result = _get_setting("smtp_password", "")
        smtp_from_result = _get_setting("smtp_from", "")
        smtp_tls_result = _get_setting("smtp_tls", "1") == "1"
    return templates.TemplateResponse("admin_settings.html", {
        "request": request, 
        "carjet_pct": cj_pct, 
        "carjet_off": cj_off, 
        "abbycar_pct": abbycar_pct_result,
        "abbycar_low_deposit_pct": abbycar_low_deposit_pct_result,
        "abbycar_low_deposit_enabled": abbycar_low_deposit_enabled_result,
        "website_pct": website_pct_result,
        "smtp_host": smtp_host_result,
        "smtp_port": smtp_port_result,
        "smtp_username": smtp_username_result,
        "smtp_password": smtp_password_result,
        "smtp_from": smtp_from_result,
        "smtp_tls": smtp_tls_result,
        "saved": err is None, 
        "error": err
    })

@app.post("/admin/users/{user_id}/toggle-enabled")
async def admin_users_toggle_enabled(request: Request, user_id: int):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    with _db_lock:
        con = _db_connect()
        try:
            cur = con.execute("SELECT enabled FROM users WHERE id=?", (user_id,))
            r = cur.fetchone()
            if not r:
                raise HTTPException(status_code=404, detail="Not found")
            new_val = 0 if int(r[0] or 0) else 1
            con.execute("UPDATE users SET enabled=? WHERE id=?", (new_val, user_id))
            con.commit()
        finally:
            con.close()
    try:
        log_activity(request, "admin_edit_user", details=f"user_id={user_id}")
    except Exception:
        pass
    return RedirectResponse(url="/admin/users", status_code=HTTP_303_SEE_OTHER)

@app.post("/admin/users/{user_id}/reset-password")
async def admin_users_reset_password(request: Request, user_id: int):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    gen_pw = secrets.token_urlsafe(8)
    pw_hash = _hash_password(gen_pw)
    to_email = None
    username = None
    with _db_lock:
        con = _db_connect()
        try:
            cur = con.execute("SELECT username, email FROM users WHERE id=?", (user_id,))
            r = cur.fetchone()
            if not r:
                raise HTTPException(status_code=404, detail="Not found")
            username = r[0]
            to_email = (r[1] or "").strip()
            con.execute("UPDATE users SET password_hash=? WHERE id=?", (pw_hash, user_id))
            con.commit()
        finally:
            con.close()
    try:
        if to_email:
            _send_creds_email(to_email, username or "", gen_pw)
    except Exception:
        pass
    try:
        log_activity(request, "admin_reset_password", details=f"user_id={user_id}")
    except Exception:
        pass
    return RedirectResponse(url="/admin/users", status_code=HTTP_303_SEE_OTHER)

@app.get("/admin/users/{user_id}/edit", response_class=HTMLResponse)
async def admin_users_edit(request: Request, user_id: int):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    with _db_lock:
        con = _db_connect()
        try:
            cur = con.execute("SELECT id, username, first_name, last_name, email, mobile, profile_picture_path, is_admin, enabled FROM users WHERE id=?", (user_id,))
            r = cur.fetchone()
            if not r:
                raise HTTPException(status_code=404, detail="Not found")
            u = {
                "id": r[0], "username": r[1], "first_name": r[2] or "", "last_name": r[3] or "",
                "email": r[4] or "", "mobile": r[5] or "", "profile_picture_path": r[6] or "",
                "is_admin": bool(r[7]), "enabled": bool(r[8])
            }
        finally:
            con.close()
    return templates.TemplateResponse("admin_edit_user.html", {"request": request, "u": u, "error": None})

@app.post("/admin/users/{user_id}/edit")
async def admin_users_edit_post(
    request: Request,
    user_id: int,
    first_name: str = Form(""),
    last_name: str = Form(""),
    mobile: str = Form(""),
    email: str = Form(""),
    is_admin: str = Form("0"),
    enabled: str = Form("1"),
    new_password: str = Form(""),
    picture: Optional[UploadFile] = File(None),
):
    import sys
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    try:
        pic_data = None
        pic_path = None
        if picture and picture.filename:
            # Guardar foto na base de dados como BLOB
            pic_data = await picture.read()
            print(f"[UPLOAD] üì∏ Profile picture uploaded: {len(pic_data)} bytes", file=sys.stderr, flush=True)
            # Manter path para compatibilidade (usar ID do user)
            pic_path = f"/api/profile-picture/{user_id}"
        
        with _db_lock:
            con = _db_connect()
            try:
                # Converter para boolean (PostgreSQL) ou integer (SQLite)
                is_admin_val = True if is_admin in ("1","true","on") else False
                enabled_val = True if enabled in ("1","true","on") else False
                
                if pic_data:
                    print(f"[UPLOAD] üíæ Saving to DB: user_id={user_id}, blob_size={len(pic_data)}", file=sys.stderr, flush=True)
                    con.execute(
                        "UPDATE users SET first_name=?, last_name=?, mobile=?, email=?, profile_picture_path=?, profile_picture_data=?, is_admin=?, enabled=? WHERE id=?",
                        (first_name, last_name, mobile, email, pic_path, pic_data, is_admin_val, enabled_val, user_id)
                    )
                else:
                    con.execute(
                        "UPDATE users SET first_name=?, last_name=?, mobile=?, email=?, is_admin=?, enabled=? WHERE id=?",
                        (first_name, last_name, mobile, email, is_admin_val, enabled_val, user_id)
                    )
                # Optional password change
                if new_password and new_password.strip():
                    pw_hash = _hash_password(new_password.strip())
                    con.execute("UPDATE users SET password_hash=? WHERE id=?", (pw_hash, user_id))
                con.commit()
                print(f"[UPLOAD] ‚úÖ User {user_id} updated successfully", file=sys.stderr, flush=True)
            finally:
                con.close()
        return RedirectResponse(url="/admin/users", status_code=HTTP_303_SEE_OTHER)
    except Exception as e:
        print(f"[UPLOAD] ‚ùå Error updating user {user_id}: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/admin/users/{user_id}/delete")
async def admin_users_delete(request: Request, user_id: int):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    # prevent deleting self
    current_username = request.session.get("username")
    with _db_lock:
        con = _db_connect()
        try:
            cur = con.execute("SELECT username FROM users WHERE id=?", (user_id,))
            r = cur.fetchone()
            if not r:
                raise HTTPException(status_code=404, detail="Not found")
            if r[0] == current_username:
                raise HTTPException(status_code=400, detail="Cannot delete own account")
            con.execute("DELETE FROM users WHERE id=?", (user_id,))
            con.commit()
        finally:
            con.close()
    return RedirectResponse(url="/admin/users", status_code=HTTP_303_SEE_OTHER)

# --- Admin UI ---
@app.get("/test/carjet-mobile", response_class=HTMLResponse)
async def test_carjet_mobile(request: Request):
    """CarJet Mobile Scraping Test Page"""
    return templates.TemplateResponse("carjet_mobile_test.html", {"request": request})

@app.get("/test/mobile-scraping-live", response_class=HTMLResponse)
async def test_mobile_scraping_live(request: Request):
    """Live Mobile Scraping Test with Real API"""
    return templates.TemplateResponse("test_mobile_scraping_live.html", {"request": request})

@app.post("/api/test-mobile-scraping")
async def test_mobile_scraping(request: Request):
    """Test mobile scraping endpoint"""
    import time
    import httpx
    from datetime import datetime, timedelta
    
    try:
        data = await request.json()
        location = data.get('location', 'albufeira')
        days = int(data.get('days', 3))
        
        # Build mobile URL
        today = datetime.now()
        pickup = today + timedelta(days=7)
        dropoff = pickup + timedelta(days=days)
        
        location_codes = {
            'albufeira': 'albufeira-pt',
            'faro-airport': 'faro-airport-pt'
        }
        
        pickup_str = pickup.strftime('%Y-%m-%d')
        dropoff_str = dropoff.strftime('%Y-%m-%d')
        loc_code = location_codes.get(location, 'albufeira-pt')
        
        mobile_url = f"https://m.carjet.com/en/car-hire/{loc_code}/{pickup_str}/{dropoff_str}/"
        
        # Mobile user agent
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        start_time = time.time()
        
        # Fetch mobile page
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(mobile_url, headers=headers)
            html = response.text
        
        response_time = int((time.time() - start_time) * 1000)
        
        # Simple parsing to count vehicles (we'll improve this)
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        
        # Try to find vehicle cards (adjust selectors based on actual mobile HTML)
        vehicles = []
        vehicle_cards = soup.find_all(['div', 'article'], class_=lambda x: x and ('car' in x.lower() or 'vehicle' in x.lower() or 'result' in x.lower()))
        
        for card in vehicle_cards[:5]:  # Get first 5 as sample
            try:
                # Try to extract basic info
                name_elem = card.find(['h2', 'h3', 'h4', 'span'], class_=lambda x: x and ('name' in x.lower() or 'title' in x.lower() or 'model' in x.lower()))
                price_elem = card.find(['span', 'div'], class_=lambda x: x and ('price' in x.lower() or 'cost' in x.lower()))
                
                vehicle = {
                    'name': name_elem.get_text(strip=True) if name_elem else 'Unknown',
                    'price': price_elem.get_text(strip=True) if price_elem else 'N/A'
                }
                vehicles.append(vehicle)
            except:
                continue
        
        return {
            "success": True,
            "url": mobile_url,
            "vehicleCount": len(vehicle_cards),
            "responseTime": response_time,
            "vehicles": vehicles,
            "htmlLength": len(html),
            "statusCode": response.status_code
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.get("/fix-schema", response_class=HTMLResponse)
async def fix_schema_page(request: Request):
    """Emergency page to fix PostgreSQL schema"""
    return templates.TemplateResponse("fix_schema.html", {"request": request})

@app.get("/admin/backup", response_class=HTMLResponse)
async def admin_backup(request: Request):
    """Backup & Restore page"""
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    return templates.TemplateResponse("admin_backup.html", {"request": request})

@app.get("/admin/users", response_class=HTMLResponse)
async def admin_users(request: Request):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    users = []
    try:
        with _db_lock:
            con = _db_connect()
            try:
                cur = con.execute("SELECT id, username, first_name, last_name, email, mobile, is_admin, enabled FROM users ORDER BY id DESC")
                for r in cur.fetchall():
                    users.append({
                        "id": r[0], "username": r[1], "first_name": r[2] or "", "last_name": r[3] or "",
                        "email": r[4] or "", "mobile": r[5] or "", "is_admin": bool(r[6]), "enabled": bool(r[7])
                    })
            finally:
                con.close()
    except Exception:
        return JSONResponse({"ok": False, "error": "Failed to load users"}, status_code=500)
    return templates.TemplateResponse("admin_users.html", {"request": request, "users": users})


@app.get("/admin/users/new", response_class=HTMLResponse)
async def admin_users_new(request: Request):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    return templates.TemplateResponse("admin_new_user.html", {"request": request, "error": None})

@app.post("/admin/users/new")
async def admin_users_new_post(
    request: Request,
    username: str = Form(...),
    first_name: str = Form(""),
    last_name: str = Form(""),
    mobile: str = Form(""),
    email: str = Form(""),
    is_admin: str = Form("0"),
    picture: Optional[UploadFile] = File(None),
):
    try:
        require_admin(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    import sys
    u = (username or "").strip()
    if not u:
        return templates.TemplateResponse("admin_new_user.html", {"request": request, "error": "Username required"})
    
    try:
        # generate password
        gen_pw = secrets.token_urlsafe(8)
        pw_hash = _hash_password(gen_pw)
        pic_data = None
        pic_path = None
        if picture and picture.filename:
            # Guardar foto na base de dados como BLOB
            pic_data = await picture.read()
            print(f"[NEW_USER] üì∏ Profile picture uploaded: {len(pic_data)} bytes", file=sys.stderr, flush=True)
            # Path ser√° definido ap√≥s inser√ß√£o (precisa do ID)
            pic_path = ""  # Ser√° atualizado depois
        
        with _db_lock:
            con = _db_connect()
            try:
                # Convert to boolean for PostgreSQL
                is_admin_bool = True if (is_admin in ("1","true","on")) else False
                enabled_bool = True
                con.execute(
                    "INSERT INTO users (username, password_hash, first_name, last_name, mobile, email, profile_picture_path, profile_picture_data, is_admin, enabled, created_at) VALUES (?,?,?,?,?,?,?,?,?,?,?)",
                    (u, pw_hash, first_name, last_name, mobile, email, pic_path or "", pic_data, is_admin_bool, enabled_bool, time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
                )
                
                # Se tem foto, atualizar path com o ID do user
                if pic_data:
                    cur = con.execute("SELECT id FROM users WHERE username=?", (u,))
                    user_id = cur.fetchone()[0]
                    print(f"[NEW_USER] üíæ Updating path for user {user_id}", file=sys.stderr, flush=True)
                    con.execute("UPDATE users SET profile_picture_path=? WHERE id=?", (f"/api/profile-picture/{user_id}", user_id))
                con.commit()
                print(f"[NEW_USER] ‚úÖ User '{u}' created successfully", file=sys.stderr, flush=True)
            except sqlite3.IntegrityError:
                return templates.TemplateResponse("admin_new_user.html", {"request": request, "error": "Username already exists"})
            finally:
                con.close()
    except Exception as e:
        print(f"[NEW_USER] ‚ùå Error creating user: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        return templates.TemplateResponse("admin_new_user.html", {"request": request, "error": f"Error: {str(e)}"})
    # send email if provided
    if email:
        _send_creds_email(email, u, gen_pw)
    try:
        log_activity(request, "admin_create_user", details=f"username={u}")
    except Exception:
        pass
    return RedirectResponse(url="/admin/users", status_code=HTTP_303_SEE_OTHER)


@app.get("/api/profile-picture/{user_id}")
async def get_profile_picture(user_id: int):
    """Serve profile picture from database BLOB"""
    from fastapi.responses import Response
    import sys
    
    with _db_lock:
        con = _db_connect()
        try:
            cur = con.execute("SELECT profile_picture_data FROM users WHERE id=?", (user_id,))
            row = cur.fetchone()
            if row and row[0]:
                # Retornar imagem do BLOB
                # PostgreSQL retorna memoryview, converter para bytes
                blob = row[0]
                if isinstance(blob, memoryview):
                    blob = bytes(blob)
                
                print(f"[PROFILE_PIC] ‚úÖ Serving BLOB for user {user_id} ({len(blob)} bytes)", file=sys.stderr, flush=True)
                
                # Detectar tipo de imagem pelos magic bytes
                if blob[:2] == b'\xff\xd8':
                    media_type = "image/jpeg"
                elif blob[:4] == b'\x89PNG':
                    media_type = "image/png"
                elif blob[:4] == b'GIF8':
                    media_type = "image/gif"
                else:
                    media_type = "image/png"  # fallback
                return Response(content=blob, media_type=media_type)
            else:
                # Retornar imagem default se n√£o tiver foto
                print(f"[PROFILE_PIC] ‚ö†Ô∏è No BLOB for user {user_id}, using default", file=sys.stderr, flush=True)
                from pathlib import Path
                default_pic = Path(__file__).parent / "static" / "profiles" / "default-avatar.png"
                if default_pic.exists():
                    return Response(content=default_pic.read_bytes(), media_type="image/png")
                # Se n√£o existir default, retornar 404
                raise HTTPException(status_code=404, detail="No profile picture")
        finally:
            con.close()


@app.get("/api/prices")
async def get_prices(request: Request):
    require_auth(request)
    url = request.query_params.get("url") or TARGET_URL
    refresh = str(request.query_params.get("refresh", "")).strip().lower() in ("1","true","yes","on")
    # Serve from cache if fresh
    if not refresh:
        cached = _cache_get(url)
        if cached:
            # also refresh in background to keep fresh
            asyncio.create_task(_refresh_prices_background(url))
            return JSONResponse(cached)
    else:
        try:
            # Invalidate cache entry if exists
            _PRICES_CACHE.pop(url, None)
        except Exception:
            pass
    # If we have stale data (beyond TTL) we could still serve it while refreshing. For simplicity, compute now.
    try:
        # Fast path: direct fetch for CarJet s/b URLs (often returns full list without UI)
        if isinstance(url, str) and ("carjet.com/do/list/" in url) and ("s=" in url) and ("b=" in url):
            try:
                data_fast = await _compute_prices_for(url)
                fast_items = (data_fast or {}).get("items") or []
                if fast_items:
                    out = {"ok": True, "items": fast_items}
                    try:
                        # Persist the HTML if provided for inspection
                        html_fast = (data_fast or {}).get("html") or ""
                        if html_fast:
                            stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                            (DEBUG_DIR / f"pw-url-direct-fast-{stamp}.html").write_text(html_fast, encoding='utf-8')
                    except Exception:
                        pass
                    _cache_set(url, out)
                    return JSONResponse(out)
            except Exception:
                pass
        # Playwright-first for CarJet list pages to ensure the search is triggered (UI-driven)
        if USE_PLAYWRIGHT and _HAS_PLAYWRIGHT and isinstance(url, str) and ("carjet.com/do/list/" in url):
            try:
                from playwright.async_api import async_playwright
                import sys
                async with async_playwright() as p:
                    # Try WebKit (Safari) first
                    async def run_with(browser):
                        context = await browser.new_context(
                            locale="pt-PT",
                            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.0.1 Safari/605.1.15",
                        )
                        page = await context.new_page()
                        # Best-effort: set currency/lang cookies upfront
                        try:
                            await context.add_cookies([
                                {"name": "monedaForzada", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                                {"name": "moneda", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                                {"name": "currency", "value": "EUR", "domain": ".carjet.com", "path": "/"},
                                {"name": "country", "value": "PT", "domain": ".carjet.com", "path": "/"},
                                {"name": "idioma", "value": "PT", "domain": ".carjet.com", "path": "/"},
                                {"name": "lang", "value": "pt", "domain": ".carjet.com", "path": "/"},
                            ])
                        except Exception:
                            pass
                        captured = []
                        async def _on_resp(resp):
                            try:
                                u = resp.url or ""
                                if any(k in u for k in ("modalFilter.asp", "carList.asp", "/do/list/pt", "filtroUso.asp")):
                                    t = await resp.text()
                                    if t:
                                        captured.append((u, t))
                                        # Persist capture for offline inspection
                                        try:
                                            stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                            name = "pw-url-capture-" + re.sub(r"[^a-z0-9]+", "-", (u or "").lower())[-60:]
                                            (DEBUG_DIR / f"{name}-{stamp}.html").write_text(t, encoding='utf-8')
                                        except Exception:
                                            pass
                            except Exception:
                                pass
                        page.on("response", _on_resp)
                        # Warm up session on homepage before opening s/b URL
                        try:
                            base_lang = "pt"
                            m = re.search(r"/do/list/([a-z]{2})", url)
                            if m: base_lang = m.group(1)
                            home_path = "aluguel-carros/index.htm" if base_lang.lower()=="pt" else "index.htm"
                            await page.goto(f"https://www.carjet.com/{home_path}", wait_until="networkidle", timeout=25000)
                            try:
                                await page.evaluate("""try{ document.cookie='moneda=EUR; path=/; domain=.carjet.com'; document.cookie='lang=pt; path=/; domain=.carjet.com'; }catch(e){}""")
                            except Exception:
                                pass
                            try:
                                await page.wait_for_timeout(500)
                            except Exception:
                                pass
                        except Exception:
                            pass
                        await page.goto(url, wait_until="networkidle", timeout=35000)
                        
                        # ===== FILTRAR APENAS AUTOPRUDENTE =====
                        try:
                            # Aguardar filtros carregarem
                            await page.wait_for_selector('#chkAUP', timeout=5000)
                            
                            # Desmarcar todos os checkboxes de suppliers primeiro
                            await page.evaluate("""
                                document.querySelectorAll('input[name="frmPrv"]').forEach(cb => {
                                    if (cb.checked) cb.click();
                                });
                            """)
                            
                            # Aguardar um pouco
                            await page.wait_for_timeout(500)
                            
                            # Marcar apenas AUTOPRUDENTE
                            aup_checkbox = await page.query_selector('#chkAUP')
                            if aup_checkbox:
                                is_checked = await aup_checkbox.is_checked()
                                if not is_checked:
                                    await aup_checkbox.click()
                                    print("[PLAYWRIGHT ASYNC] Filtro AUTOPRUDENTE ativado", file=sys.stderr, flush=True)
                                    
                                    # Aguardar p√°gina recarregar com filtro
                                    await page.wait_for_load_state("networkidle", timeout=10000)
                                else:
                                    print("[PLAYWRIGHT ASYNC] Checkbox AUTOPRUDENTE j√° estava marcado", file=sys.stderr, flush=True)
                                    
                        except Exception as e:
                            print(f"[PLAYWRIGHT ASYNC] Erro ao filtrar AUTOPRUDENTE: {e}", file=sys.stderr, flush=True)
                            # Continuar mesmo se falhar o filtro
                            pass
                        # ===== FIM FILTRO AUTOPRUDENTE =====
                        
                        # Handle consent if present
                        try:
                            for sel in [
                                "#didomi-notice-agree-button",
                                ".didomi-continue-without-agreeing",
                                "button:has-text('Aceitar')",
                                "button:has-text('I agree')",
                                "button:has-text('Accept')",
                            ]:
                                try:
                                    c = page.locator(sel)
                                    if await c.count() > 0:
                                        try: await c.first.click(timeout=1500)
                                        except Exception: pass
                                        await page.wait_for_timeout(200)
                                        break
                                except Exception:
                                    pass
                        except Exception:
                            pass
                        # Click "Atualizar/Pesquisar" if present and trigger native submit
                        try:
                            # Try specific CarJet selectors first
                            for sel in [
                                "button[name=send].btn-search",
                                "#btn_search",
                                ".btn-search",
                                "button:has-text('Pesquisar')",
                                "button:has-text('Atualizar')",
                                "input[type=submit]",
                                "button[type=submit]",
                            ]:
                                try:
                                    b = page.locator(sel)
                                    if await b.count() > 0:
                                        try: await b.first.click(timeout=2000)
                                        except Exception: pass
                                        break
                                except Exception:
                                    pass
                            try:
                                await page.evaluate("""
                                  try { if (typeof comprobar_errores_3==='function' && comprobar_errores_3()) { if (typeof filtroUsoForm==='function') filtroUsoForm(); if (typeof submit_fechas==='function') submit_fechas('/do/list/pt'); } } catch(e) {}
                                """)
                            except Exception:
                                pass
                            try: await page.wait_for_load_state('networkidle', timeout=40000)
                            except Exception: pass
                        except Exception:
                            pass
                        # Screenshot and scroll cycles
                        try:
                            stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                            await page.screenshot(path=str(DEBUG_DIR / f"pw-url-after-search-{stamp}.png"), full_page=True)
                        except Exception:
                            pass
                        try:
                            for _ in range(3):
                                for __ in range(5):
                                    try: await page.mouse.wheel(0, 1600)
                                    except Exception: pass
                                    await page.wait_for_timeout(250)
                                try:
                                    ok = await page.locator("section.newcarlist article, .newcarlist article, article.car, li.result, li.car, .car-item, .result-row").count()
                                    if (ok or 0) > 0:
                                        break
                                except Exception:
                                    pass
                                try: await page.wait_for_load_state('networkidle', timeout=8000)
                                except Exception: pass
                        except Exception:
                            pass
                        # Explicit waits for known XHRs to maximize chances
                        try:
                            await page.wait_for_response(lambda r: 'modalFilter.asp' in (r.url or ''), timeout=40000)
                        except Exception:
                            pass
                        try:
                            await page.wait_for_response(lambda r: 'carList.asp' in (r.url or ''), timeout=40000)
                        except Exception:
                            pass
                        html = await page.content()
                        final_url = page.url
                        await context.close()
                        return html, final_url, captured

                    # Chromium-first
                    browser = await p.chromium.launch(headless=True)
                    html_pw, final_url_pw, cap_pw = await run_with(browser)
                    await browser.close()
                    items = []
                    # Prefer parsing captured bodies first
                    if (not items) and cap_pw:
                        try:
                            base_net = "https://www.carjet.com/do/list/pt"
                            for (_u, body) in cap_pw:
                                its = parse_prices(body, base_net)
                                its = convert_items_gbp_to_eur(its)
                                its = apply_price_adjustments(its, base_net)
                                if its: items = its; break
                        except Exception:
                            pass
                    if (not items) and html_pw:
                        try:
                            items = parse_prices(html_pw, final_url_pw or url)
                            items = convert_items_gbp_to_eur(items)
                            items = apply_price_adjustments(items, final_url_pw or url)
                        except Exception:
                            items = []
                    # WebKit fallback if still empty
                    if not items:
                        try:
                            browser2 = await p.webkit.launch(headless=True)
                            html2, final2, cap2 = await run_with(browser2)
                            await browser2.close()
                            # Prefer captured responses
                            if (not items) and cap2:
                                base_net = "https://www.carjet.com/do/list/pt"
                                for (_u, body) in cap2:
                                    its = parse_prices(body, base_net)
                                    its = convert_items_gbp_to_eur(its)
                                    its = apply_price_adjustments(its, base_net)
                                    if its: items = its; break
                            if (not items) and html2:
                                its = parse_prices(html2, final2 or url)
                                its = convert_items_gbp_to_eur(its)
                                its = apply_price_adjustments(its, final2 or url)
                                if its: items = its
                        except Exception:
                            pass
                    if items:
                        data = {"ok": True, "items": items}
                        _cache_set(url, data)
                        return JSONResponse(data)
                    # Direct POST fallback using page.request (within session)
                    try:
                        async with async_playwright() as p3:
                            browser3 = await p3.chromium.launch(headless=True)
                            context3 = await browser3.new_context(
                                locale="pt-PT",
                            )
                            page3 = await context3.new_page()
                            form_data = {}
                            try:
                                form_data = await page3.evaluate("""
                                  () => {
                                    try {
                                      const f = document.querySelector('form');
                                      if (!f) return {};
                                      const fd = new FormData(f);
                                      const o = Object.fromEntries(fd.entries());
                                      return o;
                                    } catch(e) { return {}; }
                                  }
                                """)
                            except Exception:
                                form_data = {}
                            # Ensure minimal fields
                            if not form_data or Object.keys(form_data).length < 3:
                                form_data = {"moneda":"EUR", "idioma":"PT"}
                            r3 = await page3.request.post("https://www.carjet.com/do/list/pt", data=form_data)
                            html3 = ""
                            try: html3 = await r3.text()
                            except Exception: html3 = ""
                            if html3:
                                try:
                                    stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                    (DEBUG_DIR / f"pw-url-direct-post-{stamp}.html").write_text(html3, encoding='utf-8')
                                except Exception:
                                    pass
                                its3 = parse_prices(html3, "https://www.carjet.com/do/list/pt")
                                its3 = convert_items_gbp_to_eur(its3)
                                its3 = apply_price_adjustments(its3, "https://www.carjet.com/do/list/pt")
                                if its3:
                                    data = {"ok": True, "items": its3}
                                    _cache_set(url, data)
                                    await context3.close(); await browser3.close()
                                    return JSONResponse(data)
                            await context3.close(); await browser3.close()
                    except Exception:
                        pass
            except Exception:
                pass
        data = await _compute_prices_for(url)
        _cache_set(url, data)
        return JSONResponse(data)
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/ai-pricing-analysis")
async def ai_pricing_analysis(request: Request):
    """
    AI-powered pricing analysis endpoint
    Analyzes historical pricing data and provides positioning insights
    """
    try:
        require_auth(request)
    except HTTPException:
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    try:
        body = await request.json()
        reference_supplier = body.get("referenceSupplier", "autoprudente")
        analysis_period = int(body.get("analysisPeriod", 90))
        location = body.get("location", "Albufeira")
        
        print(f"[AI ANALYSIS] Supplier: {reference_supplier}, Period: {analysis_period} days, Location: {location}")
        
        # Mock AI analysis for now - replace with real DB queries later
        # This would query historical data from database
        group_insights = []
        grupos = ['B1', 'B2', 'D', 'E1', 'E2', 'F', 'G', 'J1', 'J2', 'L1', 'L2', 'M1', 'M2', 'N']
        group_names = {
            'B1': 'Mini 4 Doors', 'B2': 'Mini Automatic', 'D': 'Economy',
            'E1': 'Compact 4 Doors', 'E2': 'Compact Automatic', 'F': 'Intermediate',
            'G': 'Intermediate Automatic', 'J1': 'Compact SUV', 'J2': 'Intermediate SUV',
            'L1': 'Standard', 'L2': 'Standard Automatic', 'M1': 'Premium', 'M2': 'Premium Automatic', 'N': 'Minivan'
        }
        
        total_data_points = 0
        avg_positions = []
        avg_price_diffs = []
        
        for grupo in grupos:
            # Mock data - would query real historical data
            sample_size = int(50 + (hash(grupo) % 100))
            avg_position = round(2 + (hash(grupo) % 3), 1)
            avg_price_diff = round(0.25 + ((hash(grupo) % 10) / 10), 2)
            cheapest_count = int(10 + (hash(grupo) % 20))
            confidence = min(95, 50 + sample_size // 2)
            
            total_data_points += sample_size
            avg_positions.append(avg_position)
            avg_price_diffs.append(avg_price_diff)
            
            # Determine position color
            if avg_position <= 2:
                position_color = "green"
            elif avg_position <= 3:
                position_color = "yellow"
            else:
                position_color = "red"
            
            # Determine price diff color
            if avg_price_diff <= 0.50:
                price_diff_color = "green"
            elif avg_price_diff <= 1.00:
                price_diff_color = "yellow"
            else:
                price_diff_color = "red"
            
            # Generate AI recommendation
            if avg_position <= 2:
                recommendation = f"You're consistently competitive in {grupo}. Maintain current positioning with slight adjustments based on demand."
                suggested_strategy = f"Follow Lowest Price +{avg_price_diff:.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": avg_price_diff,
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            elif avg_position <= 3:
                recommendation = f"Room for improvement in {grupo}. Consider reducing prices by ‚Ç¨0.20-0.50 to gain better positioning."
                suggested_strategy = f"Follow Lowest Price +{max(0, avg_price_diff - 0.30):.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": max(0, avg_price_diff - 0.30),
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            else:
                recommendation = f"High prices detected in {grupo}. Reduce by ‚Ç¨0.50-1.00 to improve market position."
                suggested_strategy = f"Follow Lowest Price +{max(0, avg_price_diff - 0.60):.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": max(0, avg_price_diff - 0.60),
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            
            group_insights.append({
                "group": grupo,
                "groupName": group_names[grupo],
                "sampleSize": sample_size,
                "avgPosition": avg_position,
                "avgPriceDiff": avg_price_diff,
                "cheapestCount": cheapest_count,
                "confidence": confidence,
                "positionColor": position_color,
                "priceDiffColor": price_diff_color,
                "recommendation": recommendation,
                "suggestedStrategy": suggested_strategy,
                "strategyConfig": strategy_config
            })
        
        overall_avg_position = sum(avg_positions) / len(avg_positions) if avg_positions else 0
        overall_avg_price_diff = sum(avg_price_diffs) / len(avg_price_diffs) if avg_price_diffs else 0
        overall_confidence = min(95, 50 + (total_data_points // len(grupos)) // 2)
        
        result = {
            "ok": True,
            "dataPoints": total_data_points,
            "avgPosition": overall_avg_position,
            "avgPriceDiff": overall_avg_price_diff,
            "confidence": overall_confidence,
            "groupInsights": group_insights,
            "referenceSupplier": reference_supplier,
            "analysisPeriod": analysis_period,
            "location": location
        }
        
        print(f"[AI ANALYSIS] Generated insights for {len(group_insights)} groups, {total_data_points} data points")
        return JSONResponse(result)
        
    except Exception as e:
        print(f"[AI ANALYSIS ERROR] {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/ai-deep-analysis")
async def ai_deep_analysis(request: Request):
    """
    Deep analysis endpoint for 24-month historical pricing data
    Analyzes positioning trends across all time periods
    """
    try:
        require_auth(request)
    except HTTPException:
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    try:
        body = await request.json()
        locations = body.get("locations", ["Albufeira", "Aeroporto de Faro"])
        reference_supplier = body.get("referenceSupplier", "autoprudente")
        results = body.get("results", [])
        
        print(f"[DEEP ANALYSIS] Locations: {locations}, Supplier: {reference_supplier}, Results: {len(results)}")
        
        # Analyze all results
        grupos = ['B1', 'B2', 'D', 'E1', 'E2', 'F', 'G', 'J1', 'J2', 'L1', 'L2', 'M1', 'M2', 'N']
        group_names = {
            'B1': 'Mini 4 Doors', 'B2': 'Mini Automatic', 'D': 'Economy',
            'E1': 'Compact 4 Doors', 'E2': 'Compact Automatic', 'F': 'Intermediate',
            'G': 'Intermediate Automatic', 'J1': 'Compact SUV', 'J2': 'Intermediate SUV',
            'L1': 'Standard', 'L2': 'Standard Automatic', 'M1': 'Premium', 'M2': 'Premium Automatic', 'N': 'Minivan'
        }
        
        # Aggregate data by group
        group_data = {grupo: [] for grupo in grupos}
        total_data_points = 0
        all_suppliers_found = set()  # Track ALL suppliers in the data
        
        # Normalize supplier names for matching (keep ALL suppliers, just standardize format)
        def normalize_supplier(name):
            # Just clean and lowercase, keep ALL suppliers
            name = str(name).strip().lower()
            # Standardize common variations for better grouping
            if 'autoprudente' in name or 'auto prudente' in name:
                return 'autoprudente'
            elif 'hertz' in name:
                return 'hertz'
            elif 'europcar' in name:
                return 'europcar'
            elif 'keddy' in name:
                return 'keddy'
            elif 'thrifty' in name:
                return 'thrifty'
            elif 'goldcar' in name:
                return 'goldcar'
            elif 'ok mobility' in name:
                return 'ok_mobility'
            elif 'centauro' in name:
                return 'centauro'
            elif 'surprice' in name:
                return 'surprice'
            elif 'firefly' in name:
                return 'firefly'
            elif 'sixt' in name:
                return 'sixt'
            elif 'avis' in name:
                return 'avis'
            elif 'budget' in name:
                return 'budget'
            elif 'enterprise' in name:
                return 'enterprise'
            elif 'national' in name:
                return 'national'
            elif 'dollar' in name:
                return 'dollar'
            elif 'alamo' in name:
                return 'alamo'
            # Keep any other supplier as-is (cleaned)
            return name.replace(' ', '_')
        
        for result in results:
            items = result.get('items', [])
            if not items:
                continue
            
            # Group items by car group
            for item in items:
                grupo = item.get('group', '')
                if grupo not in grupos:
                    continue
                
                supplier = normalize_supplier(item.get('supplier', ''))
                price = float(item.get('price_num', 0))
                
                if price > 0:
                    all_suppliers_found.add(supplier)  # Track supplier
                    group_data[grupo].append({
                        'supplier': supplier,
                        'price': price,
                        'date': result.get('date'),
                        'days': result.get('days')
                    })
                    total_data_points += 1
        
        # Log all suppliers found
        print(f"[DEEP ANALYSIS] Found {len(all_suppliers_found)} unique suppliers: {sorted(all_suppliers_found)}")
        
        # Calculate insights per group
        group_insights = []
        all_positions = []
        all_price_diffs = []
        
        for grupo in grupos:
            data = group_data[grupo]
            if len(data) < 10:  # Need minimum data points
                continue
            
            # Find reference supplier positions
            ref_positions = []
            ref_price_diffs = []
            cheapest_count = 0
            
            # Group by date+days for position calculation
            date_groups = {}
            for item in data:
                key = f"{item['date']}_{item['days']}"
                if key not in date_groups:
                    date_groups[key] = []
                date_groups[key].append(item)
            
            # Calculate position for each date/day combination
            for key, items in date_groups.items():
                # Sort by price
                sorted_items = sorted(items, key=lambda x: x['price'])
                
                # Find reference supplier
                ref_item = next((x for x in sorted_items if x['supplier'] == reference_supplier), None)
                if not ref_item:
                    continue
                
                # Calculate position (1-indexed)
                position = sorted_items.index(ref_item) + 1
                ref_positions.append(position)
                
                # Calculate price difference vs lowest
                lowest_price = sorted_items[0]['price']
                price_diff = ref_item['price'] - lowest_price
                ref_price_diffs.append(price_diff)
                
                if position == 1:
                    cheapest_count += 1
            
            if not ref_positions:
                continue
            
            avg_position = sum(ref_positions) / len(ref_positions)
            avg_price_diff = sum(ref_price_diffs) / len(ref_price_diffs)
            sample_size = len(ref_positions)
            confidence = min(95, 50 + (sample_size // 10))
            
            all_positions.append(avg_position)
            all_price_diffs.append(avg_price_diff)
            
            # Determine colors
            position_color = "green" if avg_position <= 2 else "yellow" if avg_position <= 3 else "red"
            price_diff_color = "green" if avg_price_diff <= 0.50 else "yellow" if avg_price_diff <= 1.00 else "red"
            
            # Generate recommendation
            if avg_position <= 2:
                recommendation = f"Excellent positioning in {grupo} over 24 months. You're consistently in top 2. Maintain current strategy."
                suggested_strategy = f"Follow Lowest Price +{avg_price_diff:.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": round(avg_price_diff, 2),
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            elif avg_position <= 3:
                recommendation = f"Good positioning in {grupo}, but room for improvement. Consider reducing by ‚Ç¨0.20-0.40 to rank higher."
                suggested_strategy = f"Follow Lowest Price +{max(0, avg_price_diff - 0.30):.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": round(max(0, avg_price_diff - 0.30), 2),
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            else:
                recommendation = f"High prices detected in {grupo} consistently. Reduce by ‚Ç¨0.50-1.00 to improve competitiveness."
                suggested_strategy = f"Follow Lowest Price +{max(0, avg_price_diff - 0.70):.2f}‚Ç¨"
                strategy_config = {
                    "type": "follow_lowest",
                    "diffType": "euros",
                    "diffValue": round(max(0, avg_price_diff - 0.70), 2),
                    "minPriceDay": None,
                    "minPriceMonth": None
                }
            
            group_insights.append({
                "group": grupo,
                "groupName": group_names[grupo],
                "sampleSize": sample_size,
                "avgPosition": round(avg_position, 1),
                "avgPriceDiff": round(avg_price_diff, 2),
                "cheapestCount": cheapest_count,
                "confidence": confidence,
                "positionColor": position_color,
                "priceDiffColor": price_diff_color,
                "recommendation": recommendation,
                "suggestedStrategy": suggested_strategy,
                "strategyConfig": strategy_config
            })
        
        overall_avg_position = sum(all_positions) / len(all_positions) if all_positions else 0
        overall_avg_price_diff = sum(all_price_diffs) / len(all_price_diffs) if all_price_diffs else 0
        overall_confidence = min(95, 50 + (total_data_points // 100))
        
        result = {
            "ok": True,
            "dataPoints": total_data_points,
            "avgPosition": overall_avg_position,
            "avgPriceDiff": overall_avg_price_diff,
            "confidence": overall_confidence,
            "groupInsights": group_insights,
            "referenceSupplier": reference_supplier,
            "locations": locations,
            "allSuppliers": sorted(list(all_suppliers_found)),
            "totalSuppliers": len(all_suppliers_found),
            "analysisType": "deep_scan_24months_both_locations"
        }
        
        print(f"[DEEP ANALYSIS] Complete: {len(group_insights)} groups, {total_data_points} data points, {len(all_suppliers_found)} suppliers")
        return JSONResponse(result)
        
    except Exception as e:
        print(f"[DEEP ANALYSIS ERROR] {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/price-calendar-analysis")
async def price_calendar_analysis(request: Request):
    """
    Analyze daily price patterns and detect when prices change
    Identifies pricing periods and generates calendar recommendations
    """
    try:
        require_auth(request)
    except HTTPException:
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    try:
        body = await request.json()
        locations = body.get("locations", ["Albufeira", "Aeroporto de Faro"])
        reference_supplier = body.get("referenceSupplier", "autoprudente")
        results = body.get("results", [])
        
        print(f"[CALENDAR ANALYSIS] Locations: {locations}, Supplier: {reference_supplier}, Results: {len(results)}")
        
        # Organize data by date
        price_by_date = {}
        
        # Normalize supplier name
        def normalize_supplier(name):
            return str(name).strip().lower().replace(' ', '_')
        
        for result in results:
            location = result.get('location')
            date = result.get('date')
            items = result.get('items', [])
            
            if not date or not items:
                continue
            
            # Find reference supplier price
            ref_price = None
            all_prices = []
            
            for item in items:
                supplier = normalize_supplier(item.get('supplier', ''))
                price = float(item.get('price_num', 0))
                
                if price > 0:
                    all_prices.append(price)
                    if reference_supplier.lower() in supplier:
                        ref_price = price
            
            if ref_price and all_prices:
                if date not in price_by_date:
                    price_by_date[date] = []
                
                price_by_date[date].append({
                    'location': location,
                    'price': ref_price,
                    'lowest': min(all_prices),
                    'avg': sum(all_prices) / len(all_prices)
                })
        
        # Sort dates
        sorted_dates = sorted(price_by_date.keys())
        
        # Detect price changes (when price differs by >5% from previous)
        changes = []
        periods = []
        
        if len(sorted_dates) > 0:
            current_period = {
                'start_date': sorted_dates[0],
                'prices': []
            }
            
            prev_price = None
            for date in sorted_dates:
                date_data = price_by_date[date]
                avg_price = sum(d['price'] for d in date_data) / len(date_data)
                
                current_period['prices'].append(avg_price)
                
                if prev_price is not None:
                    change_pct = abs(avg_price - prev_price) / prev_price * 100
                    
                    if change_pct > 5:  # 5% threshold
                        # Price changed! End current period
                        changes.append({
                            'date': date,
                            'old_price': prev_price,
                            'new_price': avg_price,
                            'change_pct': round(change_pct, 1)
                        })
                        
                        # Save period
                        period_avg = sum(current_period['prices']) / len(current_period['prices'])
                        periods.append({
                            'start': current_period['start_date'],
                            'end': sorted_dates[sorted_dates.index(date) - 1],
                            'duration_days': len(current_period['prices']),
                            'avg_price': round(period_avg, 2)
                        })
                        
                        # Start new period
                        current_period = {
                            'start_date': date,
                            'prices': [avg_price]
                        }
                
                prev_price = avg_price
            
            # Add final period
            if current_period['prices']:
                period_avg = sum(current_period['prices']) / len(current_period['prices'])
                periods.append({
                    'start': current_period['start_date'],
                    'end': sorted_dates[-1],
                    'duration_days': len(current_period['prices']),
                    'avg_price': round(period_avg, 2)
                })
        
        # Calculate patterns
        avg_frequency = 0
        if len(changes) > 1:
            # Calculate average days between changes
            change_dates = [datetime.strptime(c['date'], '%Y-%m-%d') for c in changes]
            intervals = [(change_dates[i+1] - change_dates[i]).days for i in range(len(change_dates)-1)]
            avg_frequency = round(sum(intervals) / len(intervals), 1) if intervals else 0
        
        # Recommendations
        recommendations = []
        if len(changes) > 0:
            recommendations.append(f"Detected {len(changes)} significant price changes over the analyzed period")
        if len(periods) > 0:
            recommendations.append(f"Identified {len(periods)} distinct pricing periods")
            avg_period_length = sum(p['duration_days'] for p in periods) / len(periods)
            recommendations.append(f"Average pricing period lasts {int(avg_period_length)} days")
        if avg_frequency > 0:
            recommendations.append(f"Prices change approximately every {int(avg_frequency)} days")
            recommendations.append(f"Consider updating your prices every {max(1, int(avg_frequency * 0.8))} days to stay competitive")
        
        result = {
            "ok": True,
            "totalChanges": len(changes),
            "changes": changes[:50],  # Limit to 50 most recent
            "periods": periods,
            "patterns": {
                "avgFrequency": avg_frequency,
                "competitiveLag": 0,  # Would calculate by comparing change dates with competitors
                "bestDay": "N/A"  # Would analyze which day of week changes happen
            },
            "recommendations": recommendations,
            "dataPoints": len(sorted_dates),
            "locations": locations
        }
        
        print(f"[CALENDAR ANALYSIS] Complete: {len(changes)} changes, {len(periods)} periods")
        return JSONResponse(result)
        
    except Exception as e:
        print(f"[CALENDAR ANALYSIS ERROR] {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/track-by-params")
async def track_by_params(request: Request):
    try:
        if not bool(str(os.getenv("DEV_NO_AUTH", "")).strip().lower() in ("1","true","yes","on")):
            require_auth(request)
    except HTTPException:
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    try:
        body = await request.json()
    except Exception:
        body = {}
    location = str(body.get("location") or "").strip()
    start_date = str(body.get("start_date") or "").strip()
    start_time = str(body.get("start_time") or "15:00").strip() or "15:00"  # Default 15:00 (consistente com rota√ß√£o)
    end_date_in = str(body.get("end_date") or "").strip()
    end_time = str(body.get("end_time") or "15:00").strip() or "15:00"  # Default 15:00 (consistente com rota√ß√£o)
    # days is optional if end_date provided
    try:
        days = int(body.get("days") or 0)
    except Exception:
        days = 0
    lang = str(body.get("lang") or "pt").strip() or "pt"
    currency = str(body.get("currency") or "EUR").strip() or "EUR"
    # quick=1 enables fast mode (skip some waits/screenshots)
    try:
        quick = int(body.get("quick") or 0)
    except Exception:
        quick = 0
    if not location or not start_date:
        return _no_store_json({"ok": False, "error": "Missing location or start_date"}, status_code=400)
    
    # LOG REQUEST PARAMS
    import sys
    print(f"\n{'='*60}")
    print(f"[API] REQUEST: location={location}, start_date={start_date}, start_time={start_time}, days={days}")
    print(f"[API] end_date_in={end_date_in}, end_time={end_time}")
    print(f"{'='*60}\n")
    print(f"[API] REQUEST: location={location}, start_date={start_date}, start_time={start_time}, days={days}", file=sys.stderr, flush=True)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DATE ROTATION - Variar datas para evitar detec√ß√£o
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    date_rotation_enabled = True
    date_rotation_max_days = 4
    
    # Carregar configura√ß√µes da base de dados
    try:
        with _db_lock:
            con = _db_connect()
            try:
                row = con.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = ?",
                    ("date_rotation_enabled",)
                ).fetchone()
                if row:
                    date_rotation_enabled = row[0].lower() in ('true', '1', 'yes')
                
                row = con.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = ?",
                    ("date_rotation_max_days",)
                ).fetchone()
                if row:
                    date_rotation_max_days = int(row[0])
            finally:
                con.close()
    except Exception as e:
        print(f"[DATE_ROTATION] Erro ao carregar configura√ß√µes: {e}", file=sys.stderr, flush=True)
    
    # Aplicar rota√ß√£o de datas
    original_start_date = start_date
    if date_rotation_enabled and date_rotation_max_days > 0:
        try:
            base_date = datetime.strptime(start_date, "%Y-%m-%d")
            days_offset = random.randint(0, date_rotation_max_days)
            rotated_date = base_date + timedelta(days=days_offset)
            start_date = rotated_date.strftime("%Y-%m-%d")
            print(f"[DATE_ROTATION] Original: {original_start_date}, Rotated: {start_date} (+{days_offset} days)", file=sys.stderr, flush=True)
        except Exception as e:
            print(f"[DATE_ROTATION] Erro: {e}, usando data original", file=sys.stderr, flush=True)
    else:
        print(f"[DATE_ROTATION] Desativado, usando data original: {start_date}", file=sys.stderr, flush=True)
    
    try:
        # Build start datetime with provided time (usando data rotacionada)
        start_dt = datetime.fromisoformat(f"{start_date}T{start_time}")
    except Exception:
        return _no_store_json({"ok": False, "error": "Invalid start_date (YYYY-MM-DD)"}, status_code=400)
    # Determine end datetime
    if end_date_in:
        try:
            end_dt = datetime.fromisoformat(f"{end_date_in}T{end_time}")
        except Exception:
            return _no_store_json({"ok": False, "error": "Invalid end_date (YYYY-MM-DD)"}, status_code=400)
        if end_dt <= start_dt:
            return _no_store_json({"ok": False, "error": "end_date/time must be after start"}, status_code=400)
        days = max(1, (end_dt - start_dt).days)
    else:
        if days <= 0:
            return _no_store_json({"ok": False, "error": "Missing days or end_date"}, status_code=400)
        end_dt = start_dt + timedelta(days=days)
    print(f"[API] COMPUTED: start_dt={start_dt.date()}, end_dt={end_dt.date()}, days={days}")
    print(f"[API] COMPUTED: start_dt={start_dt.date()}, end_dt={end_dt.date()}, days={days}", file=sys.stderr, flush=True)
    try:
        items: List[Dict[str, Any]] = []
        base = f"https://www.carjet.com/do/list/{lang}"
        
        # MODO DE TESTE COM DADOS MOCKADOS (TEST_MODE_LOCAL=2)
        if TEST_MODE_LOCAL == 2:
            print(f"[MOCK MODE] Generating mock data for {location}, {days} days")
            # Pre√ßo base varia por localiza√ß√£o
            base_price = 12.0 if 'faro' in location.lower() else 14.0
            items = []
            mock_cars = [
                # B1 - Mini 4 Doors
                ("Fiat 500", "Group B1", "Greenmotion"),
                ("Citroen C1", "Group B1", "Goldcar"),
                # B2 - Mini 5 Doors
                ("Toyota Aygo", "Group B2", "Surprice"),
                ("Volkswagen UP", "Group B2", "Centauro"),
                ("Fiat Panda", "Group B2", "OK Mobility"),
                # D - Economy
                ("Renault Clio", "Group D", "Goldcar"),
                ("Peugeot 208", "Group D", "Europcar"),
                ("Ford Fiesta", "Group D", "Hertz"),
                ("Seat Ibiza", "Group D", "Thrifty"),
                ("Hyundai i20", "Group D", "Centauro"),
                # E1 - Mini Automatic
                ("Fiat 500 Auto", "Group E1", "OK Mobility"),
                ("Peugeot 108 Auto", "Group E1", "Goldcar"),
                # E2 - Economy Automatic
                ("Opel Corsa Auto", "Group E2", "Europcar"),
                ("Ford Fiesta Auto", "Group E2", "Hertz"),
                # F - SUV
                ("Nissan Juke", "Group F", "Auto Prudente Rent a Car"),
                ("Peugeot 2008", "Group F", "Goldcar"),
                ("Renault Captur", "Group F", "Surprice"),
                # G - Premium
                ("Mini Cooper Countryman", "Group G", "Thrifty"),
                # J1 - Crossover
                ("Citroen C3 Aircross", "Group J1", "Centauro"),
                ("Fiat 500X", "Group J1", "OK Mobility"),
                ("VW T-Cross", "Group J1", "Europcar"),
                # J2 - Station Wagon
                ("Seat Leon SW", "Group J2", "Goldcar"),
                ("Peugeot 308 SW", "Group J2", "Hertz"),
                # L1 - SUV Automatic
                ("Peugeot 3008 Auto", "Group L1", "Auto Prudente Rent a Car"),
                ("Nissan Qashqai Auto", "Group L1", "Goldcar"),
                ("Toyota C-HR Auto", "Group L1", "Thrifty"),
                # L2 - Station Wagon Automatic
                ("Toyota Corolla SW Auto", "Group L2", "Europcar"),
                ("Opel Astra SW Auto", "Group L2", "Surprice"),
                # M1 - 7 Seater
                ("Dacia Lodgy", "Group M1", "Greenmotion"),
                ("Peugeot Rifter", "Group M1", "Centauro"),
                # M2 - 7 Seater Automatic
                ("Renault Grand Scenic Auto", "Group M2", "Goldcar"),
                ("VW Caddy Auto", "Group M2", "Auto Prudente Rent a Car"),
                # N - 9 Seater
                ("Ford Tourneo", "Group N", "Europcar"),
                ("Mercedes Vito Auto", "Group N", "Thrifty"),
            ]
            # Varia fornecedores por localiza√ß√£o
            location_modifier = 0.0 if 'faro' in location.lower() else 3.0
            for idx, (car, cat, sup) in enumerate(mock_cars):
                price = base_price + (idx * 1.5) + (days * 0.3) + location_modifier
                # Varia fornecedor para Albufeira
                if 'albufeira' in location.lower() and idx % 3 == 0:
                    sup = "Centauro" if sup != "Centauro" else "Goldcar"
                # Extrair c√≥digo do grupo da categoria (ex: "Group B1" -> "B1")
                group_code = cat.replace("Group ", "").strip() if "Group " in cat else "Others"
                items.append({
                    "id": idx,
                    "car": car,
                    "supplier": sup,
                    "price": f"‚Ç¨{price * days:.2f}",
                    "currency": "EUR",
                    "category": cat,
                    "group": group_code,
                    "transmission": "Automatic" if "Auto" in car else "Manual",
                    "photo": "",
                    "link": "",
                })
            print(f"[MOCK MODE] Generated {len(items)} mock items for {location} covering all groups")
            return _no_store_json({
                "ok": True,
                "items": items,
                "location": location,
                "start_date": start_dt.date().isoformat(),
                "start_time": start_dt.strftime("%H:%M"),
                "end_date": end_dt.date().isoformat(),
                "end_time": end_dt.strftime("%H:%M"),
                "days": days,
            })
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # M√âTODO 0: DESATIVADO - ScraperAPI n√£o funciona bem com CarJet
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Usar POST direto ou Selenium em vez disso
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # POST DIRETO REMOVIDO - Ir sempre diretamente para SELENIUM (mais r√°pido e fi√°vel)
        # SELENIUM headless funciona sempre e poupa 5-10s de tentativas falhadas
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # M√âTODO DESATIVADO: ScraperAPI (N√ÉO USAR - Bloqueado)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if False:
            try:
                import httpx
                import sys
                from urllib.parse import urlencode
                print(f"[SCRAPERAPI] Iniciando scraping para {location}", file=sys.stderr, flush=True)
                
                # Mapear localiza√ß√£o
                carjet_loc = location
                if 'faro' in location.lower():
                    carjet_loc = 'Faro Aeroporto (FAO)'
                elif 'albufeira' in location.lower():
                    carjet_loc = 'Albufeira Cidade'
                
                # Formato de datas para CarJet (dd/mm/yyyy)
                start_str = start_dt.strftime("%d/%m/%Y")
                end_str = end_dt.strftime("%d/%m/%Y")
                
                # Construir URL CarJet com par√¢metros
                carjet_params = {
                    'pickup': carjet_loc,
                    'dropoff': carjet_loc,
                    'fechaRecogida': start_str,
                    'fechaEntrega': end_str,
                    'fechaRecogidaSelHour': '10:00',
                    'fechaEntregaSelHour': '10:00',
                }
                target_url = f"https://www.carjet.com/aluguel-carros/index.htm?{urlencode(carjet_params)}"
                
                # Construir URL ScraperAPI
                scraper_params = {
                    'api_key': SCRAPER_API_KEY,
                    'url': target_url,
                    'render_js': 'true',
                    'wait': '3000',
                    'country': 'pt',
                }
                scraper_url = f"http://api.scrapeops.io/v1/?{urlencode(scraper_params)}"
                
                print(f"[SCRAPERAPI] Target: {target_url[:100]}...", file=sys.stderr, flush=True)
                print(f"[SCRAPERAPI] Fazendo request via ScraperOps...", file=sys.stderr, flush=True)
                
                # Fazer request via ScraperAPI
                async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
                    response = await client.get(scraper_url)
                
                
                if response.status_code == 200:
                    html_content = response.text
                    print(f"[SCRAPERAPI] ‚úÖ HTML recebido: {len(html_content)} bytes", file=sys.stderr, flush=True)
                    
                    # Parse o HTML
                    items = parse_prices(html_content, target_url)
                    print(f"[SCRAPERAPI] Parsed {len(items)} items antes convers√£o", file=sys.stderr, flush=True)
                    
                    # Converter GBP para EUR
                    items = convert_items_gbp_to_eur(items)
                    print(f"[SCRAPERAPI] {len(items)} items ap√≥s GBP‚ÜíEUR", file=sys.stderr, flush=True)
                    
                    # Aplicar ajustes
                    items = apply_price_adjustments(items, target_url)
                    print(f"[SCRAPERAPI] {len(items)} items ap√≥s ajustes", file=sys.stderr, flush=True)
                    
                    if items:
                        print(f"[SCRAPERAPI] ‚úÖ {len(items)} carros encontrados!", file=sys.stderr, flush=True)
                        if items:
                            print(f"[SCRAPERAPI] Primeiro: {items[0].get('car', 'N/A')} - {items[0].get('price', 'N/A')}", file=sys.stderr, flush=True)
                        # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                        items = normalize_and_sort(items, supplier_priority=None)
                        return _no_store_json({
                            "ok": True,
                            "items": items,
                            "location": location,
                            "start_date": start_dt.date().isoformat(),
                            "start_time": start_dt.strftime("%H:%M"),
                            "end_date": end_dt.date().isoformat(),
                            "end_time": end_dt.strftime("%H:%M"),
                            "days": days,
                        })
                    else:
                        print(f"[SCRAPERAPI] ‚ö†Ô∏è Parse retornou 0 items", file=sys.stderr, flush=True)
                else:
                    print(f"[SCRAPERAPI] ‚ùå HTTP {response.status_code}", file=sys.stderr, flush=True)
                    print(f"[SCRAPERAPI] Tentando fallback para Playwright...", file=sys.stderr, flush=True)
            except Exception as e:
                import sys
                print(f"[SCRAPERAPI ERROR] {e}", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc(file=sys.stderr)
                print(f"[SCRAPERAPI] Tentando fallback para Playwright...", file=sys.stderr, flush=True)
        
        # FALLBACK: Tentar Playwright se ScraperAPI falhou (DESATIVADO - usar Selenium)
        if False and TEST_MODE_LOCAL == 0 and not items and _HAS_PLAYWRIGHT:
            try:
                from playwright.async_api import async_playwright
                import sys
                print(f"[PLAYWRIGHT] Iniciando scraping direto para {location}", file=sys.stderr, flush=True)
                
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context(
                        locale="pt-PT",
                        viewport={"width": 1920, "height": 1080},
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    )
                    page = await context.new_page()
                    
                    try:
                        # Ir para p√°gina inicial do CarJet PT
                        print(f"[PLAYWRIGHT] Acessando CarJet homepage PT...", file=sys.stderr, flush=True)
                        await page.goto("https://www.carjet.com/aluguer-carros/index.htm", wait_until="domcontentloaded", timeout=45000)
                        
                        # Aguardar p√°gina carregar
                        await page.wait_for_timeout(2000)
                        
                        # Fechar cookies via JS (mais robusto)
                        await page.evaluate("""() => {
                            try {
                                document.querySelectorAll('[id*=cookie], [class*=cookie], [id*=consent], [class*=consent]').forEach(el => el.remove());
                            } catch(e) {}
                        }""")
                        
                        # Mapear localiza√ß√£o
                        carjet_loc = location
                        if 'faro' in location.lower():
                            carjet_loc = 'Faro Aeroporto (FAO)'
                        elif 'albufeira' in location.lower():
                            carjet_loc = 'Albufeira Cidade'
                        
                        print(f"[PLAYWRIGHT] Preenchendo formul√°rio via JS: {carjet_loc}", file=sys.stderr, flush=True)
                        
                        # Preencher formul√°rio via JavaScript (mais robusto que seletores)
                        start_str = start_dt.strftime("%d/%m/%Y")
                        end_str = end_dt.strftime("%d/%m/%Y")
                        
                        await page.evaluate("""
                            ({loc, start, end, startTime, endTime}) => {
                                function fill(sel, val) {
                                    const el = document.querySelector(sel);
                                    if (el) { 
                                        el.value = val; 
                                        el.dispatchEvent(new Event('change', {bubbles: true}));
                                        return true;
                                    }
                                    return false;
                                }
                                const r1 = fill('input[name="pickup"]', loc);
                                const r2 = fill('input[name="dropoff"]', loc);
                                const r3 = fill('input[name="fechaRecogida"]', start);
                                const r4 = fill('input[name="fechaEntrega"]', end);
                                const h1 = document.querySelector('select[name="fechaRecogidaSelHour"]');
                                const h2 = document.querySelector('select[name="fechaEntregaSelHour"]');
                                if (h1) h1.value = startTime || '16:00';
                                if (h2) h2.value = endTime || '10:00';
                                return {r1, r2, r3, r4};
                            }
                        """, {"loc": carjet_loc, "start": start_str, "end": end_str, "startTime": start_dt.strftime("%H:%M"), "endTime": end_dt.strftime("%H:%M")})

                        # Click the primary search/submit button and wait for results
                        try:
                            btn = None
                            try:
                                btn = page.get_by_role("button", name=re.compile(r"(Pesquisar|Buscar|Search)", re.I))
                            except Exception:
                                btn = None
                            if btn and await btn.is_visible():
                                await btn.click(timeout=3000)
                            else:
                                cand = page.locator("button:has-text('Pesquisar'), button:has-text('Buscar'), button:has-text('Search'), input[type=submit], button[type=submit]")
                                if await cand.count() > 0:
                                    try:
                                        await cand.first.click(timeout=3000)
                                    except Exception:
                                        pass
                            try:
                                await page.wait_for_load_state("networkidle", timeout=10000)
                            except Exception:
                                pass
                        except Exception:
                            pass

                        # If we landed on a 'war=' URL without the secure params, try a direct POST fallback
                        try:
                            current_url = page.url or ""
                            if ("war=" in current_url) and ("s=" not in current_url) and ("b=" not in current_url):
                                from urllib.parse import urlparse as _urlparse
                                print(f"[PLAYWRIGHT] war URL detected, attempting direct POST fallback...", file=sys.stderr, flush=True)
                                payload_dp = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                                rdp = await page.request.post(f"https://www.carjet.com/do/list/{lang}", data=payload_dp)
                                if rdp and rdp.ok:
                                    html_dp = await rdp.text()
                                    its_dp = parse_prices(html_dp, f"https://www.carjet.com/do/list/{lang}")
                                    its_dp = convert_items_gbp_to_eur(its_dp)
                                    its_dp = apply_price_adjustments(its_dp, f"https://www.carjet.com/do/list/{lang}")
                                    if its_dp:
                                        print(f"[PLAYWRIGHT] ‚úÖ Fallback POST retornou {len(its_dp)} carros", file=sys.stderr, flush=True)
                                        # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                                        its_dp = normalize_and_sort(its_dp, supplier_priority=None)
                                        return _no_store_json({
                                            "ok": True,
                                            "items": its_dp,
                                            "location": location,
                                            "start_date": start_dt.date().isoformat(),
                                            "start_time": start_dt.strftime("%H:%M"),
                                            "end_date": end_dt.date().isoformat(),
                                            "end_time": end_dt.strftime("%H:%M"),
                                            "days": days,
                                        })
                        except Exception:
                            pass
                        
                        await page.wait_for_timeout(1000)
                        
                        print(f"[PLAYWRIGHT] Submetendo formul√°rio...", file=sys.stderr, flush=True)
                        
                        # Submeter via JS (mais confi√°vel)
                        await page.evaluate("""() => {
                            const form = document.querySelector('form');
                            if (form) form.submit();
                        }""")
                        
                        # Aguardar navega√ß√£o para p√°gina de resultados
                        print(f"[PLAYWRIGHT] Aguardando navega√ß√£o...", file=sys.stderr, flush=True)
                        await page.wait_for_url('**/do/list/**', timeout=90000)
                        
                        print(f"[PLAYWRIGHT] Aguardando carros carregarem...", file=sys.stderr, flush=True)
                        await page.wait_for_timeout(8000)
                        
                        # Extrair URL e HTML
                        final_url = page.url
                        html_content = await page.content()
                        print(f"[PLAYWRIGHT] URL final: {final_url}", file=sys.stderr, flush=True)
                        print(f"[PLAYWRIGHT] ‚úÖ HTML capturado: {len(html_content)} bytes", file=sys.stderr, flush=True)
                        
                        # Parse
                        items = parse_prices(html_content, page.url)
                        print(f"[PLAYWRIGHT] Parsed {len(items)} items antes convers√£o", file=sys.stderr, flush=True)
                        
                        # Converter GBP para EUR
                        items = convert_items_gbp_to_eur(items)
                        print(f"[PLAYWRIGHT] {len(items)} items ap√≥s GBP‚ÜíEUR", file=sys.stderr, flush=True)
                        
                        # Aplicar ajustes
                        items = apply_price_adjustments(items, page.url)
                        print(f"[PLAYWRIGHT] {len(items)} items ap√≥s ajustes", file=sys.stderr, flush=True)
                        
                        if items:
                            print(f"[PLAYWRIGHT] ‚úÖ {len(items)} carros encontrados!", file=sys.stderr, flush=True)
                            if items:
                                print(f"[PLAYWRIGHT] Primeiro: {items[0].get('car', 'N/A')} - {items[0].get('price', 'N/A')}", file=sys.stderr, flush=True)
                            # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                            items = normalize_and_sort(items, supplier_priority=None)
                            return _no_store_json({
                                "ok": True,
                                "items": items,
                                "location": location,
                                "start_date": start_dt.date().isoformat(),
                                "start_time": start_dt.strftime("%H:%M"),
                                "end_date": end_dt.date().isoformat(),
                                "end_time": end_dt.strftime("%H:%M"),
                                "days": days,
                            })
                        else:
                            print(f"[PLAYWRIGHT] ‚ö†Ô∏è Parse retornou 0 items", file=sys.stderr, flush=True)
                    
                    finally:
                        await browser.close()
            
            except Exception as e:
                import sys
                print(f"[PLAYWRIGHT ERROR] {e}", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc(file=sys.stderr)
        
        # MODO DE TESTE LOCAL: Usar URL s/b pr√©-configurada
        test_url = None
        print(f"[DEBUG] TEST_MODE_LOCAL={TEST_MODE_LOCAL}, location={location.lower()}, days={days}")
        
        # Tentar carregar URLs din√¢micas do .env (FARO_XD, ALBUFEIRA_XD)
        if TEST_MODE_LOCAL == 0:
            loc_prefix = None
            if 'faro' in location.lower():
                loc_prefix = 'FARO'
            elif 'albufeira' in location.lower():
                loc_prefix = 'ALBUFEIRA'
            if loc_prefix:
                env_key = f"{loc_prefix}_{days}D"
                test_url = os.getenv(env_key, "").strip()
                if test_url and test_url.startswith('http'):
                    print(f"[DEBUG] Found dynamic URL in .env: {env_key}={test_url[:80]}...", file=sys.stderr, flush=True)
                else:
                    test_url = None
        
        if TEST_MODE_LOCAL == 1:
            print(f"[DEBUG] Checking location: faro={'faro' in location.lower()}, albufeira={'albufeira' in location.lower()}")
            if 'faro' in location.lower() and TEST_FARO_URL:
                test_url = TEST_FARO_URL
                print(f"[DEBUG] Using Faro test URL")
            elif 'albufeira' in location.lower() and TEST_ALBUFEIRA_URL:
                test_url = TEST_ALBUFEIRA_URL
                print(f"[DEBUG] Using Albufeira test URL")
        
        if test_url:
            try:
                import requests
                import sys
                print(f"[TEST MODE] Usando URL pr√©-configurada para {location}", file=sys.stderr, flush=True)
                r = requests.get(test_url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Cookie': 'monedaForzada=EUR; moneda=EUR; currency=EUR'
                }, timeout=15)
                
                print(f"[TEST MODE] Fetched {len(r.text)} bytes", file=sys.stderr, flush=True)
                # DEBUG: Save HTML
                try:
                    with open(DEBUG_DIR / f"test_mode_html_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html", 'w') as f:
                        f.write(r.text)
                except:
                    pass
                items = parse_prices(r.text, TEST_FARO_URL)
                print(f"[TEST MODE] Parsed {len(items)} items", file=sys.stderr, flush=True)
                if items:
                    print(f"[TEST MODE] Primeiro pre√ßo ANTES convers√£o: {items[0].get('price', 'N/A')}", file=sys.stderr, flush=True)
                # CONVERTER GBP‚ÜíEUR pois CarJet retorna em Libras
                items = convert_items_gbp_to_eur(items)
                print(f"[TEST MODE] After GBP‚ÜíEUR conversion: {len(items)} items", file=sys.stderr, flush=True)
                if items:
                    print(f"[TEST MODE] Primeiro pre√ßo DEPOIS convers√£o: {items[0].get('price', 'N/A')}", file=sys.stderr, flush=True)
                # Aplicar ajustes de pre√ßo se configurados
                items = apply_price_adjustments(items, test_url)
                print(f"[TEST MODE] After price adjustments: {len(items)} items", file=sys.stderr, flush=True)
                
                if items:
                    print(f"[TEST MODE] {len(items)} carros encontrados!", file=sys.stderr, flush=True)
                    # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                    items = normalize_and_sort(items, supplier_priority=None)
                    try:
                        prices = [float(item.get('price_num', 0)) for item in items if item.get('price_num')]
                        min_price = min(prices) if prices else None
                        max_price = max(prices) if prices else None
                        avg_price = sum(prices) / len(prices) if prices else None
                        
                        save_search_to_history(
                            location=location,
                            start_date=start_dt.date().isoformat(),
                            end_date=end_dt.date().isoformat(),
                            days=days,
                            results_count=len(items),
                            min_price=min_price,
                            max_price=max_price,
                            avg_price=avg_price,
                            user="admin",
                            search_params=f"lang={selected_language['name']}, hour={selected_hour}, device={selected_device['name']}"
                        )
                    except Exception as e:
                        print(f"[SELENIUM] Erro ao salvar hist√≥rico: {e}", file=sys.stderr, flush=True)
                    
                    # RETORNAR resultado final
                    return _no_store_json({
                        "ok": True,
                        "items": items,
                        "location": location,
                        "start_date": start_dt.date().isoformat(),
                        "start_time": start_dt.strftime("%H:%M"),
                        "end_date": end_dt.date().isoformat(),
                        "end_time": end_dt.strftime("%H:%M"),
                        "days": days,
                    })
            except Exception as e:
                import sys
                print(f"[TEST MODE ERROR] {e}", file=sys.stderr, flush=True)
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # M√âTODO 2: SELENIUM (PRINCIPAL - Mais confi√°vel) ‚úÖ RECOMENDADO
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Com todas as rota√ß√µes implementadas:
        # ‚úÖ Rota√ß√£o de horas - 14:30-17:00 (6 op√ß√µes)
        # ‚úÖ Rota√ß√£o de dispositivos - 4 devices mobile
        # ‚úÖ Rota√ß√£o de timezones - 4 europeus
        # ‚úÖ Rota√ß√£o de referrers - 5 op√ß√µes
        # ‚úÖ Cache clearing - Desativado completamente
        # ‚úÖ Seletor universal testado - #recogida_lista li:first-child a
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print(f"[SELENIUM] Iniciando scraping SIMPLES (igual ao teste) para {location}", file=sys.stderr, flush=True)
        try:
            # Usar fun√ß√£o simples que funciona 100% - N√ÉO BLOQUEANTE
            from selenium_simple import scrape_carjet_simple
            import asyncio
            
            result = await asyncio.to_thread(
                scrape_carjet_simple, location, start_dt, end_dt
            )
            
            if result.get('ok'):
                html_content = result.get('html')
                final_url = result.get('url')
                
                print(f"[SELENIUM] ‚úÖ Scraping simples bem-sucedido!", file=sys.stderr, flush=True)
                print(f"[SELENIUM] Fazendo parse de {len(html_content)} bytes...", file=sys.stderr, flush=True)
                
                items = parse_prices(html_content, final_url)
                print(f"[SELENIUM] Parsed {len(items)} items", file=sys.stderr, flush=True)
                items = convert_items_gbp_to_eur(items)
                print(f"[SELENIUM] {len(items)} ap√≥s GBP‚ÜíEUR", file=sys.stderr, flush=True)
                items = apply_price_adjustments(items, final_url)
                print(f"[SELENIUM] {len(items)} ap√≥s ajustes", file=sys.stderr, flush=True)
                
                if items:
                    print(f"[SELENIUM] ‚úÖ {len(items)} carros encontrados!", file=sys.stderr, flush=True)
                    items = normalize_and_sort(items, supplier_priority=None)
                    
                    # DEBUG: Verificar se campo photo est√° presente
                    photos_count = sum(1 for item in items if item.get('photo'))
                    print(f"[DEBUG] üì∏ Fotos encontradas: {photos_count}/{len(items)} carros ({(photos_count/len(items)*100):.1f}%)", file=sys.stderr, flush=True)
                    if photos_count > 0:
                        print(f"[DEBUG] üì∏ Exemplo de foto: {items[0].get('photo', 'N/A')[:100]}", file=sys.stderr, flush=True)
                    
                    return _no_store_json({
                        "ok": True,
                        "items": items,
                        "location": location,
                        "start_date": start_dt.date().isoformat(),
                        "start_time": start_dt.strftime("%H:%M"),
                        "end_date": end_dt.date().isoformat(),
                        "end_time": end_dt.strftime("%H:%M"),
                        "days": days,
                    })
            else:
                print(f"[SELENIUM] ‚ö†Ô∏è Scraping simples falhou: {result.get('error')}", file=sys.stderr, flush=True)
                
        except Exception as e:
            print(f"[SELENIUM] ‚ùå Erro no scraping simples: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
        
        # Se chegou aqui, tentar m√©todo antigo como fallback
        print(f"[SELENIUM] Tentando m√©todo complexo como fallback...", file=sys.stderr, flush=True)
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from webdriver_manager.chrome import ChromeDriverManager
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            import time
            
            # ============================================
            # ROTA√á√ÉO MULTI-IDIOMA (6 idiomas - Holand√™s removido por problemas)
            # ========== FOR√áAR PORTUGU√äS (IGUAL AO TESTE) ==========
            lang = 'pt'  # For√ßar portugu√™s
            selected_language = {
                'name': 'Portugu√™s',
                'url': 'https://www.carjet.com/aluguel-carros/index.htm',
                'faro': 'Faro Aeroporto (FAO)',
                'albufeira': 'Albufeira Cidade'
            }
            
            # Mapear location para formato CarJet
            carjet_location = location
            if 'faro' in location.lower():
                carjet_location = selected_language['faro']
            elif 'albufeira' in location.lower():
                carjet_location = selected_language['albufeira']
            
            carjet_url = selected_language['url']
            
            print(f"[SELENIUM] Idioma: {selected_language['name']} (FIXO - igual ao teste)", file=sys.stderr, flush=True)
            print(f"[SELENIUM] URL: {carjet_url}", file=sys.stderr, flush=True)
            print(f"[SELENIUM] Local: {carjet_location}", file=sys.stderr, flush=True)
            
            # ============================================
            # ROTA√á√ÉO DE DATAS (0-4 dias aleat√≥rio)
            # ============================================
            # random j√° importado globalmente
            from datetime import timedelta as td
            
            # Adicionar offset aleat√≥rio de 0-4 dias √†s datas
            date_offset = random.randint(0, 4)
            start_dt = start_dt + td(days=date_offset)
            end_dt = end_dt + td(days=date_offset)
            
            print(f"[SELENIUM] Offset de datas: +{date_offset} dias", file=sys.stderr, flush=True)
            print(f"[SELENIUM] Datas ajustadas: {start_dt.date()} - {end_dt.date()}", file=sys.stderr, flush=True)
            
            # ============================================
            # ROTA√á√ÉO DE HORAS (14:30-17:00 aleat√≥rio)
            # ============================================
            # Horas dispon√≠veis no Carjet (de 30 em 30 minutos)
            available_hours = ['14:30', '15:00', '15:30', '16:00', '16:30', '17:00']
            selected_hour = random.choice(available_hours)
            
            # Ajustar start_dt e end_dt para usar a hora selecionada
            start_dt = start_dt.replace(hour=int(selected_hour.split(':')[0]), minute=int(selected_hour.split(':')[1]))
            end_dt = end_dt.replace(hour=int(selected_hour.split(':')[0]), minute=int(selected_hour.split(':')[1]))
            
            print(f"[SELENIUM] Hora selecionada: {selected_hour}", file=sys.stderr, flush=True)
            
            # ============================================
            # FOR√áAR DEVICE E REFERRER IGUAIS AO TESTE
            # ============================================
            
            selected_device = {
                'name': 'iPhone 13 Pro',
                'ua': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
                'width': 390,
                'height': 844,
                'pixelRatio': 3.0
            }
            
            selected_timezone = 'Europe/Lisbon'
            selected_referrer = ''  # SEM REFERRER (igual ao teste)
            
            print(f"[SELENIUM] Device: {selected_device['name']}", file=sys.stderr, flush=True)
            print(f"[SELENIUM] Timezone: {selected_timezone}", file=sys.stderr, flush=True)
            print(f"[SELENIUM] Language: {selected_language['name']}", file=sys.stderr, flush=True)
            print(f"[SELENIUM] Referrer: {selected_referrer if selected_referrer else 'Direct'}", file=sys.stderr, flush=True)
            
            chrome_options = Options()
            # chrome_options.add_argument('--headless')  # ‚ùå DESATIVADO - Igual ao teste manual!
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument(f'user-agent={selected_device["ua"]}')
            
            # EMULA√á√ÉO MOBILE COMPLETA com device espec√≠fico
            mobile_emulation = {
                "deviceMetrics": { 
                    "width": selected_device['width'], 
                    "height": selected_device['height'], 
                    "pixelRatio": selected_device['pixelRatio']
                },
                "userAgent": selected_device['ua']
            }
            chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)
            
            # Anti-dete√ß√£o
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # Prefer√™ncias m√≠nimas (IGUAL AO TESTE - sem bloquear cookies!)
            # N√ÉO bloquear cookies - pode fazer CarJet comportar-se diferente!
            # chrome_options.add_experimental_option("prefs", {
            #     "intl.accept_languages": selected_lang,
            # })
            
            # Detectar sistema operacional e definir caminho do Chrome
            import platform
            system = platform.system()
            if system == 'Darwin':  # macOS
                chrome_options.binary_location = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
            elif system == 'Linux':  # Linux (Render)
                # No Linux com Docker, o Chrome est√° em /usr/bin/google-chrome-stable
                if os.path.exists('/usr/bin/google-chrome-stable'):
                    chrome_options.binary_location = '/usr/bin/google-chrome-stable'
                # N√£o definir binary_location deixa o Selenium encontrar automaticamente
            
            # Iniciar driver - tentar com Chrome instalado primeiro
            try:
                # Tentar usar Chrome do sistema (melhor para Mac ARM)
                driver = webdriver.Chrome(options=chrome_options)
                print(f"[SELENIUM] ‚úÖ Chrome iniciado com sucesso!", file=sys.stderr, flush=True)
            except Exception as e:
                print(f"[SELENIUM] ‚ö†Ô∏è Erro ao iniciar Chrome: {e}", file=sys.stderr, flush=True)
                print(f"[SELENIUM] Tentando com ChromeDriverManager...", file=sys.stderr, flush=True)
                # Fallback para ChromeDriverManager
                driver = webdriver.Chrome(
                    service=Service(ChromeDriverManager().install()),
                    options=chrome_options
                )
            
            # FUN√á√ÉO HELPER: Autodetectar e REJEITAR cookies (mais simples!)
            def reject_cookies_if_present(step_name=""):
                """Detecta e REJEITA cookies automaticamente. Retorna True se encontrou cookies."""
                try:
                    result = driver.execute_script("""
                        // Procurar e clicar no bot√£o de REJEITAR cookies
                        const buttons = document.querySelectorAll('button, a, [role="button"]');
                        let found = false;
                        for (let btn of buttons) {
                            const text = btn.textContent.toLowerCase().trim();
                            // Procurar por "rejeitar", "recusar", "reject", etc.
                            if (text.includes('rejeitar') || text.includes('recusar') || 
                                text.includes('reject') || text.includes('rechazar') ||
                                text.includes('weiger') || text.includes('afwijzen') ||  // Holand√™s
                                text.includes('n√£o aceitar') || text.includes('decline')) {
                                btn.click();
                                console.log('‚úì Cookies rejeitados:', btn.textContent);
                                found = true;
                                break;
                            }
                        }
                        // Se n√£o encontrou bot√£o de rejeitar, tentar fechar/remover o banner
                        if (!found) {
                            document.querySelectorAll('[id*=cookie], [class*=cookie], [id*=didomi], [class*=didomi], [id*=consent], [class*=consent]').forEach(el => {
                                el.remove();
                            });
                        }
                        document.body.style.overflow = 'auto';
                        return found;
                    """)
                    if result:
                        print(f"[SELENIUM] ‚úì Cookies rejeitados {step_name}", file=sys.stderr, flush=True)
                    else:
                        print(f"[SELENIUM] ‚ÑπÔ∏è  Banner removido {step_name}", file=sys.stderr, flush=True)
                    return result
                except Exception as e:
                    print(f"[SELENIUM] ‚ö† Erro ao verificar cookies {step_name}: {e}", file=sys.stderr, flush=True)
                    return False
            
            # TIMEOUT GLOBAL: Se ficar preso mais de 60 segundos, abortar
            import signal
            def timeout_handler(signum, frame):
                raise TimeoutError("Selenium ficou preso por mais de 60 segundos!")
            
            # Configurar timeout (apenas em sistemas Unix)
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(60)  # 60 segundos de timeout
            except:
                pass  # Windows n√£o suporta SIGALRM
            
            try:
                # Esconder webdriver
                driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                    'source': '''
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => undefined
                        });
                    '''
                })
                
                print(f"[SELENIUM] Configurando Chrome com mobile UA...", file=sys.stderr, flush=True)
                driver.set_page_load_timeout(20)  # Igual ao teste manual
                
                # Definir referrer se n√£o for direct
                if selected_referrer:
                    print(f"[SELENIUM] Definindo referrer...", file=sys.stderr, flush=True)
                    driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {
                        'headers': {'Referer': selected_referrer}
                    })
                
                print(f"[SELENIUM] Acessando CarJet ({selected_language['name']})...", file=sys.stderr, flush=True)
                driver.get(carjet_url)
                
                # Rejeitar cookies (IGUAL AO TESTE)
                time.sleep(0.5)
                if reject_cookies_if_present(""):
                    print(f"[SELENIUM] ‚úÖ Cookies rejeitados", file=sys.stderr, flush=True)
                time.sleep(0.5)
                
                # DETECTAR IDIOMA e ajustar nome da localiza√ß√£o
                page_url = driver.current_url
                print(f"[SELENIUM] URL atual: {page_url}", file=sys.stderr, flush=True)
                
                # Se estiver em ingl√™s, usar nomes em ingl√™s
                if '/index.htm' in page_url and '/aluguel-carros/' not in page_url and '/aluguer-carros/' not in page_url:
                    print(f"[SELENIUM] P√°gina em INGL√äS detectada!", file=sys.stderr, flush=True)
                    if carjet_location == 'Albufeira Cidade':
                        carjet_location = 'Albufeira City'
                    elif carjet_location == 'Faro Aeroporto':
                        carjet_location = 'Faro Airport'
                    print(f"[SELENIUM] Local ajustado para: {carjet_location}", file=sys.stderr, flush=True)
                
                try:
                    # ========== ORDEM CORRETA DO TESTE: LOCAL ‚Üí DROPDOWN ‚Üí DATAS ==========
                    
                    # PASSO 1: Escrever local (IGUAL AO TESTE)
                    print(f"[SELENIUM] PASSO 1: Escrevendo local...", file=sys.stderr, flush=True)
                    pickup_input = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.ID, "pickup"))
                    )
                    pickup_input.clear()
                    pickup_input.send_keys(carjet_location)
                    print(f"[SELENIUM] ‚úì Local digitado", file=sys.stderr, flush=True)
                    
                    # PASSO 2: Aguardar dropdown e clicar
                    print(f"[SELENIUM] PASSO 2: Aguardando dropdown...", file=sys.stderr, flush=True)
                    time.sleep(3)
                    
                    try:
                        dropdown_item = WebDriverWait(driver, 3).until(
                            EC.element_to_be_clickable((By.CSS_SELECTOR, "#recogida_lista li:first-child a"))
                        )
                        dropdown_item.click()
                        print(f"[SELENIUM] ‚úì Dropdown clicado", file=sys.stderr, flush=True)
                    except:
                        driver.execute_script("""
                            const items = document.querySelectorAll('#recogida_lista li');
                            for (let item of items) {
                                if (item.offsetParent !== null) {
                                    item.click();
                                    return true;
                                }
                            }
                        """)
                        print(f"[SELENIUM] ‚úì Dropdown clicado (JS)", file=sys.stderr, flush=True)
                    
                    time.sleep(1)
                    
                    # PASSO 3: Preencher datas e horas (DEPOIS do dropdown!)
                    print(f"[SELENIUM] PASSO 3: Preenchendo datas e horas...", file=sys.stderr, flush=True)
                    
                    result = driver.execute_script("""
                        function fill(sel, val) {
                            const el = document.querySelector(sel);
                            if (el) { 
                                el.value = val; 
                                el.dispatchEvent(new Event('input', {bubbles: true}));
                                el.dispatchEvent(new Event('change', {bubbles: true}));
                                el.dispatchEvent(new Event('blur', {bubbles: true}));  // ‚Üê IGUAL AO TESTE!
                                console.log('Preenchido:', sel, '=', val);
                                return true;
                            }
                            console.error('N√£o encontrado:', sel);
                            return false;
                        }
                        
                        // Preencher datas
                        const r1 = fill('input[id="fechaRecogida"]', arguments[0]);
                        const r2 = fill('input[id="fechaDevolucion"]', arguments[1]);
                        
                        // Preencher horas
                        const h1 = document.querySelector('select[id="fechaRecogidaSelHour"]');
                        let h1_ok = false;
                        if (h1) { 
                            h1.value = arguments[2]; 
                            h1.dispatchEvent(new Event('change', {bubbles: true}));
                            console.log('Hora recolha:', h1.value);
                            h1_ok = true;
                        }
                        
                        const h2 = document.querySelector('select[id="fechaDevolucionSelHour"]');
                        let h2_ok = false;
                        if (h2) { 
                            h2.value = arguments[3]; 
                            h2.dispatchEvent(new Event('change', {bubbles: true}));
                            console.log('Hora devolu√ß√£o:', h2.value);
                            h2_ok = true;
                        }
                        
                        return {
                            fechaRecogida: r1,
                            fechaDevolucion: r2,
                            horaRecogida: h1_ok,
                            horaDevolucion: h2_ok,
                            allFilled: r1 && r2 && h1_ok && h2_ok
                        };
                    """, start_dt.strftime("%d/%m/%Y"), end_dt.strftime("%d/%m/%Y"), start_dt.strftime("%H:%M"), start_dt.strftime("%H:%M"))
                    
                    print(f"[SELENIUM] ‚úì Datas e horas preenchidas: {result}", file=sys.stderr, flush=True)
                    
                except Exception as e:
                    print(f"[SELENIUM] Erro ao preencher: {e}", file=sys.stderr, flush=True)
                
                # PASSO 4: Submit (IGUAL AO TESTE)
                print(f"[SELENIUM] PASSO 4: Submetendo...", file=sys.stderr, flush=True)
                driver.execute_script("window.scrollBy(0, 300);")
                time.sleep(0.5)
                driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(0.5)
                driver.execute_script("document.querySelector('form').submit();")
                
                print(f"[SELENIUM] Aguardando navega√ß√£o inicial...", file=sys.stderr, flush=True)
                time.sleep(3)
                
                # Aguardar at√© que a URL contenha /do/list/ (resultado final)
                print(f"[SELENIUM] Aguardando p√°gina de resultados...", file=sys.stderr, flush=True)
                max_wait = 40  # 40 segundos m√°ximo
                waited = 0
                while waited < max_wait:
                    current_url = driver.current_url
                    if '/do/list/' in current_url and 's=' in current_url and 'b=' in current_url:
                        print(f"[SELENIUM] ‚úÖ P√°gina de resultados carregada ap√≥s {waited}s", file=sys.stderr, flush=True)
                        break
                    else:
                        print(f"[SELENIUM] Aguardando... URL atual: {current_url[:80]}... ({waited}s)", file=sys.stderr, flush=True)
                        time.sleep(3)
                        waited += 3
                
                # Aguardar mais um pouco para garantir que o conte√∫do carregou
                print(f"[SELENIUM] Aguardando conte√∫do carregar...", file=sys.stderr, flush=True)
                time.sleep(5)
                
                final_url = driver.current_url
                
                # DEBUG: Salvar URL e HTML para an√°lise
                try:
                    import sys
                    print(f"[SELENIUM] URL final: {final_url}", file=sys.stderr, flush=True)
                    with open(DEBUG_DIR / f"selenium_url_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", 'w') as f:
                        f.write(f"Final URL: {final_url}\n")
                        f.write(f"Has s=: {'s=' in final_url}\n")
                        f.write(f"Has b=: {'b=' in final_url}\n")
                except:
                    pass
                
                # N√ÉO fazer retry com war= - aceitar o resultado como est√°
                # O retry pode causar cliques extras e bagun√ßar o formul√°rio
                if 'war=' in final_url:
                    print(f"[SELENIUM] ‚ö†Ô∏è war= detectado (sem disponibilidade ou erro)", file=sys.stderr, flush=True)
                
                # Se obtivemos URL s/b v√°lida, pegar HTML do driver ANTES de fechar
                if 's=' in final_url and 'b=' in final_url:
                    print(f"[SELENIUM] ‚úÖ URL s/b obtida! Pegando HTML do driver...", file=sys.stderr, flush=True)
                    
                    # Usar page_source do driver em vez de fazer novo request
                    html_content = driver.page_source
                    
                    # Agora pode fechar o driver
                    driver.quit()
                    
                    print(f"[SELENIUM] Fazendo parse de {len(html_content)} bytes...", file=sys.stderr, flush=True)
                    items = parse_prices(html_content, final_url)
                    print(f"[SELENIUM] Parsed {len(items)} items", file=sys.stderr, flush=True)
                    items = convert_items_gbp_to_eur(items)
                    print(f"[SELENIUM] {len(items)} ap√≥s GBP‚ÜíEUR", file=sys.stderr, flush=True)
                    items = apply_price_adjustments(items, final_url)
                    print(f"[SELENIUM] {len(items)} ap√≥s ajustes", file=sys.stderr, flush=True)
                    
                    if items:
                        print(f"[SELENIUM] ‚úÖ {len(items)} carros encontrados!", file=sys.stderr, flush=True)
                        # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                        items = normalize_and_sort(items, supplier_priority=None)
                        # SUCESSO! Retornar resultados
                        return _no_store_json({
                            "ok": True,
                            "items": items,
                            "location": location,
                            "start_date": start_dt.date().isoformat(),
                            "start_time": start_dt.strftime("%H:%M"),
                            "end_date": end_dt.date().isoformat(),
                            "end_time": end_dt.strftime("%H:%M"),
                            "days": days,
                        })
                else:
                    print(f"[SELENIUM] ‚ö†Ô∏è URL s/b N√ÉO obtida! URL: {final_url}", file=sys.stderr, flush=True)
                    driver.quit()
                    # Fallback: tentar POST direto para /do/list/{lang}
                    try:
                        import requests
                        payload = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                        headers_dp = {
                            "Origin": "https://www.carjet.com",
                            "Referer": f"https://www.carjet.com/do/list/{lang}",
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                            "Accept-Language": "pt-PT,pt;q=0.9,en;q=0.6",
                            "Cookie": "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt",
                        }
                        rdp = requests.post(f"https://www.carjet.com/do/list/{lang}", data=payload, headers=headers_dp, timeout=20)
                        if rdp.status_code == 200 and (rdp.text or '').strip():
                            html_dp = rdp.text
                            its_dp = parse_prices(html_dp, f"https://www.carjet.com/do/list/{lang}")
                            its_dp = convert_items_gbp_to_eur(its_dp)
                            its_dp = apply_price_adjustments(its_dp, f"https://www.carjet.com/do/list/{lang}")
                            if its_dp:
                                print(f"[SELENIUM] ‚úÖ Fallback POST retornou {len(its_dp)} carros", file=sys.stderr, flush=True)
                                # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
                                its_dp = normalize_and_sort(its_dp, supplier_priority=None)
                                return _no_store_json({
                                    "ok": True,
                                    "items": its_dp,
                                    "location": location,
                                    "start_date": start_dt.date().isoformat(),
                                    "start_time": start_dt.strftime("%H:%M"),
                                    "end_date": end_dt.date().isoformat(),
                                    "end_time": end_dt.strftime("%H:%M"),
                                    "days": days,
                                })
                    except Exception as _e:
                        print(f"[SELENIUM] Fallback POST erro: {_e}", file=sys.stderr, flush=True)
                    # Se ainda sem resultados, retornar vazio rapidamente para permitir nova tentativa
                    return _no_store_json({
                        "ok": True,
                        "items": [],
                        "location": location,
                        "start_date": start_dt.date().isoformat(),
                        "start_time": start_dt.strftime("%H:%M"),
                        "end_date": end_dt.date().isoformat(),
                        "end_time": end_dt.strftime("%H:%M"),
                        "days": days,
                    })
            except TimeoutError as e:
                print(f"[SELENIUM TIMEOUT] ‚è∞ Ficou preso! {e}", file=sys.stderr, flush=True)
                print(f"[SELENIUM TIMEOUT] Fechando Chrome e abortando...", file=sys.stderr, flush=True)
                try:
                    driver.quit()
                except:
                    pass
                # Cancelar alarm
                try:
                    signal.alarm(0)
                except:
                    pass
                # RETORNAR vazio em caso de timeout
                return _no_store_json({
                    "ok": True,
                    "items": [],
                    "location": location,
                    "start_date": start_dt.date().isoformat(),
                    "start_time": start_dt.strftime("%H:%M"),
                    "end_date": end_dt.date().isoformat(),
                    "end_time": end_dt.strftime("%H:%M"),
                    "days": days,
                    "warning": "Timeout: Scraping ficou preso e foi abortado"
                })
            except Exception as e:
                print(f"[SELENIUM ERROR interno] {e}", file=sys.stderr, flush=True)
                try:
                    driver.quit()
                except:
                    pass
                # Cancelar alarm
                try:
                    signal.alarm(0)
                except:
                    pass
                # RETORNAR vazio em caso de erro tamb√©m
                return _no_store_json({
                    "ok": True,
                    "items": [],
                    "location": location,
                    "start_date": start_dt.date().isoformat(),
                    "start_time": start_dt.strftime("%H:%M"),
                    "end_date": end_dt.date().isoformat(),
                    "end_time": end_dt.strftime("%H:%M"),
                    "days": days,
                })
        except Exception as e:
            print(f"[SELENIUM ERROR] {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc(file=sys.stderr)
            # Cancelar alarm
            try:
                signal.alarm(0)
            except:
                pass
            # RETORNAR vazio para n√£o travar
            return _no_store_json({
                "ok": True,
                "items": [],
                "location": location,
                "start_date": start_dt.date().isoformat(),
                "start_time": start_dt.strftime("%H:%M"),
                "end_date": end_dt.date().isoformat(),
                "end_time": end_dt.strftime("%H:%M"),
                "days": days,
            })
        
        # Fallback se Playwright falhou (N√ÉO DEVE CHEGAR AQUI SE SELENIUM FALHOU!)
        if USE_PLAYWRIGHT and _HAS_PLAYWRIGHT:
            try:
                from playwright.async_api import async_playwright
                async with async_playwright() as p:
                    # Chromium-first strategy
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context(
                        locale="pt-PT",
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
                    )
                    page = await context.new_page()
                    captured_bodies: List[str] = []
                    async def _on_resp(resp):
                        try:
                            u = resp.url or ""
                            if any(k in u for k in ("modalFilter.asp", "carList.asp", "/do/list/pt", "filtroUso.asp")):
                                try:
                                    t = await resp.text()
                                    if t:
                                        captured_bodies.append(t)
                                        try:
                                            stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                            name = "pw-capture-" + re.sub(r"[^a-z0-9]+", "-", u.lower())[-60:]
                                            (DEBUG_DIR / f"{name}-{stamp}.html").write_text(t, encoding='utf-8')
                                        except Exception:
                                            pass
                                except Exception:
                                    pass
                        except Exception:
                            pass
                    page.on("response", _on_resp)
                    # 1) Open homepage to mint session
                    home_path = "aluguel-carros/index.htm" if lang.lower()=="pt" else "index.htm"
                    await page.goto(f"https://www.carjet.com/{home_path}", wait_until="networkidle", timeout=35000)
                    # Handle consent if present
                    try:
                        for sel in [
                            "#didomi-notice-agree-button",
                            ".didomi-continue-without-agreeing",
                            "button:has-text('Aceitar')",
                            "button:has-text('I agree')",
                            "button:has-text('Accept')",
                        ]:
                            try:
                                c = page.locator(sel)
                                if await c.count() > 0:
                                    try: await c.first.click(timeout=1500)
                                    except Exception: pass
                                    await page.wait_for_timeout(200)
                                    break
                            except Exception:
                                pass
                    except Exception:
                        pass
                    # 2) Type exact location as in browser autosuggest, then submit form programmatically
                    try:
                        exact_loc = location
                        lo = (location or '').lower()
                        if 'albufeira' in lo:
                            exact_loc = 'Albufeira Cidade'
                        elif 'faro' in lo:
                            exact_loc = 'Faro Aeroporto (FAO)'
                        # Try common selectors for the location input
                        loc_inp = None
                        for sel in ["input[name='pickup']", "#pickup", "input[placeholder*='local' i]", "input[aria-label*='local' i]", "input[type='search']"]:
                            try:
                                h = await page.query_selector(sel)
                                if h:
                                    loc_inp = h; break
                            except Exception:
                                pass
                        if loc_inp:
                            try:
                                await loc_inp.click()
                                await loc_inp.fill("")
                                await loc_inp.type(exact_loc, delay=50)
                                # Wait for dropdown and click the exact match if visible
                                try:
                                    opt = page.locator(f"text={exact_loc}")
                                    if await opt.count() > 0:
                                        await opt.first.click(timeout=2000)
                                except Exception:
                                    # fallback: press Enter to accept first suggestion
                                    try:
                                        await loc_inp.press('Enter')
                                    except Exception:
                                        pass
                                await page.wait_for_timeout(300)
                            except Exception:
                                pass
                    except Exception:
                        pass
                    # 2.b) Fill pickup/dropoff dates and times via visible inputs (simulate human typing)
                    try:
                        pickup_dmY = start_dt.strftime('%d/%m/%Y')
                        dropoff_dmY = end_dt.strftime('%d/%m/%Y')
                        pickup_HM = start_dt.strftime('%H:%M')
                        dropoff_HM = end_dt.strftime('%H:%M')
                        # Try native calendar UI first using the known triggers
                        async def select_date_via_picker(trigger_alt: str, target_dmY: str):
                            try:
                                trig = page.locator(f"img.ui-datepicker-trigger[alt='{trigger_alt}']")
                                if await trig.count() > 0:
                                    await trig.first.click()
                                    # Parse target day/month/year
                                    td, tm, ty = target_dmY.split('/')
                                    # Max 12 next clicks safeguard
                                    for _ in range(13):
                                        try:
                                            title = await page.locator('.ui-datepicker-title').inner_text()
                                        except Exception:
                                            title = ''
                                        # Title like 'Outubro 2025'
                                        ok_month = (tm in target_dmY)  # coarse guard; we do direct day pick below
                                        # Try clicking the exact day link
                                        day_locator = page.locator(f".ui-datepicker-calendar td a:text-is('{int(td)}')")
                                        if await day_locator.count() > 0:
                                            try:
                                                await day_locator.first.click()
                                                await page.wait_for_timeout(200)
                                                break
                                            except Exception:
                                                pass
                                        # Navigate next month
                                        try:
                                            nxt = page.locator('.ui-datepicker-next')
                                            if await nxt.count() > 0:
                                                await nxt.first.click()
                                                await page.wait_for_timeout(150)
                                            else:
                                                break
                                        except Exception:
                                            break
                            except Exception:
                                pass
                        await select_date_via_picker('Data de recolha', pickup_dmY)
                        await select_date_via_picker('Data de entrega', dropoff_dmY)
                        fill_dates_js = """
                          (pDate, pTime, dDate, dTime) => {
                            const setVal = (sel, val) => { const el = document.querySelector(sel); if (!el) return false; el.focus(); el.value = val; el.dispatchEvent(new Event('input', {bubbles:true})); el.dispatchEvent(new Event('change', {bubbles:true})); return true; };
                            const tryAll = (sels, val) => { for (const s of sels) { if (setVal(s, val)) return true; } return false; };
                            // Pickup date/time candidates
                            tryAll(['#fechaRecogida','input[name=fechaRecogida]','input[name=pickupDate]','input[type=date][name*=recog]','input[type=date][name*=pickup]','input[placeholder*="recolh" i]','input[aria-label*="recolh" i]'], pDate);
                            tryAll(['#fechaRecogidaSelHour','input[name=fechaRecogidaSelHour]','input[name=pickupTime]','input[type=time][name*=recog]','input[type=time][name*=pickup]','#h-recogida'], pTime);
                            // Dropoff date/time candidates
                            tryAll(['#fechaEntrega','#fechaDevolucion','input[name=fechaEntrega]','input[name=fechaDevolucion]','input[name=dropoffDate]','input[type=date][name*=entreg]','input[type=date][name*=devol]','input[placeholder*="entreg" i]','input[aria-label*="entreg" i]'], dDate);
                            tryAll(['#fechaEntregaSelHour','#fechaDevolucionSelHour','input[name=fechaEntregaSelHour]','input[name=fechaDevolucionSelHour]','input[name=dropoffTime]','input[type=time][name*=entreg]','input[type=time][name*=devol]','input[type=time][name*=drop]','#h-devolucion'], dTime);
                          }
                        """
                        await page.evaluate(fill_dates_js, pickup_dmY, pickup_HM, dropoff_dmY, dropoff_HM)
                        await page.wait_for_timeout(300)
                    except Exception:
                        pass
                    # Programmatic submit with full payload to ensure consistent parameters
                    payload = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                    submit_js = """
                      (url, data) => {
                        const f = document.createElement('form');
                        f.method = 'POST';
                        f.action = url;
                        for (const [k,v] of Object.entries(data||{})) {
                          const i = document.createElement('input');
                          i.type = 'hidden'; i.name = k; i.value = String(v ?? '');
                          f.appendChild(i);
                        }
                        document.body.appendChild(f);
                        f.submit();
                      }
                    """
                    await page.evaluate(submit_js, f"https://www.carjet.com/do/list/{lang}", payload)
                    try:
                        await page.wait_for_load_state("networkidle", timeout=40000)
                    except Exception:
                        pass
                    # Additionally trigger native on-page submit to mimic button onclick
                    try:
                        await page.evaluate("""
                          try {
                            if (typeof comprobar_errores_3 === 'function') {
                              if (comprobar_errores_3()) {
                                if (typeof filtroUsoForm === 'function') filtroUsoForm();
                                if (typeof submit_fechas === 'function') submit_fechas('/do/list/pt');
                              }
                            }
                          } catch (e) {}
                        """)
                        try:
                            await page.wait_for_load_state("networkidle", timeout=40000)
                        except Exception:
                            pass
                    except Exception:
                        pass
                    # 3-5) Up to 3 cycles: click 'Pesquisar' (if present), scroll, wait for results
                    for _ in range(3):
                        try:
                            btn = page.locator("button:has-text('Pesquisar'), button:has-text('Buscar'), input[type=submit], button[type=submit]")
                            if await btn.count() > 0:
                                try:
                                    await btn.first.click(timeout=3000)
                                except Exception:
                                    pass
                        except Exception:
                            pass
                        try:
                            await page.wait_for_load_state("networkidle", timeout=40000)
                        except Exception:
                            pass
                        # Best-effort screenshot after search click (skip if quick=1)
                        if not quick:
                            try:
                                stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                await page.screenshot(path=str(DEBUG_DIR / f"pw-after-search-{stamp}.png"), full_page=True)
                            except Exception:
                                pass
                        try:
                            for __ in range(5):
                                try:
                                    await page.mouse.wheel(0, 1600)
                                except Exception:
                                    pass
                                await page.wait_for_timeout(300)
                        except Exception:
                            pass
                        # Check if results appeared; if so, break
                        try:
                            ok = await page.locator("section.newcarlist article, .newcarlist article, article.car, li.result, li.car, .car-item, .result-row").count()
                            if (ok or 0) > 0:
                                break
                        except Exception:
                            pass
                    # Wait specifically for known backend calls and dump responses
                    mf_body = ""; cl_body = ""
                    try:
                        resp_mf = await page.wait_for_response(lambda r: 'modalFilter.asp' in (r.url or ''), timeout=40000)
                        try:
                            mf_body = await resp_mf.text()
                            if mf_body:
                                stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                (DEBUG_DIR / f"pw-modalFilter-{stamp}.html").write_text(mf_body, encoding='utf-8')
                        except Exception:
                            pass
                    except Exception:
                        pass
                    try:
                        resp_cl = await page.wait_for_response(lambda r: 'carList.asp' in (r.url or ''), timeout=40000)
                        try:
                            cl_body = await resp_cl.text()
                            if cl_body:
                                stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                (DEBUG_DIR / f"pw-carList-{stamp}.html").write_text(cl_body, encoding='utf-8')
                        except Exception:
                            pass
                    except Exception:
                        pass
                    html_pw = await page.content()
                    final_url = page.url
                    print(f"[DEBUG] Fechando browser, URL final: {final_url}", file=sys.stderr, flush=True)
                    await context.close(); await browser.close()
                    print(f"[DEBUG] Browser fechado, HTML size: {len(html_pw)} bytes", file=sys.stderr, flush=True)
                if html_pw:
                    items_pw = parse_prices(html_pw, final_url or base)
                    items_pw = convert_items_gbp_to_eur(items_pw)
                    items_pw = apply_price_adjustments(items_pw, final_url or base)
                    items = items_pw
                # If still empty, try parsing network-captured bodies
                if (not items) and (cl_body or mf_body):
                    try:
                        base_net = "https://www.carjet.com/do/list/pt"
                        if cl_body:
                            its = parse_prices(cl_body, base_net)
                            its = convert_items_gbp_to_eur(its)
                            its = apply_price_adjustments(its, base_net)
                            if its:
                                items = its
                        if (not items) and mf_body:
                            its2 = parse_prices(mf_body, base_net)
                            its2 = convert_items_gbp_to_eur(its2)
                            its2 = apply_price_adjustments(its2, base_net)
                            if its2:
                                items = its2
                    except Exception:
                        pass
                if (not items) and captured_bodies:
                    try:
                        base_net = "https://www.carjet.com/do/list/pt"
                        for body in captured_bodies:
                            its = parse_prices(body, base_net)
                            its = convert_items_gbp_to_eur(its)
                            its = apply_price_adjustments(its, base_net)
                            if its:
                                items = its
                                break
                    except Exception:
                        pass
                # Final fallback: if we ended on a CarJet list URL, delegate to URL-based compute
                try:
                    if (not items) and (final_url or '').startswith("https://www.carjet.com/do/list/"):
                        data_f = await _compute_prices_for(final_url)
                        its_f = (data_f or {}).get('items') or []
                        if its_f:
                            items = its_f
                except Exception:
                    pass
                # Direct POST fallback within Playwright session
                try:
                    if not items:
                        payload_dp = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                        rdp = await page.request.post(f"https://www.carjet.com/do/list/{lang}", data=payload_dp)
                        try:
                            html_dp = await rdp.text()
                        except Exception:
                            html_dp = ""
                        if html_dp:
                            try:
                                stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                (DEBUG_DIR / f"pw-direct-post-{stamp}.html").write_text(html_dp, encoding='utf-8')
                            except Exception:
                                pass
                            its_dp = parse_prices(html_dp, f"https://www.carjet.com/do/list/{lang}")
                            its_dp = convert_items_gbp_to_eur(its_dp)
                            its_dp = apply_price_adjustments(its_dp, f"https://www.carjet.com/do/list/{lang}")
                            if its_dp:
                                items = its_dp
                except Exception:
                    pass
                # Engine fallback: if Chromium didn't produce items, try WebKit with Safari UA
                if (not items) and USE_PLAYWRIGHT and _HAS_PLAYWRIGHT:
                    try:
                        from playwright.async_api import async_playwright
                        async with async_playwright() as p2:
                            browser2 = await p2.webkit.launch(headless=True)
                            context2 = await browser2.new_context(
                                locale="pt-PT",
                                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.0.1 Safari/605.1.15",
                            )
                            page2 = await context2.new_page()
                            captured2: List[str] = []
                            async def _on_resp2(resp):
                                try:
                                    u = resp.url or ""
                                    if any(k in u for k in ("modalFilter.asp", "carList.asp", "/do/list/pt", "filtroUso.asp")):
                                        try:
                                            t = await resp.text()
                                            if t:
                                                captured2.append(t)
                                                try:
                                                    stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                                    name = "pw2-capture-" + re.sub(r"[^a-z0-9]+", "-", u.lower())[-60:]
                                                    (DEBUG_DIR / f"{name}-{stamp}.html").write_text(t, encoding='utf-8')
                                                except Exception:
                                                    pass
                                        except Exception:
                                            pass
                                except Exception:
                                    pass
                            page2.on("response", _on_resp2)
                            # Open homepage and perform same submission
                            home_path2 = "aluguel-carros/index.htm" if lang.lower()=="pt" else "index.htm"
                            await page2.goto(f"https://www.carjet.com/{home_path2}", wait_until="networkidle", timeout=35000)
                            # Type exact location per autosuggest
                            try:
                                exact_loc2 = location
                                lo2 = (location or '').lower()
                                if 'albufeira' in lo2:
                                    exact_loc2 = 'Albufeira Cidade'
                                elif 'faro' in lo2:
                                    exact_loc2 = 'Faro Aeroporto (FAO)'
                                loc2 = None
                                for sel in ["input[name='pickup']", "#pickup", "input[placeholder*='local' i]", "input[aria-label*='local' i]", "input[type='search']"]:
                                    try:
                                        h = await page2.query_selector(sel)
                                        if h:
                                            loc2 = h; break
                                    except Exception:
                                        pass
                                if loc2:
                                    try:
                                        await loc2.click(); await loc2.fill(""); await loc2.type(exact_loc2, delay=50)
                                        try:
                                            opt2 = page2.locator(f"text={exact_loc2}")
                                            if await opt2.count() > 0:
                                                await opt2.first.click(timeout=2000)
                                        except Exception:
                                            try: await loc2.press('Enter')
                                            except Exception: pass
                                        await page2.wait_for_timeout(300)
                                    except Exception:
                                        pass
                            except Exception:
                                pass
                            # Dates and hours
                            try:
                                pickup_dmY2 = start_dt.strftime('%d/%m/%Y')
                                dropoff_dmY2 = end_dt.strftime('%d/%m/%Y')
                                pickup_HM2 = start_dt.strftime('%H:%M')
                                dropoff_HM2 = end_dt.strftime('%H:%M')
                                fill_js2 = """
                                  (pDate, pTime, dDate, dTime) => {
                                    const setVal = (sel, val) => { const el = document.querySelector(sel); if (!el) return false; el.focus(); el.value = val; el.dispatchEvent(new Event('input', {bubbles:true})); el.dispatchEvent(new Event('change', {bubbles:true})); return true; };
                                    const tryAll = (sels, val) => { for (const s of sels) { if (setVal(s, val)) return true; } return false; };
                                    tryAll(['#fechaRecogida','input[name=fechaRecogida]','input[name=pickupDate]','input[type=date][name*=recog]','input[type=date][name*=pickup]','input[placeholder*="recolh" i]','input[aria-label*="recolh" i]'], pDate);
                                    tryAll(['#fechaRecogidaSelHour','input[name=fechaRecogidaSelHour]','input[name=pickupTime]','input[type=time][name*=recog]','input[type=time][name*=pickup]','#h-recogida'], pTime);
                                    tryAll(['#fechaEntrega','#fechaDevolucion','input[name=fechaEntrega]','input[name=fechaDevolucion]','input[name=dropoffDate]','input[type=date][name*=entreg]','input[type=date][name*=devol]','input[placeholder*="entreg" i]','input[aria-label*="entreg" i]'], dDate);
                                    tryAll(['#fechaEntregaSelHour','#fechaDevolucionSelHour','input[name=fechaEntregaSelHour]','input[name=fechaDevolucionSelHour]','input[name=dropoffTime]','input[type=time][name*=entreg]','input[type=time][name*=devol]','input[type=time][name*=drop]','#h-devolucion'], dTime);
                                  }
                                """
                                await page2.evaluate(fill_js2, pickup_dmY2, pickup_HM2, dropoff_dmY2, dropoff_HM2)
                                await page2.wait_for_timeout(300)
                            except Exception:
                                pass
                            # Submit programmatically and via native function
                            try:
                                payload2 = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                                submit_js2 = """
                                  (url, data) => { const f = document.createElement('form'); f.method='POST'; f.action=url; for (const [k,v] of Object.entries(data||{})) { const i=document.createElement('input'); i.type='hidden'; i.name=k; i.value=String(v??''); f.appendChild(i);} document.body.appendChild(f); f.submit(); }
                                """
                                await page2.evaluate(submit_js2, f"https://www.carjet.com/do/list/{lang}", payload2)
                                await page2.wait_for_load_state('networkidle', timeout=40000)
                                try:
                                    await page2.evaluate("""
                                      try { if (typeof comprobar_errores_3==='function' && comprobar_errores_3()) { if (typeof filtroUsoForm==='function') filtroUsoForm(); if (typeof submit_fechas==='function') submit_fechas('/do/list/pt'); } } catch(e) {}
                                    """)
                                    await page2.wait_for_load_state('networkidle', timeout=40000)
                                except Exception:
                                    pass
                            except Exception:
                                pass
                            # Wait for known responses
                            mf2 = ""; cl2 = ""
                            try:
                                r1 = await page2.wait_for_response(lambda r: 'modalFilter.asp' in (r.url or ''), timeout=40000)
                                try: mf2 = await r1.text()
                                except Exception: mf2 = ""
                            except Exception:
                                pass
                            try:
                                r2 = await page2.wait_for_response(lambda r: 'carList.asp' in (r.url or ''), timeout=40000)
                                try: cl2 = await r2.text()
                                except Exception: cl2 = ""
                            except Exception:
                                pass
                            html2 = await page2.content()
                            final2 = page2.url
                            await context2.close(); await browser2.close()
                        # parse order: DOM, carList, modalFilter, captured list
                        if not items:
                            try:
                                if html2:
                                    its = parse_prices(html2, final2 or base)
                                    its = convert_items_gbp_to_eur(its); its = apply_price_adjustments(its, final2 or base)
                                    if its: items = its
                            except Exception:
                                pass
                        if (not items) and cl2:
                            try:
                                base_net2 = "https://www.carjet.com/do/list/pt"
                                its = parse_prices(cl2, base_net2); its = convert_items_gbp_to_eur(its); its = apply_price_adjustments(its, base_net2)
                                if its: items = its
                            except Exception:
                                pass
                        if (not items) and mf2:
                            try:
                                base_net2 = "https://www.carjet.com/do/list/pt"
                                its = parse_prices(mf2, base_net2); its = convert_items_gbp_to_eur(its); its = apply_price_adjustments(its, base_net2)
                                if its: items = its
                            except Exception:
                                pass
                        if (not items) and captured2:
                            try:
                                base_net2 = "https://www.carjet.com/do/list/pt"
                                for body in captured2:
                                    its = parse_prices(body, base_net2); its = convert_items_gbp_to_eur(its); its = apply_price_adjustments(its, base_net2)
                                    if its:
                                        items = its; break
                            except Exception:
                                pass
                        # Final fallback for Chromium attempt: try URL-based compute if we have a CarJet list URL
                        try:
                            if (not items) and (final2 or '').startswith("https://www.carjet.com/do/list/"):
                                data_f2 = await _compute_prices_for(final2)
                                its_f2 = (data_f2 or {}).get('items') or []
                                if its_f2:
                                    items = its_f2
                        except Exception:
                            pass
                        # Direct POST fallback within Playwright session (Chromium)
                        try:
                            if not items:
                                payload_dp2 = build_carjet_form(location, start_dt, end_dt, lang=lang, currency=currency)
                                rdp2 = await page2.request.post(f"https://www.carjet.com/do/list/{lang}", data=payload_dp2)
                                try:
                                    html_dp2 = await rdp2.text()
                                except Exception:
                                    html_dp2 = ""
                                if html_dp2:
                                    try:
                                        stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')
                                        (DEBUG_DIR / f"pw2-direct-post-{stamp}.html").write_text(html_dp2, encoding='utf-8')
                                    except Exception:
                                        pass
                                    its_dp2 = parse_prices(html_dp2, f"https://www.carjet.com/do/list/{lang}")
                                    its_dp2 = convert_items_gbp_to_eur(its_dp2)
                                    its_dp2 = apply_price_adjustments(its_dp2, f"https://www.carjet.com/do/list/{lang}")
                                    if its_dp2:
                                        items = its_dp2
                        except Exception:
                            pass
                    except Exception:
                        pass
            except Exception:
                # Playwright falhou silenciosamente, tentar fallback
                items = []
        html = ""
        if not items:
            html = try_direct_carjet(location, start_dt, end_dt, lang=lang, currency=currency)
        # DEBUG: persist fetched HTML for troubleshooting
        try:
            _stamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
            _loc_tag = re.sub(r"[^a-z0-9]+", "-", (location or "").lower())
            if html:
                (_fp := (DEBUG_DIR / f"track_params-{_loc_tag}-{start_dt.date()}-{days}d-{_stamp}.html")).write_text(html or "", encoding="utf-8")
        except Exception:
            pass
        if not items:
            if not html:
                return _no_store_json({"ok": False, "error": "Upstream fetch failed"}, status_code=502)
            items = parse_prices(html, base)
            items = convert_items_gbp_to_eur(items)
            items = apply_price_adjustments(items, base)
        # DEBUG: write a compact summary JSON (count and first 5 items)
        try:
            import json as _json
            _sum = {
                "ts": _stamp,
                "location": location,
                "start": start_dt.isoformat(),
                "end": end_dt.isoformat(),
                "days": days,
                "count": len(items or []),
                "preview": (items or [])[:5],
            }
            (DEBUG_DIR / f"track_params-summary-{_loc_tag}-{_stamp}.json").write_text(_json.dumps(_sum, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception:
            pass
        # No additional fallback needed; Playwright was already attempted first when enabled
        print(f"\n[API] ‚úÖ RESPONSE: {len(items)} items for {days} days")
        if items:
            print(f"[API] First car: {items[0].get('car', 'N/A')} - {items[0].get('price', 'N/A')}")
        else:
            print(f"[API] ‚ö†Ô∏è  NO ITEMS RETURNED!")
        print(f"{'='*60}\n")
        print(f"[API] RESPONSE: {len(items)} items, days={days}, start={start_dt.date()}, end={end_dt.date()}", file=sys.stderr, flush=True)
        if items:
            print(f"[API] First car: {items[0].get('car', 'N/A')} - {items[0].get('price', 'N/A')}", file=sys.stderr, flush=True)
        # APLICAR NORMALIZE_AND_SORT para adicionar campo 'group'
        items = normalize_and_sort(items, supplier_priority=None)
        return _no_store_json({
            "ok": True,
            "items": items,
            "location": location,
            "start_date": start_dt.date().isoformat(),
            "start_time": start_dt.strftime("%H:%M"),
            "end_date": end_dt.date().isoformat(),
            "end_time": end_dt.strftime("%H:%M"),
            "days": days,
        })
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"[API ERROR] track-by-params failed: {str(e)}")
        print(f"{'='*60}\n")
        import traceback
        traceback.print_exc()
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, status_code=500)

@app.get("/debug/vars")
async def debug_vars():
    return JSONResponse({
        "USE_PLAYWRIGHT": USE_PLAYWRIGHT,
        "_HAS_PLAYWRIGHT": _HAS_PLAYWRIGHT,
        "SCRAPER_SERVICE": SCRAPER_SERVICE,
    })

@app.get("/ph")
async def placeholder_image(car: str = "Car"):
    try:
        label = (car or "Car").strip()
        if len(label) > 32:
            label = label[:32] + "‚Ä¶"
        # Teal background (#009cb6) to match site, white centered text
        svg = f'''<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" width="320" height="180" viewBox="0 0 320 180" role="img">
  <rect width="320" height="180" fill="#009cb6"/>
  <text x="160" y="90" fill="#ffffff" font-family="-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,sans-serif" font-size="18" text-anchor="middle" dominant-baseline="middle">{label}</text>
  <text x="160" y="160" fill="rgba(255,255,255,0.7)" font-family="-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,sans-serif" font-size="12" text-anchor="middle">Image unavailable</text>
</svg>'''
        resp = Response(content=svg, media_type="image/svg+xml; charset=utf-8")
        resp.headers["Cache-Control"] = "public, max-age=86400"
        return resp
    except Exception:
        return Response(status_code=500)

def _normalize_model_for_image(name: str) -> str:
    s = (name or "").lower()
    s = re.sub(r"\b(auto|automatic|manual|station\s*wagon|estate|sw|variant|break|tourer|grandtour|grand\s*tour|kombi|sportbreak|sport\s*brake|st)\b", "", s)
    s = re.sub(r"[^a-z0-9]+", " ", s).strip()
    # common brand/model reorderings are left as-is
    return " ".join(s.split())

def _build_commons_query(name: str) -> str:
    key = _normalize_model_for_image(name)
    # bias towards car photos
    return f"{key} car"

def _save_cache_image(key: str, content: bytes, ext: str) -> Path:
    p = CACHE_CARS_DIR / f"{key}{ext}"
    with open(p, "wb") as f:
        f.write(content)
    return p

def _find_cached_image(key: str) -> Optional[Path]:
    for ext in (".jpg", ".jpeg", ".png", ".webp"):
        p = CACHE_CARS_DIR / f"{key}{ext}"
        if p.exists() and p.stat().st_size > 0:
            return p
    return None

@app.get("/imglookup")
async def img_lookup(car: str):
    try:
        car = car or "Car"
        key = _normalize_model_for_image(car).replace(" ", "-")
        cached = _find_cached_image(key)
        if cached:
            ct = "image/jpeg"
            if cached.suffix == ".png": ct = "image/png"
            elif cached.suffix == ".webp": ct = "image/webp"
            with open(cached, "rb") as f:
                b = f.read()
            resp = Response(content=b, media_type=ct)
            resp.headers["Cache-Control"] = "public, max-age=86400"
            return resp

        # Wikimedia Commons API search for files
        import json as _json
        api = "https://commons.wikimedia.org/w/api.php"
        params = {
            "action": "query",
            "format": "json",
            "prop": "imageinfo",
            "generator": "search",
            "gsrsearch": _build_commons_query(car),
            "gsrlimit": "5",
            "gsrnamespace": "6",  # File namespace
            "iiprop": "url|mime",
            "iiurlwidth": "480",
            "origin": "*",
        }
        r = requests.get(api, params=params, timeout=10, headers={"User-Agent": "PriceTracker/1.0"})
        url = None
        mime = None
        if r.ok:
            data = r.json()
            pages = (data.get("query", {}) or {}).get("pages", {})
            for _, pg in pages.items():
                ii = (pg.get("imageinfo") or [{}])[0]
                url = ii.get("thumburl") or ii.get("url")
                mime = ii.get("mime") or "image/jpeg"
                if url:
                    break
        if url:
            ir = requests.get(url, timeout=10, headers={"User-Agent": "PriceTracker/1.0"})
            if ir.ok and ir.content:
                ext = ".jpg"
                if (mime or "").endswith("png"): ext = ".png"
                elif (mime or "").endswith("webp"): ext = ".webp"
                path = _save_cache_image(key, ir.content, ext)
                resp = Response(content=ir.content, media_type=mime or "image/jpeg")
                resp.headers["Cache-Control"] = "public, max-age=86400"
                return resp
        # Fallback to placeholder
        return await placeholder_image(car)
    except Exception:
        return await placeholder_image(car)

@app.get("/api/debug/check-vehicle")
async def debug_check_vehicle(car_name: str):
    """Verifica se um carro est√° no dicion√°rio VEHICLES"""
    car_clean = clean_car_name(car_name)
    in_vehicles = car_clean in VEHICLES
    
    result = {
        "original": car_name,
        "cleaned": car_clean,
        "in_vehicles": in_vehicles
    }
    
    if in_vehicles:
        vehicle_info = VEHICLES[car_clean]
        result["vehicle_info"] = vehicle_info
        if isinstance(vehicle_info, dict) and 'group' in vehicle_info:
            result["group"] = vehicle_info['group']
    
    return JSONResponse(result)

@app.get("/api/debug_direct")
async def debug_direct(request: Request):
    params = request.query_params
    location = params.get("location", "Albufeira")
    pickup_date = params.get("date")
    pickup_time = params.get("time", "10:00")
    days = int(params.get("days", 1))
    lang = params.get("lang", "pt")
    currency = params.get("currency", "EUR")
    if not pickup_date:
        return JSONResponse({"ok": False, "error": "Missing date (YYYY-MM-DD)"}, status_code=400)

    try:
        from datetime import datetime, timedelta
        start_dt = datetime.fromisoformat(pickup_date + "T" + pickup_time)
        end_dt = start_dt + timedelta(days=days)
        html = try_direct_carjet(location, start_dt, end_dt, lang=lang, currency=currency)
        if not html:
            return JSONResponse({"ok": False, "error": "Empty HTML from direct POST"}, status_code=500)

        # Save to debug file
        from datetime import datetime as _dt
        stamp = _dt.utcnow().strftime("%Y%m%dT%H%M%S")
        filename = f"debug-direct-{location.replace(' ', '-')}-{pickup_date}-{days}d.html"
        out_path = DEBUG_DIR / filename
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(html)

        # Quick selector counts
        soup = BeautifulSoup(html, "lxml")
        # Price-like selector counts and inline dataMap presence
        counts = {
            ".price": len(soup.select(".price")),
            ".amount": len(soup.select(".amount")),
            "[class*='price']": len(soup.select("[class*='price']")),
            "a[href]": len(soup.select("a[href]")),
        }
        try:
            import json as _json
            m = re.search(r"var\s+dataMap\s*=\s*(\[.*?\]);", html, re.S)
            if m:
                arr = _json.loads(m.group(1))
                counts["has_dataMap"] = True
                counts["dataMap_len"] = len(arr)
            else:
                counts["has_dataMap"] = False
                counts["dataMap_len"] = 0
        except Exception:
            counts["has_dataMap"] = False
            counts["dataMap_len"] = 0
        return JSONResponse({
            "ok": True,
            "url": f"https://www.carjet.com/do/list/{lang} (direct)",
            "debug_file": f"/static/debug/{filename}",
            "counts": counts,
        })
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


def parse_prices(html: str, base_url: str) -> List[Dict[str, Any]]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict[str, Any]] = []
    # Flattened page text to infer context-specific categories (e.g., automatic families)
    try:
        _page_text = soup.get_text(" ", strip=True).lower()
    except Exception:
        _page_text = ""

    # Helper: detect automatic transmission markers from name or card text or explicit label
    def _is_auto_flag(name_lc: str, card_text_lc: str, trans_label: str) -> bool:
        try:
            if (trans_label or '').lower() == 'automatic':
                return True
            return bool(AUTO_RX.search(name_lc or '') or AUTO_RX.search(card_text_lc or ''))
        except Exception:
            return False

    # Blocklist of car models to exclude
    _blocked_models = [
        "Mercedes S Class Auto",
        "MG ZS Auto",
        "Mercedes CLA Coupe Auto",
        "Mercedes A Class",
        "Mercedes A Class Auto",
        "BMW 1 Series Auto",
        "BMW 3 Series SW Auto",
        "Volvo V60 Auto",
        "Volvo XC40 Auto",
        "Mercedes C Class Auto",
        "Tesla Model 3 Auto",
        "Electric",
        "BMW 2 Series Gran Coupe Auto",
        "Mercedes C Class SW Auto",
        "Mercedes E Class Auto",
        "Mercedes E Class SW Auto",
        "BMW 5 Series SW Auto",
        "BMW X1 Auto",
        "Mercedes CLE Coupe Auto",
        "Volkswagen T-Roc Cabrio",
        "Mercedes GLA Auto",
        "Volvo XC60 Auto",
        "Volvo EX30 Auto",
        "BMW 3 Series Auto",
        "Volvo V60 4x4 Auto",
        "Hybrid",
        "Mazda MX5 Cabrio Auto",
        "Mercedes CLA Auto",
    ]

    def _norm_text(s: str) -> str:
        s = (s or "").strip().lower()
        # remove duplicate spaces and commas spacing
        s = " ".join(s.replace(",", " ").split())
        return s

    _blocked_norm = set(_norm_text(x) for x in _blocked_models)

    def _is_blocked_model(name: str) -> bool:
        n = _norm_text(name)
        if not n:
            return False
        if n in _blocked_norm:
            return True
        # Regex-based strong match on key model families and powertrains
        patterns = [
            r"\bmercedes\s+s\s*class\b",
            r"\bmercedes\s+cla\b",
            r"\bmercedes\s+cle\b",
            r"\bmercedes\s+a\s*class\b",
            r"\bmercedes\s+c\s*class\b",
            r"\bmercedes\s+e\s*class\b",
            r"\bmercedes\s+gla\b",
            r"\bbmw\s+1\s*series\b",
            r"\bbmw\s+2\s*series\b",
            r"\bbmw\s+3\s*series\b",
            r"\bbmw\s+5\s*series\b",
            r"\bbmw\s*x1\b",
            r"\bvolvo\s+v60\b",
            r"\bvolvo\s+xc40\b",
            r"\bvolvo\s+xc60\b",
            r"\bvolvo\s+ex30\b",
            r"\btesla\s+model\s*3\b",
            r"\bmg\s+zs\b",
            r"\bmazda\s+mx5\b",
            r"\bvolkswagen\s+t-roc\b",
            r"\belectric\b",
            r"\bhybrid\b",
        ]
        import re as _re
        for p in patterns:
            if _re.search(p, n):
                return True
        # also check if any blocked long phrase is contained in name
        for b in _blocked_norm:
            if len(b) >= 6 and b in n:
                return True
        return False

    # --- Photo cache helpers (SQLite) ---
    def _photo_db_path() -> str:
        try:
            from pathlib import Path
            return str((Path(__file__).resolve().parent / "car_images.db"))
        except Exception:
            return "car_images.db"

    def _get_conn():
        try:
            import sqlite3
            return sqlite3.connect(_photo_db_path())
        except Exception:
            return None

    def _init_photos_table():
        conn = _get_conn()
        if not conn:
            return
        try:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS car_images (
                    model_key TEXT PRIMARY KEY,
                    photo_url TEXT,
                    updated_at TEXT
                )
                """
            )
            conn.commit()
        except Exception:
            pass
        finally:
            try:
                conn.close()
            except Exception:
                pass
            # FINAL SAFEGUARD: If Clio wagon fell into D/E2, flip to J2/L2
            try:
                _nm = (car_name or '').lower()
                _txt_final = ''
                try:
                    _txt_final = card.get_text(' ', strip=True).lower()
                except Exception:
                    _txt_final = ''
                if re.search(r"\brenault\s*clio\b", _nm) and re.search(r"\b(sw|st|sport\s*tourer|tourer|break|estate|kombi|grandtour|grand\s*tour|sporter|wagon)\b", _nm):
                    if _is_auto_flag(_nm, _txt_final, transmission_label):
                        category = 'Station Wagon Automatic'
                    else:
                        category = 'Estate/Station Wagon'
            except Exception:
                pass

    def _normalize_model_key(name: str) -> str:
        s = (name or "").strip().lower()
        for w in ("suv", "economy", "mini", "estate", "station wagon", "premium", "7 seater", "9 seater"):
            if s.endswith(" " + w):
                s = s[: -len(w) - 1].strip()
        s = " ".join(s.split())
        return s

    def _cache_get_photo(key: str) -> str:
        conn = _get_conn()
        if not conn:
            return ""
        try:
            cur = conn.execute("SELECT photo_url FROM car_images WHERE model_key = ?", (key,))
            row = cur.fetchone()
            return row[0] if row and row[0] else ""
        except Exception:
            return ""
        finally:
            try:
                conn.close()
            except Exception:
                pass

    def _cache_set_photo(key: str, url: str):
        if not (key and url):
            return
        _init_photos_table()
        conn = _get_conn()
        if not conn:
            return
        try:
            from datetime import datetime as _dt
            conn.execute(
                "INSERT INTO car_images (model_key, photo_url, updated_at) VALUES (?, ?, ?) "
                "ON CONFLICT(model_key) DO UPDATE SET photo_url=excluded.photo_url, updated_at=excluded.updated_at",
                (key, url, _dt.utcnow().isoformat(timespec="seconds"))
            )
            conn.commit()
        except Exception:
            pass
        finally:
            try:
                conn.close()
            except Exception:
                pass

    def map_grupo(grupo: str) -> str:
        if not grupo:
            return ""
        g = str(grupo).upper()
        # N-codes examples
        if g.startswith("N"):
            if g == "N07":
                return "7 Seater"
            if g.startswith("N09") or g == "N9" or g == "N90" or g == "N099":
                return "9 Seater"
            return "People Carrier"
        # S-codes: estate/station wagon
        if g.startswith("S"):
            return "Estate/Station Wagon"
        # A-codes: automatic variants; infer base from page context
        if g.startswith("A"):
            txt = _page_text
            if any(k in txt for k in ("estate", "station wagon", "estatecars", "carrinha")):
                return "Estate/Station Wagon"
            if "suv" in txt:
                return "SUV"
            if any(k in txt for k in ("7 lugares", "7 seats", "7 seater")):
                return "7 Seater"
            if any(k in txt for k in ("9 lugares", "9 seats", "9 seater")):
                return "9 Seater"
            if any(k in txt for k in ("mini", "pequeno")):
                return "Mini"
            if any(k in txt for k in ("economy", "econ√≥mico", "economico")):
                return "Economy"
            # Fallback: treat as Economy automatic if no context hints
            return "Economy"
        # F/M codes frequently used for SUVs in provided samples
        if g.startswith("F"):
            return "SUV"
        if g.startswith("M"):
            # People carriers: infer 7 vs 9 seater from page text
            txt = _page_text
            if any(k in txt for k in ("9 lugares", "9 seats", "9 seater")):
                return "9 Seater"
            return "7 Seater"
        # Premium families observed as J/L in samples
        if g.startswith("J"):
            return "Premium"
        if g.startswith("L"):
            return "Premium"
        # C-codes numeric mapping
        if g.startswith("C"):
            try:
                n = int(g[1:])
            except Exception:
                return g
            if 1 <= n <= 4:
                return "Mini"
            if 5 <= n <= 9:
                return "Economy"
            if 10 <= n <= 19:
                return "Compact"
            if 20 <= n <= 29:
                return "Intermediate"
            if 30 <= n <= 39:
                return "Standard"
            if 40 <= n <= 49:
                return "Full-size"
            if 60 <= n <= 69:
                return "SUV"
            return g
        return g

    # Transmission label from global radio (if present)
    transmission_label = ""
    try:
        t_inp = soup.select_one("input[name='frmTrans'][checked]")
        if t_inp and t_inp.has_attr("value"):
            v = (t_inp.get("value") or "").lower()
            if v == "au":
                transmission_label = "Automatic"
            elif v == "mn":
                transmission_label = "Manual"
            elif v == "el":
                transmission_label = "Electric"
    except Exception:
        pass
    # Fallback: infer from 'Filtros utilizados anteriormente' section
    if not transmission_label:
        try:
            used = soup.select_one("#filterUsed")
            if used:
                txt = used.get_text(" ", strip=True).lower()
                if "autom" in txt:
                    transmission_label = "Automatic"
                elif "manual" in txt:
                    transmission_label = "Manual"
                elif "electr" in txt:
                    transmission_label = "Electric"
        except Exception:
            pass

    # Fast path for CarJet: collect provider summaries but do not return early; we'll prefer detailed items
    summary_items: List[Dict[str, Any]] = []
    try:
        # 0) Generic object matcher as a fallback to capture provider blobs even if array/var name changes
        raw_objs = OBJ_RX.findall(html)
        if raw_objs:
            import json as _json
            supplier_alias = {
                "AUP": "Auto Prudente Rent a Car",
                "SXT": "Sixt",
                "ECR": "Europcar",
                "KED": "Keddy by Europcar",
                "EPI": "EPI",
                "ALM": "Alamo",
                "AVX": "Avis",
                "BGX": "Budget",
                "ENT": "Enterprise",
                "DTG": "Dollar",
                "FLZ": "Flizzr",
                "DTG1": "Rentacar",
                "DGT1": "Rentacar",
                "EU2": "Goldcar Non-Refundable",
                "EUR": "Goldcar",
                "EUK": "Goldcar Key'n Go",
                "GMO": "Green Motion",
                "GMO1": "Green Motion",
                "SAD": "Drivalia",
                "DOH": "Drive on Holidays",
                "D4F": "Drive4Fun",
                "DVM": "Drive4Move",
                "CAE": "Cael",
                "CEN": "Centauro",
                "ABB": "Abbycar",
                "ABB1": "Abbycar Non-Refundable",
                "BSD": "Best Deal",
                "ATR": "Autorent",
                "AUU": "Auto Union",
                "THR": "Thrifty",
                "HER": "Hertz",
                "LOC": "Million",
            }
            idx = 0
            for s in raw_objs:
                try:
                    d = _json.loads(s)
                except Exception:
                    continue
                price_text = d.get("priceStr") or ""
                if not price_text:
                    continue
                supplier_code = (d.get("id") or "").strip()
                supplier = supplier_alias.get(supplier_code, supplier_code)
                grupo = d.get("grupoVeh") or ""
                category_h = map_grupo(grupo)
                display_category = category_h or grupo
                if transmission_label == "Automatic":
                    if display_category in ("Mini", "Economy", "SUV", "Estate/Station Wagon", "7 Seater"):
                        if display_category == "Estate/Station Wagon":
                            display_category = "Station Wagon Automatic"
                        elif display_category == "7 Seater":
                            display_category = "7 Seater Automatic"
                        else:
                            display_category = f"{display_category} Automatic"
                # Best-effort photo from grupoVeh code
                photo_url = ""
                try:
                    if grupo:
                        photo_url = urljoin(base_url, f"/cdn/img/cars/S/car_{grupo}.jpg")
                except Exception:
                    photo_url = ""
                # Mapear categoria para c√≥digo de grupo
                group_code = map_category_to_group(display_category, "")
                summary_items.append({
                    "id": idx,
                    "car": "",
                    "supplier": supplier,
                    "price": price_text,
                    "currency": "",
                    "category": display_category,
                    "group": group_code,
                    "category_code": grupo,
                    "transmission": transmission_label,
                    "photo": photo_url,
                    "link": base_url,
                })
                idx += 1
            # do not return yet; prefer detailed rows

        m = DATAMAP_RX.search(html)
        if m:
            import json
            arr = json.loads(m.group(1))
            supplier_alias = {
                "AUP": "Auto Prudente Rent a Car",
                "SXT": "Sixt",
                "ECR": "Europcar",
                "KED": "Keddy by Europcar",
                "EPI": "EPI",
                "ALM": "Alamo",
                "AVX": "Avis",
                "BGX": "Budget",
                "ENT": "Enterprise",
                "DTG": "Dollar",
                "FLZ": "Flizzr",
                "EU2": "Goldcar Non-Refundable",
                "EUR": "Goldcar",
                "EUK": "Goldcar Key'n Go",
                "GMO": "Green Motion",
                "GMO1": "Green Motion",
                "SAD": "Drivalia",
                "DOH": "Drive on Holidays",
                "D4F": "Drive4Fun",
                "DVM": "Drive4Move",
                "CAE": "Cael",
                "CEN": "Centauro",
                "ABB": "Abbycar",
                "ABB1": "Abbycar Non-Refundable",
                "BSD": "Best Deal",
                "ATR": "Autorent",
                "AUU": "Auto Union",
            }
            idx = 0
            for it in arr:
                supplier_code = (it.get("id") or "").strip()
                # Additional inline overrides
                if supplier_code in ("DTG1", "DGT1"):
                    supplier = "Rentacar"
                else:
                    supplier = supplier_alias.get(supplier_code, supplier_code)
                price_text = it.get("priceStr") or ""
                if not price_text:
                    continue
                grupo = it.get("grupoVeh") or ""
                category_h = map_grupo(grupo)
                display_category = category_h or grupo
                if transmission_label == "Automatic":
                    if display_category in ("Mini", "Economy", "SUV", "Estate/Station Wagon", "7 Seater"):
                        if display_category == "Estate/Station Wagon":
                            display_category = "Station Wagon Automatic"
                        elif display_category == "7 Seater":
                            display_category = "7 Seater Automatic"
                        else:
                            display_category = f"{display_category} Automatic"
                # Mapear categoria para c√≥digo de grupo
                group_code = map_category_to_group(display_category, "")
                summary_items.append({
                    "id": idx,
                    "car": "",
                    "supplier": supplier,
                    "price": price_text,
                    "currency": "",
                    "category": display_category,
                    "group": group_code,
                    "category_code": grupo,
                    "transmission": transmission_label,
                    "link": base_url,
                })
                idx += 1
            # do not return yet; prefer detailed rows
    except Exception:
        pass

    # Pass 2: try to parse explicit car cards/rows from the HTML (preferred over regex)
    try:
        cards = soup.select("section.newcarlist article, .newcarlist article, article.car, li.result, li.car, .car-item, .result-row")
        print(f"[PARSE] Found {len(cards)} cards to parse")
        idx = 0
        cards_with_price = 0
        cards_with_name = 0
        cards_blocked = 0
        for card in cards:
            # price - PRIORIZAR .price.pr-euros (pre√ßo total em euros, N√ÉO libras nem por dia)
            price_text = ""
            
            # 1¬™ PRIORIDADE: Buscar .price.pr-euros (pre√ßo total em euros)
            # Excluir .price-day-euros e .old-price
            for span_tag in card.find_all('span'):
                classes = span_tag.get('class', [])
                if not classes:
                    continue
                
                # Verificar se tem 'price' E 'pr-euros' MAS N√ÉO tem 'day' nem 'old'
                has_price = 'price' in classes
                has_pr_euros = 'pr-euros' in classes
                has_day = any('day' in c for c in classes)
                has_old = any('old' in c for c in classes)
                
                if has_price and has_pr_euros and not has_day and not has_old:
                    price_text = span_tag.get_text(strip=True)
                    # Limpar desconto: "-25%17,05 ‚Ç¨12,79 ‚Ç¨" -> "12,79 ‚Ç¨"
                    # Pegar apenas o √∫ltimo pre√ßo (ap√≥s o √∫ltimo espa√ßo antes de ‚Ç¨)
                    if price_text.count('‚Ç¨') > 1:
                        # Tem m√∫ltiplos pre√ßos, pegar o √∫ltimo
                        parts = price_text.split('‚Ç¨')
                        # O √∫ltimo √© vazio, o pen√∫ltimo tem o pre√ßo final
                        if len(parts) >= 2:
                            price_text = parts[-2].split()[-1] + ' ‚Ç¨'
                    break  # Encontrou o pre√ßo correto em euros!
            
            # 2¬™ PRIORIDADE: Se n√£o encontrou .pr-euros, usar seletor gen√©rico (fallback)
            if not price_text:
                let_price = card.select_one(".price, .amount, [class*='price'], .nfoPriceDest, .nfoPrice, [data-price]")
                price_text = (let_price.get_text(strip=True) if let_price else "") or (card.get("data-price") or "")
            
            if not price_text:
                continue
            cards_with_price += 1
            # car/model
            name_el = card.select_one(
                ".veh-name, .vehicle-name, .model, .titleCar, .title, h3, h2, [class*='veh-name'], [class*='vehicle-name'], [class*='model']"
            )
            car_name = name_el.get_text(strip=True) if name_el else ""
            if not car_name:
                # try common data attributes
                for attr in ("data-model", "data-vehicle", "data-name", "aria-label", "title"):
                    v = (card.get(attr) or "").strip()
                    if v:
                        car_name = v
                        break
            if car_name:
                cards_with_name += 1
            # supplier: try to extract provider code from logo_XXX.* in img src, then map via alias
            supplier = ""
            try:
                supplier_alias = {
                    "AUP": "Auto Prudente Rent a Car",
                    "SXT": "Sixt",
                    "ECR": "Europcar",
                    "KED": "Keddy by Europcar",
                    "EPI": "EPI",
                    "ALM": "Alamo",
                    "AVX": "Avis",
                    "BGX": "Budget",
                    "ENT": "Enterprise",
                    "DTG": "Dollar",
                    "DTG1": "Rentacar",
                    "DGT1": "Rentacar",
                    "FLZ": "Flizzr",
                    "EU2": "Goldcar Non-Refundable",
                    "EUR": "Goldcar",
                    "EUK": "Goldcar Key'n Go",
                    "GMO": "Green Motion",
                    "GMO1": "Green Motion",
                    "SAD": "Drivalia",
                    "DOH": "Drive on Holidays",
                    "D4F": "Drive4Fun",
                    "DVM": "Drive4Move",
                    "CAE": "Cael",
                    "CEN": "Centauro",
                    "ABB": "Abbycar",
                    "ABB1": "Abbycar Non-Refundable",
                    "BSD": "Best Deal",
                    "ATR": "Autorent",
                    "AUU": "Auto Union",
                    "THR": "Thrifty",
                    "HER": "Hertz",
                    "LOC": "Million",
                }
                code = ""
                for im in card.select("img[src]"):
                    src = im.get("src") or ""
                    mcode = LOGO_CODE_RX.search(src)
                    if mcode:
                        code = (mcode.group(1) or "").upper()
                        break
                
                if code:
                    supplier = supplier_alias.get(code, code)
                if not supplier:
                    # textual fallback but avoid using car name
                    supplier_el = card.select_one(".supplier, .vendor, .partner, [class*='supplier'], [class*='vendor']")
                    txt = supplier_el.get_text(strip=True) if supplier_el else ""
                    if txt and txt.lower() != (car_name or "").lower():
                        supplier = txt
            except Exception:
                pass
            # photo: pick an image that is not a provider logo
            photo = ""
            try:
                # PRIORIDADE 1: img.cl--car-img (CarJet espec√≠fico)
                car_img = card.select_one("img.cl--car-img")
                if car_img:
                    src = (car_img.get("src") or car_img.get("data-src") or car_img.get("data-original") or "").strip()
                    if src:
                        try:
                            from urllib.parse import urljoin
                            photo = urljoin(base_url, src)
                        except Exception:
                            photo = src
                        # SEMPRE extrair nome do alt (√© mais preciso que os outros m√©todos)
                        alt_text = (car_img.get("alt") or "").strip()
                        if alt_text:
                            # "Toyota Aygo ou similar | Pequeno" -> "Toyota Aygo"
                            # "Skoda Scala ou similar " -> "Skoda Scala"
                            alt_car_name = alt_text.split('ou similar')[0].split('|')[0].strip()
                            if alt_car_name:
                                car_name = alt_car_name
                                print(f"[SCRAPING] Nome extra√≠do do alt da imagem: {car_name} (foto: {src})")
                
                # PRIORIDADE 2: prefer <picture> sources
                if not photo:
                    picture_src = None
                    for src_el in card.select("picture source[srcset], img[srcset], picture source[data-srcset], img[data-srcset]"):
                        sset = (src_el.get("srcset") or src_el.get("data-srcset") or "").strip()
                        if sset:
                            # pick the first candidate (split by comma for multiple entries, then URL before whitespace)
                            first_entry = sset.split(',')[0].strip()
                            picture_src = first_entry.split()[0]
                            if picture_src:
                                break
                    
                    # PRIORIDADE 3: Outras imagens
                    imgs = card.select("img")
                    for im in imgs:
                        src = picture_src or (
                            im.get("src") or im.get("data-src") or im.get("data-original") or im.get("data-lazy") or im.get("data-lazy-src") or ""
                        ).strip()
                        if not src:
                            continue
                        # skip logos and icons
                        if re.search(r"logo_", src, re.I):
                            continue
                        if src.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):
                            # make absolute if needed
                            try:
                                from urllib.parse import urljoin
                                photo = urljoin(base_url, src)
                            except Exception:
                                photo = src
                            # use alt/title as car_name fallback
                            if not car_name:
                                alt_t = (im.get("alt") or im.get("title") or "").strip()
                                if alt_t:
                                    car_name = alt_t
                            break
                # Also check inline background-image on card and descendants
                if not photo:
                    style_el = card.get("style") or ""
                    m_bg = BG_IMAGE_RX.search(style_el)
                    if m_bg:
                        raw = m_bg.group(1).strip().strip('\"\'')
                        try:
                            from urllib.parse import urljoin
                            photo = urljoin(base_url, f"/img?src={raw}")
                        except Exception:
                            photo = f"/img?src={raw}"
                if not photo:
                    for child in card.find_all(True):
                        st = child.get("style") or ""
                        m2 = BG_IMAGE_RX.search(st)
                        if m2:
                            raw = m2.group(1).strip().strip('\"\'')
                            try:
                                from urllib.parse import urljoin
                                photo = urljoin(base_url, f"/img?src={raw}")
                            except Exception:
                                photo = f"/img?src={raw}"
                            if photo:
                                break
                # As a final fallback, synthesize from any car_[gV].jpg reference inside this card
                if not photo:
                    html_block = str(card)
                    m_car = CAR_CODE_RX.search(html_block)
                    if m_car:
                        code = m_car.group(1)
                        try:
                            from urllib.parse import urljoin
                            photo = urljoin(base_url, f"/cdn/img/cars/S/car_{code}.jpg")
                        except Exception:
                            photo = f"/cdn/img/cars/S/car_{code}.jpg"
            except Exception:
                pass
            # category
            cat_el = card.select_one(".category, .group, .vehicle-category, [class*='category'], [class*='group'], [class*='categoria'], [class*='grupo']")
            category = cat_el.get_text(strip=True) if cat_el else ""
            # Canonicalize category to expected groups
            def _canon(cat: str) -> str:
                c = (cat or "").strip().lower()
                if not c:
                    return ""
                if "estate" in c or "station" in c or "carrinha" in c:
                    return "Estate/Station Wagon"
                if "suv" in c:
                    return "SUV"
                if "premium" in c or "lux" in c:
                    return "Premium"
                if "7" in c and ("lugar" in c or "lugares" in c or "seater" in c or "seats" in c):
                    return "7 Seater"
                if "9" in c and ("lugar" in c or "lugares" in c or "seater" in c or "seats" in c):
                    return "9 Seater"
                if "econom" in c:
                    return "Economy"
                if "mini" in c or "small" in c or "pequeno" in c:
                    return "Mini"
                return cat
            category = _canon(category)
            if not category:
                # Infer from CARD context if label missing to avoid page-wide bias
                try:
                    local_txt = card.get_text(" ", strip=True).lower()
                except Exception:
                    local_txt = ""
                if any(k in local_txt for k in ("estate", "station wagon", "estatecars", "carrinha")):
                    category = "Estate/Station Wagon"
                elif "suv" in local_txt:
                    category = "SUV"
                elif any(k in local_txt for k in ("7 lugares", "7 seats", "7 seater")):
                    category = "7 Seater"
                elif any(k in local_txt for k in ("9 lugares", "9 seats", "9 seater")):
                    category = "9 Seater"
                elif any(k in local_txt for k in ("mini", "pequeno")):
                    category = "Mini"
                elif any(k in local_txt for k in ("economy", "econ√≥mico", "economico")):
                    category = "Economy"
                # As a last resort, try to infer from car name trailing token
                if not category and car_name:
                    tail = (car_name.split()[-1] or "").lower()
                    tail_map = {
                        "suv": "SUV",
                        "economy": "Economy",
                        "mini": "Mini",
                        "wagon": "Estate/Station Wagon",
                        "estate": "Estate/Station Wagon",
                        "premium": "Premium",
                        "7": "7 Seater",
                        "7-seater": "7 Seater",
                        "9": "9 Seater",
                        "9-seater": "9 Seater",
                    }
                    category = tail_map.get(tail, category)
            # If car_name still empty, heuristically derive from local text by removing category tokens and prices
            if not car_name:
                try:
                    local_txt_full = card.get_text(" \n", strip=True)
                    lines = [l.strip() for l in local_txt_full.split("\n") if l.strip()]
                    # remove lines that are price-like
                    price_like = re.compile(r"(‚Ç¨|EUR|GBP|\¬£|\d+[\.,]\d{2})", re.I)
                    candidates = [l for l in lines if not price_like.search(l)]
                    if candidates:
                        car_name = candidates[0]
                        # strip trailing category word if present
                        if category and car_name.lower().endswith(category.lower()):
                            car_name = car_name[: -len(category)].strip()
                except Exception:
                    pass
            # Fiat 500 Cabrio -> Group G (Premium)
            try:
                _cn_lower = (car_name or "").lower()
                if re.search(r"\bfiat\s*500\b.*\b(cabrio|convertible|cabriolet)\b", _cn_lower):
                    category = "Premium"
            except Exception:
                pass
            # Mini cabrio variants -> Group G (Premium)
            try:
                _cn_lower = (car_name or "").lower()
                if re.search(r"\bmini\s+(one|cooper)\b.*\b(cabrio|convertible|cabriolet)\b", _cn_lower):
                    category = "Premium"
            except Exception:
                pass
            # Specific model mappings to requested groups
            try:
                cn = (car_name or "").lower()
                # Mini Countryman (incl. Cooper Countryman): E2 if Auto, else D (Economy)
                if re.search(r"\bmini\s+(cooper\s+)?countryman\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Economy Automatic"
                    else:
                        category = "Economy"
                # Peugeot 108 Cabrio -> G (Premium)
                if re.search(r"\bpeugeot\s*108\b.*\b(cabrio|convertible|cabriolet)\b", cn):
                    category = "Premium"
                # Fiat 500 Auto -> E1 (Mini Automatic) unless Cabrio already handled
                if re.search(r"\bfiat\s*500\b.*\b(auto|automatic)\b", cn) and not re.search(r"\b(cabrio|convertible|cabriolet)\b", cn):
                    category = "Mini Automatic"
                # Citroen C3 Auto -> E2 (Economy Automatic)
                if re.search(r"\bcitro[e√´]n\s*c3\b.*\b(auto|automatic)\b", cn) and not re.search(r"\bc3\s*aircross\b", cn):
                    category = "Economy Automatic"
                # Citroen C3 (non-Aircross, non-Auto) -> D (Economy)
                if re.search(r"\bcitro[e√´]n\s*c3\b", cn) and not re.search(r"\b(auto|automatic)\b", cn) and not re.search(r"\bc3\s*aircross\b", cn):
                    category = "Economy"
                # Citroen C3 Aircross Auto -> L1 (SUV Automatic)
                if re.search(r"\bcitro[e√´]n\s*c3\s*aircross\b.*\b(auto|automatic)\b", cn):
                    category = "SUV Automatic"
                # Toyota Aygo X -> F (SUV)
                if re.search(r"\btoyota\s*aygo\s*x\b", cn):
                    category = "SUV"
                # Fiat 500L -> J1 (Crossover)
                if re.search(r"\bfiat\s*500l\b", cn):
                    category = "Crossover"
                # Renault Clio SW/estate variants -> J2 (Estate/Station Wagon); autos will be L2 via suffix
                if re.search(r"\brenault\s*clio\b", cn) and re.search(r"\b(sw|st|sport\s*tourer|tourer|break|estate|kombi|grandtour|grand\s*tour|sporter|wagon)\b", cn):
                    category = "Estate/Station Wagon"
                # Group J1 (Crossover) models
                j1_patterns = [
                    r"\bkia\s*sportage\b",
                    r"\bnissan\s*qashqai\b",
                    r"\b(skoda|≈°koda)\s*kamiq\b",
                    r"\bhyundai\s*tucson\b",
                    r"\bseat\s*ateca\b",
                    r"\bmazda\s*cx[- ]?3\b",
                    r"\bpeugeot\s*5008\b",
                    r"\bpeugeot\s*3008\b",
                    r"\bpeugeot\s*2008\b",
                    r"\brenault\s*austral\b",
                    r"\btoyota\s*hilux\b.*\b4x4\b",
                ]
                if any(re.search(p, cn) for p in j1_patterns):
                    category = "Crossover"
                # Peugeot 308 base -> J1; 308 SW: Auto -> L2, else J2
                if re.search(r"\bpeugeot\s*308\b", cn):
                    if re.search(r"\bsw\b", cn):
                        if _is_auto_flag(cn, _page_text, transmission_label):
                            category = "Station Wagon Automatic"
                        else:
                            category = "Estate/Station Wagon"
                    else:
                        category = "Crossover"
                # VW Golf SW/Variant: Auto -> L2, else J2
                if re.search(r"\b(vw|volkswagen)\s*golf\b", cn) and re.search(r"\b(sw|variant)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # VW Passat: base & Variant -> J2; Auto -> L2
                if re.search(r"\b(vw|volkswagen)\s*passat\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Seat Leon SW/ST/Variant/Estate: Auto -> L2, else J2
                if re.search(r"\bseat\s*leon\b", cn) and re.search(r"\b(sw|st|variant|sport\s*tourer|sportstourer|estate)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Skoda Scala: base -> J2; Auto -> L2
                if re.search(r"\b(skoda|≈°koda)\s*scala\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Seat Arona -> F (SUV) regardless of transmission
                if re.search(r"\bseat\s*arona\b", cn):
                    category = "SUV"
                # Hyundai Kona/Kauai -> F (SUV) regardless of transmission
                if re.search(r"\bhyundai\s*(kona|kauai)\b", cn):
                    category = "SUV"
                # Skoda Octavia -> J2 (Station Wagon)
                if re.search(r"\b(skoda|≈°koda)\s*octavia\b", cn):
                    category = "Estate/Station Wagon"
                # Toyota Corolla SW/TS/Touring Sports: Auto -> L2 else J2
                if re.search(r"\btoyota\s*corolla\b", cn) and re.search(r"\b(sw|ts|touring\s*sports?|sport\s*touring|estate|wagon)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Toyota Corolla base (non-wagon) Auto -> E2
                if re.search(r"\btoyota\s*corolla\b", cn) and not re.search(r"\b(sw|ts|touring\s*sports?|sport\s*touring|estate|wagon)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Economy Automatic"
                # Peugeot 508 -> J2; Auto -> L2 (Station Wagon Automatic)
                if re.search(r"\bpeugeot\s*508\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Hyundai i30 -> J2; Auto -> L2
                if re.search(r"\bhyundai\s*i30\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Cupra Formentor Auto -> L1
                if re.search(r"\bcupra\s*formentor\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "SUV Automatic"
                # Renault Megane Sedan Auto -> L2
                if re.search(r"\brenault\s*megane\b", cn) and re.search(r"\bsedan\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "Station Wagon Automatic"
                # Renault Megane SW/Estate/Wagon: J2; Auto -> L2
                if re.search(r"\brenault\s*megane\b", cn) and re.search(r"\b(sw|estate|wagon|sport\s*tourer|sport\s*tourismo|tourer)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Cupra Leon SW Auto -> L2
                if re.search(r"\bcupra\s*leon\b", cn) and re.search(r"\b(sw|st|sport\s*tourer|sportstourer|estate|variant)\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "Station Wagon Automatic"
                # Toyota Yaris Cross Auto -> L1
                if re.search(r"\btoyota\s*yaris\s*cross\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "SUV Automatic"
                # Nissan Juke -> F (SUV) regardless of transmission
                if re.search(r"\bnissan\s*juke\b", cn):
                    category = "SUV"
                # Toyota Yaris Auto -> E1
                if re.search(r"\btoyota\s*yaris\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "Mini Automatic"
                # Kia Picanto Auto -> E1
                if re.search(r"\bkia\s*picanto\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "Mini Automatic"
                # VW Taigo -> F (SUV) regardless of transmission
                if re.search(r"\b(vw|volkswagen)\s*taigo\b", cn):
                    category = "SUV"
                # Mitsubishi Spacestar Auto -> E1
                if re.search(r"\bmitsubishi\s*space\s*star|spacestar\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "Mini Automatic"
                # Renault Megane Auto -> E2 (use card-level text)
                if re.search(r"\brenault\s*megane\b", cn):
                    _ct = ""
                    try:
                        _ct = card.get_text(" ", strip=True).lower()
                    except Exception:
                        _ct = ""
                    if _is_auto_flag(cn, _ct, transmission_label):
                        category = "Economy Automatic"
                # Ford Puma -> F (SUV) regardless of transmission
                if re.search(r"\bford\s*puma\b", cn):
                    category = "SUV"
                # Citroen C5 Aircross Auto -> L1
                if re.search(r"\bcitro[e√´]n\s*c5\s*aircross\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "SUV Automatic"
                # Toyota C-HR Auto -> L1
                if re.search(r"\btoyota\s*c[-\s]?hr\b|\btoyota\s*chr\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "SUV Automatic"
                # Kia Stonic -> F (SUV) regardless of transmission
                if re.search(r"\bkia\s*stonic\b", cn):
                    category = "SUV"
                # Ford EcoSport -> F (SUV) regardless of transmission
                if re.search(r"\bford\s*eco\s*sport\b|\bford\s*ecosport\b", cn):
                    category = "SUV"
                # Opel/Vauxhall Crossland X -> F (SUV); Auto remains L1 via final if needed
                if re.search(r"\b(opel|vauxhall)\s*crossland\s*x?\b", cn):
                    category = "SUV"
                # Ford Focus SW/Estate/Wagon variants: J2; Auto -> L2
                if re.search(r"\bford\s*focus\b", cn) and re.search(r"\b(sw|estate|wagon|turnier|kombi|sportbreak|sport\s*brake|tourer|touring)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Ford Focus base (non-wagon): D or E2
                if re.search(r"\bford\s*focus\b", cn) and not re.search(r"\b(sw|estate|wagon)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Economy Automatic"
                    else:
                        category = "Economy"
                # Seat Leon base (non-wagon): D or E2 (use card-level text)
                if re.search(r"\bseat\s*leon\b", cn) and not re.search(r"\b(sw|st|variant|sport\s*tourer|sportstourer|estate|wagon)\b", cn):
                    _ct = ""
                    try:
                        _ct = card.get_text(" ", strip=True).lower()
                    except Exception:
                        _ct = ""
                    if _is_auto_flag(cn, _ct, transmission_label):
                        category = "Economy Automatic"
                    else:
                        category = "Economy"
                # Kia Ceed base (non-wagon): D or E2
                if re.search(r"\bkia\s*ceed\b", cn) and not re.search(r"\b(sw|estate|wagon|sportswagon|sports\s*wagon)\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Economy Automatic"
                    else:
                        category = "Economy"
                # Opel/Vauxhall Astra: base & SW -> J2; Auto -> L2
                if re.search(r"\b(opel|vauxhall)\s*astra\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # VW T-Cross Auto -> L1 (unchanged)
                if re.search(r"\b(vw|volkswagen)\s*t[-\s]?cross\b", cn) and _is_auto_flag(cn, _page_text, transmission_label):
                    category = "SUV Automatic"
                # VW Golf Auto (hatch) -> E2 (use card-level text)
                if re.search(r"\b(vw|volkswagen)\s*golf\b", cn) and not re.search(r"\b(sw|variant|estate|wagon)\b", cn):
                    _ct = ""
                    try:
                        _ct = card.get_text(" ", strip=True).lower()
                    except Exception:
                        _ct = ""
                    if _is_auto_flag(cn, _ct, transmission_label):
                        category = "Economy Automatic"
                # Dacia Jogger -> M1 (7 Seater); automatic will auto-suffix to M2 later
                if re.search(r"\bdacia\s*jogger\b", cn):
                    category = "7 Seater"
                # Fiat 500X -> J1 (Crossover); Auto -> L1
                if re.search(r"\bfiat\s*500x\b", cn):
                    if _is_auto_flag(cn, _page_text, transmission_label):
                        category = "SUV Automatic"
                    else:
                        category = "Crossover"
                # VW Beetle Cabrio -> G (Premium)
                if re.search(r"\b(vw|volkswagen)\s*beetle\b.*\b(cabrio|convertible|cabriolet)\b", cn):
                    category = "Premium"
                # Group L1 (SUV Automatic) for specific models when Automatic is detected (including acronyms)
                try:
                    _card_txt = ""
                    try:
                        _card_txt = card.get_text(" ", strip=True).lower()
                    except Exception:
                        _card_txt = ""
                    is_auto = _is_auto_flag(cn, _card_txt, transmission_label)
                    # Only keep intended L1 autos; others remain F per latest rules
                    is_l1_model = (
                        re.search(r"\bpeugeot\s*(3008|2008|5008)\b", cn) or
                        re.search(r"\bnissan\s*qashqai\b", cn) or
                        re.search(r"\b(skoda|≈°koda)\s*kamiq\b", cn) or
                        re.search(r"\bcitro[e√´]n\s*c4\b", cn) or
                        re.search(r"\b(vw|volkswagen)\s*tiguan\b", cn) or
                        re.search(r"\bds(\s*automobiles)?\s*4\b", cn) or
                        re.search(r"\b(skoda|≈°koda)\s*karoq\b", cn) or
                        re.search(r"\bford\s*kuga\b", cn) or
                        re.search(r"\bjeep\s*renegade\b", cn) or
                        re.search(r"\brenault\s*arkana\b", cn) or
                        re.search(r"\btoyota\s*rav\s*4\b|\brav4\b", cn) or
                        re.search(r"\bcupra\s*formentor\b", cn) or
                        re.search(r"\btoyota\s*yaris\s*cross\b", cn) or
                        re.search(r"\bcitro[e√´]n\s*c5\s*aircross\b", cn) or
                        re.search(r"\btoyota\s*c[-\s]?hr\b|\btoyota\s*chr\b", cn) or
                        re.search(r"\b(vw|volkswagen)\s*t[-\s]?cross\b", cn) or
                        re.search(r"\bfiat\s*500x\b", cn)
                    )
                    if is_auto and is_l1_model:
                        category = "SUV Automatic"
                except Exception:
                    pass
                # Citroen C4 Picasso (non-Grand) -> M1 (7 Seater). Auto will suffix to M2 later
                if re.search(r"\bcitro[e√´]n\s*c4\s*picasso\b", cn) and not re.search(r"\bgrand\b", cn):
                    category = "7 Seater"
                # Citroen Grand C4 Picasso/Grand Spacetourer -> M1 base; auto will suffix to M2
                if re.search(r"\bcitro[e√´]n\s*c4\s*(grand\s*picasso|grand\s*spacetourer|grand\s*space\s*tourer)\b", cn):
                    category = "7 Seater"
            except Exception:
                pass
            # Group D (Economy) models; Auto -> Economy Automatic (use card-level text for auto detection)
            d_models = [
                r"dacia\s+sandero",
                r"peugeot\s*208",
                r"opel\s*corsa",
                r"seat\s*ibiza",
                r"seat\s*leon",
                r"kia\s*ceed",
                r"(vw|volkswagen)\s*polo",
                r"renault\s*clio",
                r"ford\s*fiesta",
                r"ford\s*focus",
                r"hyundai\s*i20",
                r"nissan\s*micra",
                r"audi\s*a1",
            ]
            if any(re.search(p, cn) for p in d_models):
                _ct = ""
                try:
                    _ct = card.get_text(" ", strip=True).lower()
                except Exception:
                    _ct = ""
                if _is_auto_flag(cn, _ct, transmission_label):
                    category = "Economy Automatic"
                else:
                    category = "Economy"
            # Force B1 mapping for specific models the user provided (non-Auto/Non-Cabrio, base Mini only)
            try:
                _b1_models = [
                    "fiat 500", "peugeot 108", "opel adam",
                    "toyota aygo", "volkswagen up", "vw up", "ford ka", "renault twingo",
                    "citroen c1", "citro√´n c1", "kia picanto"
                ]
                _cn = (car_name or "").lower()
                if any(m in _cn for m in _b1_models):
                    # do not apply B1 if auto/automatic (multi-language/abbrev) or cabrio/convertible/cabriolet
                    if (not _is_auto_flag(_cn, _page_text, transmission_label)) and not re.search(r"\b(cabrio|convertible|cabriolet)\b", _cn, re.I):
                        # exclude variants that map elsewhere: 500X/500L, Aygo X, Aircross
                        if not re.search(r"\b(500x|500l|aygo\s*x|aircross|countryman)\b", _cn):
                            # and only when category is not already a non-Mini mapping
                            if category in ("", "Mini"):
                                category = "Mini 4 Doors"
            except Exception:
                pass
            # Refine Mini into 'Mini 4 Doors' when doors info is present
            try:
                if category == "Mini":
                    _lt = ""
                    try:
                        _lt = card.get_text(" ", strip=True).lower()
                    except Exception:
                        _lt = ""
                    _cn = (car_name or "").lower()
                    four_pat = re.compile(r"\b(4\s*(doors?|portas|p)|4p|4-door|4-portas)\b", re.I)
                    if four_pat.search(_lt) or four_pat.search(_cn):
                        category = "Mini 4 Doors"
            except Exception:
                pass
            # link
            link = url_from_row(card, base_url) or base_url
            # Photo cache: upsert or read from cache based on model key
            try:
                if car_name:
                    _key = _normalize_model_key(car_name)
                    if photo:
                        _cache_set_photo(_key, photo)
                    else:
                        cached_photo = _cache_get_photo(_key)
                        if cached_photo:
                            photo = cached_photo
            except Exception:
                pass
            # Crossover override when car name is present (exclude C4 Picasso/Grand Spacetourer)
            try:
                _car_lc = (car_name or "").lower()
                is_c4_picasso_like = re.search(r"\bc4\s*(picasso|grand\s*spacetourer|grand\s*space\s*tourer)\b", _car_lc)
                if re.search(r"\b(peugeot\s*2008|peugeot\s*3008|citro[e√´]n\s*c4)\b", _car_lc, re.I) and not is_c4_picasso_like:
                    category = "Crossover"
            except Exception:
                pass
            # Automatic suffix for selected groups
            try:
                if transmission_label == "Automatic" and category in ("Mini", "Economy", "SUV", "Estate/Station Wagon", "7 Seater"):
                    if category == "Estate/Station Wagon":
                        category = "Station Wagon Automatic"
                    elif category == "7 Seater":
                        category = "7 Seater Automatic"
                    else:
                        category = f"{category} Automatic"
            except Exception:
                pass
            # FINAL OVERRIDE: Ensure Group D/E2 models are correctly placed (Peugeot 208, Opel Corsa, Seat Ibiza, VW Polo, Renault Clio, Ford Fiesta, Nissan Micra, Hyundai i20, Audi A1)
            try:
                cn2 = (car_name or "").lower()
                d_models_final = [
                    r"\bpeugeot\s*208\b",
                    r"\bopel\s*corsa\b",
                    r"\bseat\s*ibiza\b",
                    r"\bseat\s*leon\b",
                    r"\bkia\s*ceed\b",
                    r"\b(vw|volkswagen)\s*polo\b",
                    r"\bcitro[e√´]n\s*c3\b",
                    r"\brenault\s*clio\b",
                    r"\bford\s*fiesta\b",
                    r"\bford\s*focus\b",
                    r"\bnissan\s*micra\b",
                    r"\bhyundai\s*i20\b",
                    r"\baudi\s*a1\b",
                    r"\bdacia\s*sandero\b",
                ]
                # do not override if we already mapped to protected groups (wagon/crossover/suv)
                is_protected = category in ("Estate/Station Wagon", "Station Wagon Automatic", "Crossover", "SUV", "SUV Automatic")
                if (not is_protected) and any(re.search(p, cn2) for p in d_models_final):
                    if _is_auto_flag(cn2, _txt, transmission_label):
                        category = "Economy Automatic"
                    else:
                        category = "Economy"
            except Exception:
                pass
            # FINAL MANUAL OVERRIDE for D models: if manual is explicit, force D
            try:
                cn2b = (car_name or "").lower()
                is_d_family = any(re.search(p, cn2b) for p in [
                    r"\bpeugeot\s*208\b", r"\bopel\s*corsa\b", r"\bseat\s*ibiza\b",
                    r"\bseat\s*leon\b", r"\b(vw|volkswagen)\s*golf\b", r"\b(vw|volkswagen)\s*polo\b",
                    r"\brenault\s*clio\b", r"\bford\s*fiesta\b", r"\bnissan\s*micra\b",
                    r"\bhyundai\s*i20\b", r"\baudi\s*a1\b", r"\bdacia\s*sandero\b", r"\brenault\s*megane\b",
                ])
                # re-evaluate card text for manual marker
                _txt2 = ""
                try:
                    _txt2 = card.get_text(" ", strip=True).lower()
                except Exception:
                    _txt2 = ""
                is_manual = (str(transmission_label or '').lower() == 'manual') or bool(re.search(r"\bmanual\b", _txt2))
                if is_d_family and is_manual and category not in ("Estate/Station Wagon", "Station Wagon Automatic"):
                    category = "Economy"
            except Exception:
                pass
            # FINAL L2/J2 OVERRIDE: enforce wagons to wagon groups; autos -> L2
            try:
                cnf = (car_name or "").lower()
                _txt = ""
                try:
                    _txt = card.get_text(" ", strip=True).lower()
                except Exception:
                    _txt = ""
                # Renault Clio SW: force to wagon groups
                if re.search(r"\brenault\s*clio\b", cnf) and re.search(r"\b(sw|st|sport\s*tourer|tourer|break|estate|kombi|grandtour|grand\s*tour|sporter|wagon)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                cn3 = (car_name or "").lower()
                is_auto_any = _is_auto_flag(cn3, _txt, transmission_label)
                l1_model = (
                    re.search(r"\bpeugeot\s*(3008|2008|5008)\b", cn3) or
                    re.search(r"\bnissan\s*qashqai\b", cn3) or
                # ... (rest of the code remains the same)
                    re.search(r"\b(skoda|≈°koda)\s*kamiq\b", cn3) or
                    re.search(r"\bcitro[e√´]n\s*c4\b", cn3) or
                    re.search(r"\b(vw|volkswagen)\s*tiguan\b", cn3) or
                    re.search(r"\bds(\s*automobiles)?\s*4\b", cn3) or
                    re.search(r"\b(skoda|≈°koda)\s*karoq\b", cn3) or
                    re.search(r"\bford\s*kuga\b", cn3) or
                    re.search(r"\bjeep\s*renegade\b", cn3) or
                    re.search(r"\brenault\s*arkana\b", cn3) or
                    re.search(r"\btoyota\s*rav\s*4\b|\brav4\b", cn3) or
                    re.search(r"\bcupra\s*formentor\b", cn3) or
                    re.search(r"\btoyota\s*yaris\s*cross\b", cn3) or
                    re.search(r"\bcitro[e√´]n\s*c5\s*aircross\b", cn3) or
                    re.search(r"\btoyota\s*c[-\s]?hr\b|\btoyota\s*chr\b", cn3) or
                    re.search(r"\b(vw|volkswagen)\s*t[-\s]?cross\b", cn3) or
                    re.search(r"\bfiat\s*500x\b", cn3)
                )
                # don't override M2 or wagons
                is_m2 = category == "7 Seater Automatic" or re.search(r"\bc4\s*(picasso|grand\s*spacetourer|grand\s*space\s*tourer)\b", cn3)
                is_wagon = category in ("Estate/Station Wagon", "Station Wagon Automatic")
                if is_auto_any and l1_model and (not is_m2) and (not is_wagon):
                    category = "SUV Automatic"
            except Exception:
                pass
            # FINAL L2/J2 OVERRIDE: 308 SW and Scala to wagon groups; autos -> L2
            try:
                cnf = (car_name or "").lower()
                if re.search(r"\bford\s*focus\b", cnf) and re.search(r"\b(sw|estate|wagon|turnier|kombi|sportbreak|sport\s*brake|tourer|touring)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\b(vw|volkswagen)\s*golf\b", cnf) and re.search(r"\b(sw|variant)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\bfiat\s*500l\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\b(vw|volkswagen)\s*passat\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\bpeugeot\s*508\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\bhyundai\s*i30\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\btoyota\s*corolla\b", cnf) and re.search(r"\b(sw|ts|touring\s*sports?|sport\s*touring|estate|wagon)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                # Enforce E2 for Toyota Corolla base Auto
                if re.search(r"\btoyota\s*corolla\b", cnf) and not re.search(r"\b(sw|ts|touring\s*sports?|sport\s*touring|estate|wagon)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Economy Automatic"
                if re.search(r"\bseat\s*leon\b", cnf) and re.search(r"\b(sw|st|variant|sport\s*tourer|sportstourer|estate)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\b(skoda|≈°koda)\s*scala\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\bford\s*focus\b", cnf) and re.search(r"\b(sw|estate|wagon)\b", cnf) and _is_auto_flag(cnf, _txt, transmission_label):
                    category = "Station Wagon Automatic"
                if re.search(r"\b(opel|vauxhall)\s*astra\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
                if re.search(r"\brenault\s*megane\b", cnf) and re.search(r"\bsedan\b", cnf) and _is_auto_flag(cnf, _txt, transmission_label):
                    category = "Station Wagon Automatic"
                if re.search(r"\brenault\s*megane\b", cnf) and re.search(r"\b(sw|estate|wagon|sport\s*tourer|sport\s*tourismo|tourer)\b", cnf):
                    if _is_auto_flag(cnf, _txt, transmission_label):
                        category = "Station Wagon Automatic"
                    else:
                        category = "Estate/Station Wagon"
            except Exception:
                pass
            # FINAL M2 OVERRIDE: common 7-seater autos -> 7 Seater Automatic (wins over J1/D)
            try:
                cn4 = (car_name or "").lower()
                m2_patterns = [
                    r"\bcitro[e√´]n\s*c4\s*(picasso|grand\s*spacetourer|grand\s*space\s*tourer)\b",
                    r"\bcitro[e√´]n\s*grand\s*picasso\b",
                    r"\brenault\s*grand\s*sc[e√©]nic\b",
                    r"\bmercedes\s*glb\b.*\b(7\s*seater|7\s*lugares|7p|7\s*seats)\b",
                    r"\b(vw|volkswagen)\s*multivan\b",
                    r"\bpeugeot\s*rifter\b",
                ]
                if any(re.search(p, cn4) for p in m2_patterns) and _is_auto_flag(cn4, _txt, transmission_label):
                    category = "7 Seater Automatic"
            except Exception:
                pass
            # FINAL E1 OVERRIDE: Toyota Aygo Auto -> Mini Automatic (avoid uncategorized)
            try:
                cn5 = (car_name or "").lower()
                if re.search(r"\btoyota\s*aygo\b", cn5) and _is_auto_flag(cn5, _txt, transmission_label):
                    category = "Mini Automatic"
                if re.search(r"\bkia\s*picanto\b", cn5) and _is_auto_flag(cn5, _txt, transmission_label):
                    category = "Mini Automatic"
            except Exception:
                pass
            # FINAL B1 OVERRIDE: base mini models -> 'Mini 4 Doors' (when not auto/cabrio/special variants)
            try:
                b1_list = [
                    r"\bfiat\s*500\b",
                    r"\bcitro[e√´]n\s*c1\b",
                    r"\bpeugeot\s*108\b",
                    r"\bopel\s*adam\b",
                    r"\btoyota\s*aygo\b",
                    r"\b(vw|volkswagen)\s*up\b",
                    r"\bford\s*ka\b",
                    r"\brenault\s*twingo\b",
                    r"\bkia\s*picanto\b",
                ]
                _name = (car_name or "").lower()
                if any(re.search(p, _name) for p in b1_list):
                    # do not apply if this is a D/E2 economy model (protect Group D)
                    d_guard = [
                        r"\bpeugeot\s*208\b", r"\bopel\s*corsa\b", r"\bseat\s*ibiza\b",
                        r"\b(vw|volkswagen)\s*polo\b", r"\bcitro[e√´]n\s*c3\b", r"\brenault\s*clio\b",
                        r"\bford\s*fiesta\b", r"\bnissan\s*micra\b", r"\bhyundai\s*i20\b", r"\baudi\s*a1\b",
                        r"\bdacia\s*sandero\b"
                    ]
                    if any(re.search(p, _name) for p in d_guard):
                        raise Exception("skip B1 for D/E2 models")
                    # exclude autos and cabrio and special variants
                    if (not _is_auto_flag(_name, _txt, transmission_label)) \
                        and not re.search(r"\b(cabrio|convertible|cabriolet)\b", _name) \
                        and not re.search(r"\b(500x|500l|aygo\s*x|aircross|countryman)\b", _name):
                        category = "Mini 4 Doors"
            except Exception:
                pass
            # Skip blocked models - DISABLED: mostrar todos os carros
            # if car_name and _is_blocked_model(car_name):
            #     cards_blocked += 1
            #     continue
            # Mapear categoria para c√≥digo de grupo
            group_code = map_category_to_group(category, car_name, transmission_label)
            # Capitalizar nome para display (Peugeot 2008 Auto, Renault Megane SW Auto)
            car_name_display = capitalize_car_name(car_name)
            items.append({
                "id": idx,
                "car": car_name_display,
                "supplier": supplier,
                "price": price_text,
                "currency": "",
                "category": category,
                "group": group_code,
                "transmission": transmission_label,
                "photo": photo,
                "link": link,
            })
            idx += 1
        print(f"[PARSE] Stats: price={cards_with_price}, name={cards_with_name}, blocked={cards_blocked}, items={len(items)}")
        if items:
            print(f"[PARSE] Returning {len(items)} items from card parsing")
            return items
    except Exception:
        pass

    # Require an explicit currency marker to avoid capturing ratings/ages
    price_regex = re.compile(r"(?:‚Ç¨\s*\d{1,4}(?:[\.,]\d{3})*(?:[\.,]\d{2})?|\bEUR\s*\d{1,4}(?:[\.,]\d{3})*(?:[\.,]\d{2})?)", re.I)

    # Basic category keyword list (EN + PT)
    CATEGORY_KEYWORDS = [
        "mini","economy","compact","intermediate","standard","full-size","full size","suv","premium","luxury","van","estate","convertible","people carrier","minivan","midsize",
        "mini","econ√≥mico","econ√≥mico","compacto","interm√©dio","padr√£o","familiar","suv","premium","luxo","carrinha","descapot√°vel","monovolume","m√©dio"
    ]

    candidates = []
    for el in soup.find_all(text=price_regex):
        try:
            txt = el.strip()
        except Exception:
            continue
        if not txt or len(txt) > 50:
            continue
        node = el if hasattr(el, 'parent') else None
        if not node:
            continue
        # climb up to find a reasonable container (card/row)
        container = node.parent
        depth = 0
        while container and depth < 6 and container.name not in ("tr", "li", "article", "section", "div"):
            container = container.parent
            depth += 1
        if not container:
            container = node.parent
        candidates.append((container, txt))

    seen = set()
    for idx, (container, price_text) in enumerate(candidates):
        # car/model
        name_el = container.select_one(".car, .vehicle, .model, .title, .name, .veh-name, [class*='model'], [class*='vehicle']")
        car_name = name_el.get_text(strip=True) if name_el else ""
        # supplier: try explicit, else alt/title of images within container
        supplier_el = container.select_one(".supplier, .vendor, .partner, [class*='supplier'], [class*='vendor']")
        supplier = supplier_el.get_text(strip=True) if supplier_el else ""
        if not supplier:
            img = container.select_one("img[alt], img[title]")
            if img:
                supplier = img.get("alt") or img.get("title") or ""
        # category/group: try explicit labels then keyword search in container text
        cat_el = container.select_one(".category, .group, .vehicle-category, [class*='category'], [class*='group'], [class*='categoria'], [class*='grupo']")
        category = cat_el.get_text(strip=True) if cat_el else ""
        if not category:
            try:
                text = container.get_text(" ", strip=True).lower()
                match = next((kw for kw in CATEGORY_KEYWORDS if kw.lower() in text), "")
                category = match.title() if match else ""
            except Exception:
                category = ""
        # Crossover override based on model name (when available)
        try:
            _car_lc = (car_name or "").lower()
            if re.search(r"\b(peugeot\s*2008|peugeot\s*3008|citro[e√´]n\s*c4)\b", _car_lc, re.I):
                category = "Crossover"
        except Exception:
            pass

        # link
        link = url_from_row(container, base_url) or base_url

        key = (supplier, car_name, price_text)
        if key in seen:
            continue
        seen.add(key)

        # detect currency symbol present in the text
        curr = "EUR" if re.search(r"EUR", price_text, re.I) else ("EUR" if "‚Ç¨" in price_text else "")
        # Mapear categoria para c√≥digo de grupo
        group_code = map_category_to_group(category, car_name, transmission_label)
        items.append({
            "id": idx,
            "car": car_name,
            "supplier": supplier,
            "price": price_text,
            "currency": curr,
            "category": category,
            "group": group_code,
            "transmission": transmission_label,
            "link": link,
        })
        # REMOVED: if len(items) >= 50: break  # Removido limite para mostrar TODOS os carros
    # If no detailed items parsed, fall back to provider summaries to ensure prices are shown
    if not items and summary_items:
        items = summary_items
    # Ensure photos when grupo/category_code is known
    try:
        for it in items:
            if (not it.get("photo")) and it.get("category_code"):
                cc = it.get("category_code")
                it["photo"] = urljoin(base_url, f"/cdn/img/cars/S/car_{cc}.jpg")
    except Exception:
        pass
    return items


def url_from_row(row, base_url: str) -> str:
    a = row.select_one("a[href]")
    if a and a.has_attr("href"):
        href = a["href"]
        if href and not href.lower().startswith("javascript") and href != "#":
            return urljoin(base_url, href)
    for attr in ["data-href", "data-url", "data-link"]:
        el = row.select_one(f"*[{attr}]")
        if el and el.has_attr(attr):
            return urljoin(base_url, el[attr])
    clickable = row.select_one("*[onclick]")
    if clickable and clickable.has_attr("onclick"):
        m = re.search(r"https?://[^'\"]+", clickable["onclick"])  
        if m:
            return m.group(0)
    return ""


def try_direct_carjet(location_name: str, start_dt, end_dt, lang: str = "pt", currency: str = "EUR") -> str:
    try:
        sess = requests.Session()
        ua = {
            "User-Agent": "Mozilla/5.0 (compatible; PriceTracker/1.0)",
            "Accept-Language": "pt-PT,pt;q=0.9,en;q=0.6",
            "X-Forwarded-For": "185.23.160.1",
            "Referer": "https://www.carjet.com/do/list/pt",
        }
        lang = (lang or "pt").lower()
        # Pre-seed cookies to bias locale
        try:
            sess.cookies.set("monedaForzada", currency)
            sess.cookies.set("moneda", currency)
            sess.cookies.set("currency", currency)
            sess.cookies.set("idioma", lang.upper())
            sess.cookies.set("lang", lang)
            sess.cookies.set("country", "PT")
        except Exception:
            pass

        # 1) GET locale homepage to mint session and try to capture s/b tokens
        if lang == "pt":
            home_path = "aluguel-carros/index.htm"
        elif lang == "es":
            home_path = "alquiler-coches/index.htm"
        elif lang == "fr":
            home_path = "location-voitures/index.htm"
        elif lang == "de":
            home_path = "mietwagen/index.htm"
        elif lang == "it":
            home_path = "autonoleggio/index.htm"
        elif lang == "nl":
            home_path = "autohuur/index.htm"
        else:
            home_path = "index.htm"
        home_url = f"https://www.carjet.com/{home_path}"
        home = sess.get(home_url, headers=ua, timeout=20)
        s_token = None
        b_token = None
        try:
            m = re.search(r"[?&]s=([A-Za-z0-9]+)", home.text)
            if m:
                s_token = m.group(1)
            m = re.search(r"[?&]b=([A-Za-z0-9]+)", home.text)
            if m:
                b_token = m.group(1)
        except Exception:
            pass

        # 2) Prefer submitting the actual homepage form with all hidden fields preserved
        try:
            soup = BeautifulSoup(home.text, "lxml")
            form = soup.select_one("form[name='menu_tarifas'], form#booking_form")
            if form:
                action = form.get("action") or f"/do/list/{lang}"
                post_url = action if action.startswith("http") else requests.compat.urljoin(home_url, action)
                payload: Dict[str, Any] = {}
                # include all inputs
                for inp in form.select("input[name]"):
                    name = inp.get("name")
                    if not name:
                        continue
                    val = inp.get("value", "")
                    payload[name] = val
                # include selects
                for sel in form.select("select[name]"):
                    name = sel.get("name")
                    if not name:
                        continue
                    # take selected option or first
                    opt = sel.select_one("option[selected]") or sel.select_one("option")
                    payload[name] = opt.get("value") if opt else ""

                # override with our values
                override = build_carjet_form(location_name, start_dt, end_dt, lang=lang, currency=currency)
                payload.update({k: v for k, v in override.items() if v is not None})
                if s_token:
                    payload["s"] = s_token
                if b_token:
                    payload["b"] = b_token

                headers = {
                    "User-Agent": ua["User-Agent"],
                    "Origin": "https://www.carjet.com",
                    "Referer": home_url,
                }
                resp = sess.post(post_url, data=payload, headers=headers, timeout=25)
                if resp.status_code == 200 and resp.text:
                    return resp.text
        except Exception:
            pass

        # 3) Fallback: POST to /do/list/{lang} with our constructed payload
        data = build_carjet_form(location_name, start_dt, end_dt, lang=lang, currency=currency)
        if s_token:
            data["s"] = s_token
        if b_token:
            data["b"] = b_token

        headers = {
            "User-Agent": ua["User-Agent"],
            "Origin": "https://www.carjet.com",
            "Referer": home_url,
            "Accept-Language": ua.get("Accept-Language", "pt-PT,pt;q=0.9,en;q=0.6"),
            "X-Forwarded-For": ua.get("X-Forwarded-For", "185.23.160.1"),
        }
        url = f"https://www.carjet.com/do/list/{lang}"
        resp = sess.post(url, data=data, headers=headers, timeout=25)
        if resp.status_code == 200 and resp.text:
            # Detect if we were redirected to a generic homepage (wrong locale)
            homepage_like = False
            try:
                homepage_like = bool(re.search(r'hrental_pagetype"\s*:\s*"home"', resp.text) or re.search(r'data-steplist="home"', resp.text))
            except Exception:
                homepage_like = False
            if not homepage_like:
                return resp.text
            # Fallback path observed on results pages: modalFilter.asp then carList.asp
            try:
                mf_url = f"https://www.carjet.com/modalFilter.asp"
                # Minimal payload aligning with page
                mf_payload = {
                    "frmDestino": data.get("frmDestino") or data.get("dst_id") or data.get("pickupId") or "",
                    "frmFechaRecogida": f"{start_dt.strftime('%d/%m/%Y')} {start_dt.strftime('%H:%M')}",
                    "frmFechaDevolucion": f"{end_dt.strftime('%d/%m/%Y')} {end_dt.strftime('%H:%M')}",
                    "idioma": lang.upper(),
                    "frmMoneda": currency,
                    "frmTipoVeh": "CAR",
                }
                _ = sess.post(mf_url, data=mf_payload, headers=headers, timeout=20)
            except Exception:
                pass
            try:
                # Keep session tokens if available
                _q = f"idioma={lang.upper()}&case=2"
                if s_token:
                    _q += f"&s={s_token}"
                if b_token:
                    _q += f"&b={b_token}"
                cl_url = f"https://www.carjet.com/carList.asp?{_q}"
                rlist = sess.get(cl_url, headers=headers, timeout=25)
                if rlist.status_code == 200 and rlist.text:
                    return rlist.text
            except Exception:
                pass

        # If not OK or homepage detected, retry with PT-Portugal homepage and forced params on POST URL
        try:
            # Visit PT-Portugal homepage spelling (aluguer vs aluguel)
            home_url_ptpt = "https://www.carjet.com/aluguer-carros/index.htm"
            _ = sess.get(home_url_ptpt, headers=ua, timeout=20)
            headers2 = dict(headers)
            post_url2 = f"https://www.carjet.com/do/list/{lang}?idioma=PT&moneda=EUR&currency=EUR"
            resp2 = sess.post(post_url2, data=data, headers=headers2, timeout=25)
            if resp2.status_code == 200 and resp2.text:
                try:
                    if re.search(r'hrental_pagetype\"\s*:\s*\"home\"', resp2.text) or re.search(r'data-steplist=\"home\"', resp2.text):
                        pass
                    else:
                        return resp2.text
                except Exception:
                    return resp2.text
        except Exception:
            pass
    except Exception:
        pass
    return ""


def build_carjet_form(location_name: str, start_dt, end_dt, lang: str = "pt", currency: str = "EUR") -> Dict[str, Any]:
    # Build server-expected fields; include hidden destination IDs when possible
    pickup_dmY = start_dt.strftime("%d/%m/%Y")
    dropoff_dmY = end_dt.strftime("%d/%m/%Y")
    pickup_HM = start_dt.strftime("%H:%M")
    dropoff_HM = end_dt.strftime("%H:%M")
    code = LOCATION_CODES.get((location_name or "").lower(), "")
    form = {
        # free text
        "pickup": location_name,
        "dropoff": location_name,
        # hidden ids (best effort)
        "pickupId": code,
        "dst_id": code,
        "zoneCode": code,
        # dates
        "fechaRecogida": pickup_dmY,
        "fechaEntrega": dropoff_dmY,
        # times
        "fechaRecogidaSelHour": pickup_HM,
        "fechaEntregaSelHour": dropoff_HM,
        # locale hints
        "idioma": lang.upper(),
        "moneda": currency,
        "chkOneWay": "SI",
        # fields observed on list page (robustness)
        "frmDestino": code or "",
        "frmFechaRecogida": f"{pickup_dmY} {pickup_HM}",
        "frmFechaDevolucion": f"{dropoff_dmY} {dropoff_HM}",
        "frmMoneda": currency,
        "frmTipoVeh": "CAR",
    }
    return form


def fetch_with_optional_proxy(url: str, headers: Dict[str, str]):
    # Default locale headers if not provided
    try:
        headers = dict(headers or {})
        headers.setdefault("Accept-Language", "pt-PT,pt;q=0.9,en;q=0.6")
        headers.setdefault("X-Forwarded-For", "185.23.160.1")
    except Exception:
        pass
    # Prefer direct fetch with EUR cookies for CarJet to reduce latency and avoid geolocation flips
    try:
        from urllib.parse import urlparse as _urlparse
        pr = _urlparse(url)
        # Allow forcing proxy usage for CarJet via env flag (FORCE_PROXY_FOR_CARJET=1/true)
        _force_proxy_cj = False
        try:
            _force_proxy_cj = str(os.getenv("FORCE_PROXY_FOR_CARJET", "")).strip().lower() in ("1", "true", "yes", "on")
        except Exception:
            _force_proxy_cj = False
        if pr.netloc.endswith("carjet.com") and not _force_proxy_cj:
            h2 = dict(headers or {})
            h2["Cookie"] = "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt"
            if _HTTPX_CLIENT:
                return _HTTPX_CLIENT.get(url, headers=h2)
            return requests.get(url, headers=h2, timeout=20)
    except Exception:
        pass
    if SCRAPER_SERVICE.lower() == "scrapeops" and SCRAPER_API_KEY:
        try:
            params = {
                "api_key": SCRAPER_API_KEY,
                "url": url,
                "render_js": "true",
            }
            if SCRAPER_COUNTRY:
                params["country"] = SCRAPER_COUNTRY
            r = requests.get("https://proxy.scrapeops.io/v1/", params=params, headers=headers, timeout=30)
            if r.status_code in (401, 403):
                # Fallback to direct if proxy is unauthorized/forbidden
                if _HTTPX_CLIENT:
                    return _HTTPX_CLIENT.get(url, headers=headers)
                return requests.get(url, headers=headers, timeout=6)
            return r
        except Exception:
            # Fallback to direct on any proxy error
            if _HTTPX_CLIENT:
                return _HTTPX_CLIENT.get(url, headers=headers)
            return requests.get(url, headers=headers, timeout=10)
    if _HTTPX_CLIENT:
        return _HTTPX_CLIENT.get(url, headers=headers)
    return requests.get(url, headers=headers, timeout=20)


async def async_fetch_with_optional_proxy(url: str, headers: Dict[str, str]):
    try:
        headers = dict(headers or {})
        headers.setdefault("Accept-Language", "pt-PT,pt;q=0.9,en;q=0.6")
        headers.setdefault("X-Forwarded-For", "185.23.160.1")
    except Exception:
        pass
    # Prefer direct CarJet with PT/EUR cookies
    try:
        from urllib.parse import urlparse as _urlparse
        pr = _urlparse(url)
        # Allow forcing proxy usage for CarJet via env flag (FORCE_PROXY_FOR_CARJET=1/true)
        _force_proxy_cj = False
        try:
            _force_proxy_cj = str(os.getenv("FORCE_PROXY_FOR_CARJET", "")).strip().lower() in ("1", "true", "yes", "on")
        except Exception:
            _force_proxy_cj = False
        if pr.netloc.endswith("carjet.com") and not _force_proxy_cj:
            h2 = dict(headers or {})
            h2["Cookie"] = "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt"
            if _HTTPX_ASYNC:
                return await _HTTPX_ASYNC.get(url, headers=h2)
            # fallback to sync in thread
            return await asyncio.to_thread(requests.get, url, headers=h2, timeout=6)
    except Exception:
        pass
    if SCRAPER_SERVICE.lower() == "scrapeops" and SCRAPER_API_KEY:
        try:
            params = {
                "api_key": SCRAPER_API_KEY,
                "url": url,
                "render_js": "true",
            }
            if SCRAPER_COUNTRY:
                params["country"] = SCRAPER_COUNTRY
            # httpx doesn't proxy this conveniently; use requests in a thread
            r = await asyncio.to_thread(requests.get, "https://proxy.scrapeops.io/v1/", params=params, headers=headers, timeout=6)
        except TypeError:
            # Fallback: direct fetch
            if _HTTPX_ASYNC:
                return await _HTTPX_ASYNC.get(url, headers=headers)
            return await asyncio.to_thread(requests.get, url, headers=headers, timeout=6)
        try:
            if r.status_code in (401, 403):
                if _HTTPX_ASYNC:
                    return await _HTTPX_ASYNC.get(url, headers=headers)
                return await asyncio.to_thread(requests.get, url, headers=headers, timeout=6)
            return r
        except Exception:
            if _HTTPX_ASYNC:
                return await _HTTPX_ASYNC.get(url, headers=headers)
            return await asyncio.to_thread(requests.get, url, headers=headers, timeout=6)
    if _HTTPX_ASYNC:
        return await _HTTPX_ASYNC.get(url, headers=headers)
    return await asyncio.to_thread(requests.get, url, headers=headers, timeout=20)


def post_with_optional_proxy(url: str, data: Dict[str, Any], headers: Dict[str, str]):
    # Default locale headers if not provided
    try:
        headers = dict(headers or {})
        headers.setdefault("Accept-Language", "pt-PT,pt;q=0.9,en;q=0.6")
        headers.setdefault("X-Forwarded-For", "185.23.160.1")
    except Exception:
        pass
    if SCRAPER_SERVICE.lower() == "scrapeops" and SCRAPER_API_KEY:
        try:
            params = {
                "api_key": SCRAPER_API_KEY,
                "url": url,
                "render_js": "true",
            }
            if SCRAPER_COUNTRY:
                params["country"] = SCRAPER_COUNTRY
            r = requests.post("https://proxy.scrapeops.io/v1/", params=params, headers=headers, data=data, timeout=30)
            if r.status_code in (401, 403):
                if _HTTPX_CLIENT:
                    return _HTTPX_CLIENT.post(url, headers=headers, data=data)
                return requests.post(url, headers=headers, data=data, timeout=20)
            return r
        except Exception:
            if _HTTPX_CLIENT:
                return _HTTPX_CLIENT.post(url, headers=headers, data=data)
            return requests.post(url, headers=headers, data=data, timeout=20)
    if _HTTPX_CLIENT:
        return _HTTPX_CLIENT.post(url, headers=headers, data=data)
    return requests.post(url, headers=headers, data=data, timeout=20)


@app.post("/api/bulk-prices")
async def bulk_prices(request: Request):
    require_auth(request)
    body = await request.json()
    locations: List[Dict[str, Any]] = body.get("locations", [])
    supplier_priority: Optional[str] = body.get("supplier_priority")
    durations = body.get("durations", [1,2,3,4,5,6,7,8,9,14,22,31,60])

    results: List[Dict[str, Any]] = []
    headers = {"User-Agent": "Mozilla/5.0 (compatible; PriceTracker/1.0)"}

    # Global simple rate limiter (shared across requests)
    _RL_LOCK = getattr(bulk_prices, "_RL_LOCK", None)
    if _RL_LOCK is None:
        _RL_LOCK = asyncio.Lock()
        setattr(bulk_prices, "_RL_LOCK", _RL_LOCK)
    _RL_LAST = getattr(bulk_prices, "_RL_LAST", 0.0)
    _RL_MIN_INTERVAL = 1.0 / GLOBAL_FETCH_RPS if GLOBAL_FETCH_RPS and GLOBAL_FETCH_RPS > 0 else 0.0

    async def _rate_limit_tick():
        nonlocal _RL_LAST
        if _RL_MIN_INTERVAL <= 0:
            return
        async with _RL_LOCK:
            now = time.time()
            wait = _RL_MIN_INTERVAL - (now - _RL_LAST)
            if wait > 0:
                await asyncio.sleep(wait)
                now = time.time()
            _RL_LAST = now
            setattr(bulk_prices, "_RL_LAST", _RL_LAST)

    async def _fetch_parse(url: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        # Retry up to 2 attempts for transient failures
        attempts = 0
        last_exc: Optional[Exception] = None
        while attempts < BULK_MAX_RETRIES:
            attempts += 1
            t0 = time.time()
            try:
                await _rate_limit_tick()
                r = await async_fetch_with_optional_proxy(url, headers=headers)
                r.raise_for_status()
                html = r.text
                t_fetch = int((time.time() - t0) * 1000)
                t1 = time.time()
                items = await asyncio.to_thread(parse_prices, html, url)
                items = convert_items_gbp_to_eur(items)
                items = apply_price_adjustments(items, url)
                items = normalize_and_sort(items, supplier_priority)
                t_parse = int((time.time() - t1) * 1000)
                # best-effort timing log
                try:
                    with open(DEBUG_DIR / "perf_bulk.txt", "a", encoding="utf-8") as _fp:
                        _fp.write(f"{time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())} fetch_ms={t_fetch} parse_ms={t_parse} attempts={attempts} url={url[:180]}\n")
                except Exception:
                    pass
                return items, {"fetch_ms": t_fetch, "parse_ms": t_parse, "attempts": attempts}
            except Exception as e:
                last_exc = e
                await asyncio.sleep(0.3 * attempts)
        raise last_exc  # type: ignore

    for loc in locations:
        name = loc.get("name", "")
        urls: List[str] = loc.get("urls", [])
        loc_block = {"location": name, "durations": []}
        # Cap concurrency to avoid overloading Render (CPU/net)
        sem = asyncio.Semaphore(BULK_CONCURRENCY)
        async def _worker(index: int, url: str, days: int):
            async with sem:
                try:
                    items, timing = await _fetch_parse(url)
                    return {"days": days, "count": len(items), "items": items, "timing": timing}
                except Exception as e:
                    return {"days": days, "error": str(e), "items": [], "timing": {"attempts": BULK_MAX_RETRIES}}

        tasks = []
        for idx, url in enumerate(urls):
            days = durations[idx] if idx < len(durations) else None
            if not url or days is None:
                continue
            tasks.append(_worker(idx, url, days))
        if tasks:
            loc_block["durations"] = await asyncio.gather(*tasks)
        results.append(loc_block)
    return JSONResponse({"ok": True, "results": results})


@app.post("/api/track-by-url")
async def track_by_url(request: Request):
    try:
        if not bool(str(os.getenv("DEV_NO_AUTH", "")).strip().lower() in ("1","true","yes","on")):
            require_auth(request)
    except Exception:
        require_auth(request)
    body = await request.json()
    location: str = body.get("location") or ""
    pickup_date: str = body.get("pickupDate") or ""
    pickup_time: str = body.get("pickupTime", "10:00")  # HH:mm
    days: Optional[int] = body.get("days")
    url: str = body.get("url") or ""
    no_cache: bool = bool(body.get("noCache", False))
    currency: str = body.get("currency", "")
    if not url:
        return _no_store_json({"ok": False, "error": "url is required"}, status_code=400)

    try:
        from datetime import datetime
        start_dt: Optional[datetime] = None
        if pickup_date:
            try:
                start_dt = datetime.fromisoformat(pickup_date + "T" + pickup_time)
            except Exception:
                start_dt = None
        # 0) 60s in-memory cache by normalized URL
        try:
            from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
            pr0 = urlparse(url)
            qd = dict(parse_qsl(pr0.query, keep_blank_values=True))
            # normalize currency/lang params position for stable key
            norm_q = urlencode(sorted(qd.items()))
            norm_url = urlunparse((pr0.scheme, pr0.netloc, pr0.path, pr0.params, norm_q, pr0.fragment))
        except Exception:
            norm_url = url
        now_ts = time.time()
        cached = _URL_CACHE.get(norm_url)
        if (not no_cache) and cached and (now_ts - cached[0] < 60):
            payload = dict(cached[1])
            # Avoid serving cached empty results
            if payload.get("items"):
                return _no_store_json(payload)
        headers = {
            # Desktop Chrome UA improves CarJet behavior on Render/mobile
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Accept-Language": "pt-PT,pt;q=0.9,en;q=0.8",
            "Referer": "https://www.carjet.com/do/list/pt",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Cache-Control": "no-cache",
            "sec-ch-ua": '"Chromium";v="123", "Not:A-Brand";v="8"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',
        }

        # Fast path when running locally or when FAST_MODE=true: single direct fetch, no retries
        try:
            IS_RENDER = bool(os.getenv("RENDER") or os.getenv("RENDER_EXTERNAL_URL"))
        except Exception:
            IS_RENDER = False
        FAST_MODE = bool(str(os.getenv("FAST_MODE", "")).strip().lower() in ("1","true","yes","on"))
        if (not IS_RENDER) or FAST_MODE:
            try:
                fast_headers = dict(headers)
                fast_headers["Cookie"] = "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt"
                r_fast = await asyncio.to_thread(requests.get, url, headers=fast_headers, timeout=(6,20))
                r_fast.raise_for_status()
                html_fast = r_fast.text
                items_fast = await asyncio.to_thread(parse_prices, html_fast, url)
                # If homepage-like or empty, quickly try /pt variant as a second shot
                homepage_like_fast = False
                try:
                    homepage_like_fast = ("Pesquisando em mais de 1000 locadoras" in html_fast) or (re.search(r"Pesquisando\s+em\s+mais\s+de\s+1000", html_fast) is not None)
                except Exception:
                    homepage_like_fast = False
                if (not items_fast) or homepage_like_fast:
                    try:
                        from urllib.parse import urlparse as _uparse, urlunparse as _uunparse
                        prx = _uparse(url)
                        if prx.path.startswith('/do/list/') and not prx.path.startswith('/do/list/pt'):
                            pt_url = _uunparse((prx.scheme, prx.netloc, '/do/list/pt', prx.params, prx.query, prx.fragment))
                            r_fast2 = await asyncio.to_thread(requests.get, pt_url, headers=fast_headers, timeout=(6,20))
                            r_fast2.raise_for_status()
                            html_fast2 = r_fast2.text
                            items_fast2 = await asyncio.to_thread(parse_prices, html_fast2, pt_url)
                            if items_fast2:
                                html_fast = html_fast2
                                items_fast = items_fast2
                    except Exception:
                        pass
                items_fast = normalize_and_sort(items_fast, supplier_priority=None)
                payload = {
                    "ok": True,
                    "items": items_fast,
                    "location": location or _detect_location_name(html_fast) or "",
                    "start_date": (start_dt.strftime("%Y-%m-%d") if start_dt else ""),
                    "days": days,
                    "last_updated": time.strftime('%Y-%m-%d %H:%M:%S'),
                }
                _URL_CACHE[norm_url] = (time.time(), dict(payload))
                return _no_store_json(payload)
            except Exception:
                pass
        # Overall time budget to avoid long waits on mobile/Render
        budget_ms = 7000
        total_t0 = time.time()
        def remaining_ms():
            try:
                return max(0, budget_ms - int((time.time() - total_t0) * 1000))
            except Exception:
                return 0

        # 1) Direct fetch for CarJet PT results URLs to preserve locale (avoid proxy geolocation flipping)
        html = ""
        items: List[Dict[str, Any]] = []
        try:
            from urllib.parse import urlparse, parse_qs
            pr = urlparse(url)
            qs = parse_qs(pr.query)
            is_carjet = pr.netloc.endswith("carjet.com")
            is_pt_results = pr.path.startswith("/do/list/pt") and ("s" in qs and "b" in qs)
            is_carjet_list = is_carjet and pr.path.startswith("/do/list/")
        except Exception:
            is_carjet = False
            is_pt_results = False
            is_carjet_list = False

        if USE_PLAYWRIGHT and _HAS_PLAYWRIGHT and is_carjet:
            try:
                items = scrape_with_playwright(url)
                if items:
                    html = "(playwright)"
            except Exception:
                items = []
                html = ""

        if (not items) and is_carjet and (is_pt_results or is_carjet_list) and remaining_ms() > 1200:
            # Race direct URL and a /pt-normalized variant in parallel; first success wins
            direct_headers = dict(headers)
            direct_headers["Cookie"] = "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt"
            direct_headers["sec-ch-ua"] = headers.get("sec-ch-ua")
            direct_headers["sec-ch-ua-mobile"] = headers.get("sec-ch-ua-mobile")
            direct_headers["sec-ch-ua-platform"] = headers.get("sec-ch-ua-platform")
            try:
                from urllib.parse import urlparse as _uparse, urlunparse as _uunparse
                prx = _uparse(url)
                pt_url = _uunparse((prx.scheme, prx.netloc, "/do/list/pt", prx.params, prx.query, prx.fragment)) if prx.path.startswith("/do/list/") and not prx.path.startswith("/do/list/pt") else url
            except Exception:
                pt_url = url

            async def fetch_and_parse(u: str):
                try:
                    t0 = time.time()
                    r = await async_fetch_with_optional_proxy(u, direct_headers)
                    r.raise_for_status()
                    h = r.text
                    its = await asyncio.to_thread(parse_prices, h, u)
                    hp = False
                    try:
                        hp = ("Pesquisando em mais de 1000 locadoras" in h) or (re.search(r"Pesquisando\s+em\s+mais\s+de\s+1000", h) is not None)
                    except Exception:
                        hp = False
                    dt = int((time.time() - t0) * 1000)
                    try:
                        print(f"[track_by_url] direct fetch {u} took {dt}ms items={(len(its) if its else 0)} homepage={hp}")
                    except Exception:
                        pass
                    if its and not hp:
                        return (u, h, its)
                except Exception:
                    return None
                return None

            tasks = [asyncio.create_task(fetch_and_parse(url)), asyncio.create_task(fetch_and_parse(pt_url))]
            # Respect remaining time budget for the parallel race
            timeout_sec = max(0.1, remaining_ms() / 1000.0)
            done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED, timeout=timeout_sec)
            winner = None
            for d in done:
                try:
                    winner = d.result()
                except Exception:
                    winner = None
            for p in pending:
                p.cancel()
            if winner:
                _, html, items = winner
            else:
                html = ""
                items = []

        # 1.a) If Playwright is enabled, try rendering the final UI to capture client-updated totals
        if (not items) and USE_PLAYWRIGHT and _HAS_PLAYWRIGHT and is_carjet:
            try:
                html_pw = render_with_playwright(url)
                if html_pw:
                    html = html_pw
                    items = parse_prices(html_pw, url)
            except Exception:
                pass

        # 1.b) If not a PT results URL, or direct failed, use normal path (with proxy if configured)
        if not html:
            resp = await async_fetch_with_optional_proxy(url, headers=headers)
            resp.raise_for_status()
            html = resp.text
            items = await asyncio.to_thread(parse_prices, html, url)
        # Determine if we only captured provider summaries (no car names) or wrong currency
        gbp_seen = any(("¬£" in (it.get("price") or "")) or re.search(r"\bGBP\b", (it.get("price") or ""), re.I) for it in (items or []))
        homepage_like = False
        try:
            if isinstance(html, str):
                homepage_like = ("Pesquisando em mais de 1000 locadoras" in html) or (re.search(r"Pesquisando\s+em\s+mais\s+de\s+1000", html) is not None)
        except Exception:
            homepage_like = False
        only_summaries = homepage_like or (not items) or all(not (it.get("car") or "").strip() for it in items)

        # If we have items but they are GBP, convert now; continue to final response
        if items and gbp_seen:
            items = convert_items_gbp_to_eur(items)
        # Apply env-driven adjustments for Carjet
        if items:
            items = apply_price_adjustments(items, url)

        # 1.5) If items are empty or GBP/only summaries, retry with EUR hints
        if (only_summaries or not items) and remaining_ms() > 1200:
            try:
                from urllib.parse import urlencode, urlparse, parse_qsl, urlunparse
                def _with_param(u: str, key: str, value: str) -> str:
                    pr = urlparse(u)
                    q = dict(parse_qsl(pr.query, keep_blank_values=True))
                    q[key] = value
                    new_q = urlencode(q)
                    return urlunparse((pr.scheme, pr.netloc, pr.path, pr.params, new_q, pr.fragment))

                # CarJet-specific normalization: force Portuguese path and EUR params
                url_norm = url
                try:
                    pr = urlparse(url)
                    if pr.netloc.endswith("carjet.com") and pr.path.startswith("/do/list/") and not pr.path.startswith("/do/list/pt"):
                        # keep query intact, only change locale path to /pt
                        url_norm = urlunparse((pr.scheme, pr.netloc, "/do/list/pt", pr.params, pr.query, pr.fragment))
                except Exception:
                    url_norm = url

                # Build robust set of variants including language and country
                base_eur = _with_param(url_norm, "moneda", "EUR")
                eur_variants = [
                    base_eur,
                    _with_param(base_eur, "currency", "EUR"),
                    _with_param(base_eur, "cur", "EUR"),
                    _with_param(base_eur, "idioma", "PT"),
                    _with_param(base_eur, "country", "PT"),
                ]
                # Limit retries to 2 variants to reduce latency
                eur_variants = eur_variants[:2]
                eur_headers = dict(headers)
                eur_headers["Cookie"] = "monedaForzada=EUR; moneda=EUR; currency=EUR; country=PT; idioma=PT; lang=pt"
                eur_headers["sec-ch-ua"] = headers.get("sec-ch-ua")
                eur_headers["sec-ch-ua-mobile"] = headers.get("sec-ch-ua-mobile")
                eur_headers["sec-ch-ua-platform"] = headers.get("sec-ch-ua-platform")
                retried_ok = False
                for u2 in eur_variants:
                    if remaining_ms() <= 1200:
                        break
                    try:
                        t1 = time.time()
                        r2 = await async_fetch_with_optional_proxy(u2, headers=eur_headers)
                        r2.raise_for_status()
                        html2 = r2.text
                        items2 = await asyncio.to_thread(parse_prices, html2, u2)
                        gbp2 = any((("¬£" in (it.get("price") or "")) or re.search(r"\bGBP\b", (it.get("price") or ""), re.I)) for it in (items2 or []))
                        dt2 = int((time.time() - t1) * 1000)
                        try:
                            print(f"[track_by_url] eur-variant fetch {u2} took {dt2}ms items={(len(items2) if items2 else 0)} gbp={gbp2}")
                        except Exception:
                            pass
                        if items2 and not gbp2:
                            html = html2
                            items = items2
                            only_summaries = False
                            break
                    except Exception:
                        continue
                    # If proxy is configured and still GBP/summary, attempt direct fetch without proxy
                    if only_summaries and (SCRAPER_SERVICE.lower() == "scrapeops" and SCRAPER_API_KEY) and remaining_ms() > 1800:
                        try:
                            r3 = requests.get(u2, headers=headers)
                            r3.raise_for_status()
                            html3 = r3.text
                            items3 = parse_prices(html3, u2)
                            gbp3 = any(("¬£" in (it.get("price") or "")) or re.search(r"\bGBP\b", (it.get("price") or ""), re.I) for it in (items3 or []))
                            if items3 and not gbp3:
                                html = html3
                                items = items3
                                only_summaries = False
                                break
                        except Exception:
                            pass
            except Exception:
                pass

        # 2) If still no detailed items, try Playwright to render the URL fully
        if only_summaries:
            try:
                from playwright.async_api import async_playwright
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context()
                    await context.set_extra_http_headers(headers)
                    page = await context.new_page()
                    page.set_default_navigation_timeout(15000)
                    page.set_default_timeout(12000)
                    await page.goto(url, wait_until="domcontentloaded")
                    # Force currency to EUR if possible
                    try:
                        # Try in-page function first
                        await page.evaluate("() => { try { if (typeof submit_monedaForzada === 'function') { submit_monedaForzada(window.location.href, 'EUR'); } } catch(e){} }")
                        # Also click any EUR currency switchers if present
                        eurBtn = page.locator("[data-currency='EUR'], .currency .eur").first
                        if await eurBtn.count() > 0:
                            await eurBtn.click()
                            await page.wait_for_timeout(500)
                    except Exception:
                        pass
                    # wait for network results or any price-like selector
                    try:
                        await page.wait_for_response(lambda r: ("/do/list" in r.url or "/carList.asp" in r.url) and r.status == 200, timeout=20000)
                    except Exception:
                        pass
                    try:
                        await page.wait_for_selector("section.newcarlist article, .newcarlist article, .price, .amount, [class*='price']", timeout=12000)
                    except Exception:
                        pass
                    # Try to reveal hidden cars if there is a 'Ver mais' button or function
                    try:
                        for i in range(4):
                            btn = page.locator("#linkMasCoches").first
                            if await btn.count() == 0:
                                break
                            await btn.click()
                            await page.wait_for_timeout(400)
                    except Exception:
                        pass
                    try:
                        for _ in range(3):
                            await page.evaluate("() => { try { if (typeof VerMasCoches === 'function') { VerMasCoches(); } } catch(e){} }")
                            await page.wait_for_timeout(300)
                    except Exception:
                        pass
                    # Scroll to bottom to trigger any lazy loading
                    try:
                        await page.evaluate("() => { window.scrollTo(0, document.body.scrollHeight); }")
                        await page.wait_for_timeout(600)
                    except Exception:
                        pass
                    html = await page.content()
                    await browser.close()
                items = parse_prices(html, url)
            except Exception:
                items = parse_prices(html, url)
        items = normalize_and_sort(items, supplier_priority=None)
        try:
            total_dt = int((time.time() - total_t0) * 1000)
            print(f"[track_by_url] total={total_dt}ms items={(len(items) if items else 0)}")
        except Exception:
            pass
        # Try to detect dates and days from page HTML if not provided
        try:
            soup = BeautifulSoup(html, "lxml")
            txt = html
            # dataLayer hrental_startdate/hrental_enddate (YYYY-MM-DD)
            m1 = re.search(r'"hrental_startdate"\s*:\s*"(\d{4}-\d{2}-\d{2})"', txt)
            m2 = re.search(r'"hrental_enddate"\s*:\s*"(\d{4}-\d{2}-\d{2})"', txt)
            if m1:
                try:
                    start_dt = datetime.fromisoformat(m1.group(1) + "T" + (pickup_time or "10:00"))
                except Exception:
                    pass
            if m2 and m1:
                try:
                    end_dt = datetime.fromisoformat(m2.group(1) + "T" + (pickup_time or "10:00"))
                    days = (end_dt - start_dt).days if start_dt else days
                except Exception:
                    pass
            # DiasReserva in dataLayer
            if days is None:
                md = re.search(r'"DiasReserva"\s*:\s*"?(\d{1,2})"?', txt)
                if md:
                    try:
                        days = int(md.group(1))
                    except Exception:
                        pass
            # Detect location from dataLayer Destino or hidden inputs
            try:
                ml = re.search(r'"Destino"\s*:\s*"([^"]+)"', txt)
                if ml:
                    location = ml.group(1)
            except Exception:
                pass
            # Hidden inputs frmFechaRecogida / frmFechaDevolucion dd/mm/yyyy HH:MM (various id/name variants)
            if (start_dt is None) or (days is None):
                fr = soup.select_one("#frmFechaRecogida, input[name='frmFechaRecogida'], input[name='fechaRecogida']")
                fd = soup.select_one("#frmFechaDevolucion, input[name='frmFechaDevolucion'], input[name='fechaEntrega']")
                from datetime import datetime as _dt
                def _parse_dmY_HM(v: str) -> Optional[datetime]:
                    try:
                        return _dt.strptime(v, "%d/%m/%Y %H:%M")
                    except Exception:
                        return None
                if fr and fr.has_attr("value"):
                    t = fr.get("value") or ""
                    maybe = _parse_dmY_HM(t)
                    if maybe:
                        start_dt = maybe
                if fd and fd.has_attr("value"):
                    t = fd.get("value") or ""
                    maybe_end = _parse_dmY_HM(t)
                    if (maybe_end and start_dt) and (days is None):
                        days = (maybe_end - start_dt).days
                fdst = soup.select_one("#frmDestino, input[name='frmDestino'], input[name='destino']")
                if fdst and fdst.has_attr("value"):
                    val = (fdst.get("value") or "").strip()
                    if val:
                        location = val
        except Exception:
            pass
        # persist snapshot so UI can show the rows immediately
        try:
            if start_dt and days:
                save_snapshots(location, start_dt, int(days), items, currency or "")
        except Exception:
            pass
        from datetime import datetime as _dt
        payload = {
            "ok": True,
            "items": items,
            "location": location,
            "start_date": pickup_date,
            "days": days,
            "last_updated": _dt.utcnow().isoformat(timespec="seconds") + "Z",
        }
        # store in cache only if we have items
        try:
            if items:
                _URL_CACHE[norm_url] = (time.time(), payload)
        except Exception:
            pass
        # If still empty, write a small debug note (non-fatal)
        try:
            if not items:
                from html import unescape as _unesc
                title_match = re.search(r"<title>(.*?)</title>", html or "", re.I|re.S)
                ttl = _unesc(title_match.group(1)).strip() if title_match else ""
                (DEBUG_DIR / "last_empty.txt").write_text(
                    f"{time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())} | URL={url} | parsed_items=0 | title={ttl[:120]} | html_len={len(html or '')}\n",
                    encoding="utf-8"
                )
        except Exception:
            pass
        return JSONResponse(payload)

    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


def _get_vehicle_groups() -> Dict[str, Dict[str, Any]]:
    """Carrega caracter√≠sticas completas dos grupos do Admin Vehicles"""
    groups = {}
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    cursor = conn.cursor()
                    cursor.execute("SELECT code, brand, model, category, transmission, photo_url FROM car_groups WHERE enabled = 1")
                    for row in cursor.fetchall():
                        if row[0]:
                            groups[row[0]] = {'code': row[0], 'brand': row[1] or '', 'model': row[2] or '', 
                                            'category': row[3] or '', 'transmission': row[4] or '', 'photo_url': row[5] or ''}
                else:
                    cursor = conn.execute("SELECT code, brand, model, category, transmission, photo_url FROM car_groups WHERE enabled = 1")
                    for row in cursor.fetchall():
                        if row[0]:
                            groups[row[0]] = {'code': row[0], 'brand': row[1] or '', 'model': row[2] or '', 
                                            'category': row[3] or '', 'transmission': row[4] or '', 'photo_url': row[5] or ''}
            finally:
                conn.close()
    except Exception as e:
        import sys
        print(f"[WARNING] Failed to load vehicle groups: {e}", file=sys.stderr, flush=True)
    return groups

def normalize_and_sort(items: List[Dict[str, Any]], supplier_priority: Optional[str]) -> List[Dict[str, Any]]:
    # Secondary guard: blocklist filter to ensure unwanted vehicles never appear
    _blocked_models = [
        "Mercedes S Class Auto",
        "MG ZS Auto",
        "Mercedes CLA Coupe Auto",
        "Mercedes A Class",
        "Mercedes A Class Auto",
        "BMW 1 Series Auto",
        "BMW 3 Series SW Auto",
        "Volvo V60 Auto",
        "Volvo XC40 Auto",
        "Mercedes C Class Auto",
        "Tesla Model 3 Auto",
        "Electric",
        "BMW 2 Series Gran Coupe Auto",
        "Mercedes C Class SW Auto",
        "Mercedes E Class Auto",
        "Mercedes E Class SW Auto",
        "BMW 5 Series SW Auto",
        "BMW X1 Auto",
        "Mercedes CLE Coupe Auto",
        "Volkswagen T-Roc Cabrio",
        "Mercedes GLA Auto",
        "Volvo XC60 Auto",
        "Volvo EX30 Auto",
        "BMW 3 Series Auto",
        "Volvo V60 4x4 Auto",
        "Hybrid",
        "Mazda MX5 Cabrio Auto",
        "Mercedes CLA Auto",
    ]
    def _norm_text(s: str) -> str:
        s = (s or "").strip().lower()
        return " ".join(s.replace(",", " ").split())
    _blocked_norm = set(_norm_text(x) for x in _blocked_models)
    import re as _re
    _patterns = [
        r"\bmercedes\s+s\s*class\b",
        r"\bmercedes\s+cla\b",
        r"\bmercedes\s+cle\b",
        r"\bmercedes\s+a\s*class\b",
        r"\bmercedes\s+c\s*class\b",
        r"\bmercedes\s+e\s*class\b",
        r"\bmercedes\s+gla\b",
        r"\bbmw\s+1\s*series\b",
        r"\bbmw\s+2\s*series\b",
        r"\bbmw\s+3\s*series\b",
        r"\bbmw\s+5\s*series\b",
        r"\bbmw\s*x1\b",
        r"\bvolvo\s+v60\b",
        r"\bvolvo\s+xc40\b",
        r"\bvolvo\s+xc60\b",
        r"\bvolvo\s+ex30\b",
        r"\btesla\s+model\s*3\b",
        r"\bmg\s+zs\b",
        r"\bmazda\s+mx5\b",
        r"\bvolkswagen\s+t-roc\b",
        r"\belectric\b",
        r"\bhybrid\b",
    ]
    def _blocked(name: str) -> bool:
        n = _norm_text(name)
        if not n:
            return False
        if n in _blocked_norm:
            return True
        for p in _patterns:
            if _re.search(p, n):
                return True
        for b in _blocked_norm:
            if len(b) >= 6 and b in n:
                return True
        return False

    detailed: List[Dict[str, Any]] = []
    summary: List[Dict[str, Any]] = []
    import re as _re2
    
    # Helper: Buscar foto de ve√≠culo por nome (vehicle_photos)
    def _get_vehicle_photo_by_name(car_name: str) -> str:
        """Busca foto na tabela vehicle_photos pelo nome do carro"""
        if not car_name:
            return ""
        vehicle_key = car_name.lower().strip()
        try:
            with _db_lock:
                conn = _db_connect()
                try:
                    if conn.__class__.__module__ == 'psycopg2.extensions':
                        # PostgreSQL
                        with conn.cursor() as cur:
                            cur.execute("SELECT photo_url FROM vehicle_photos WHERE vehicle_name = %s", (vehicle_key,))
                            row = cur.fetchone()
                            if row and row[0]:
                                return row[0]
                    else:
                        # SQLite
                        row = conn.execute("SELECT photo_url FROM vehicle_photos WHERE vehicle_name = ?", (vehicle_key,)).fetchone()
                        if row and row[0]:
                            return row[0]
                finally:
                    conn.close()
        except Exception:
            pass
        return ""
    
    # Helper: Buscar nome editado (vehicle_name_overrides)
    def _get_edited_name(original_name: str) -> str:
        """Busca nome editado na tabela vehicle_name_overrides"""
        if not original_name:
            return original_name
        try:
            with _db_lock:
                conn = _db_connect()
                try:
                    row = conn.execute(
                        "SELECT edited_name FROM vehicle_name_overrides WHERE LOWER(original_name) = ?",
                        (original_name.lower().strip(),)
                    ).fetchone()
                    if row and row[0]:
                        return row[0]
                finally:
                    conn.close()
        except Exception:
            pass
        return original_name
    
    # Carregar caracter√≠sticas do Admin Vehicles
    vehicle_groups = _get_vehicle_groups()
    import sys
    print(f"[VEHICLES] Loaded {len(vehicle_groups)} vehicle groups from Admin", file=sys.stderr, flush=True)
    
    # Create group_photos dict from vehicle_groups
    group_photos = {g['code']: g.get('photo_url', '') for g in vehicle_groups if g.get('photo_url')}
    
    # Use dynamic FX with 1h cache; fallback 1.16
    try:
        GBP_TO_EUR = float(_fx_rate_gbp_eur())
    except Exception:
        GBP_TO_EUR = 1.16
    for it in items:
        # DISABLED: mostrar todos os carros
        # if _blocked(it.get("car", "")):
        #     continue
        price_text_in = it.get("price", "") or ""
        price_num = extract_price_number(price_text_in)
        price_curr = ""
        if "‚Ç¨" in price_text_in or _re2.search(r"\bEUR\b", price_text_in, _re2.I):
            price_curr = "EUR"
        elif "¬£" in price_text_in or _re2.search(r"\bGBP\b", price_text_in, _re2.I):
            price_curr = "GBP"
        # Convert GBP -> EUR for display and sorting
        if price_curr == "GBP" and price_num is not None:
            try:
                price_num = round(price_num * GBP_TO_EUR, 2)
                price_text_in = f"‚Ç¨{price_num:.2f}"
                price_curr = "EUR"
            except Exception:
                pass
        # Limpar nome do carro PRIMEIRO (remover "Autoautom√°tico", "ou similar", etc)
        car_name_clean = clean_car_name(it.get("car", ""))
        
        # APLICAR NOME EDITADO (vehicle_name_overrides) - PRIORIDADE M√ÅXIMA
        car_name_final = _get_edited_name(car_name_clean)
        if car_name_final != car_name_clean:
            if len(detailed) == 0 and len(summary) == 0:
                import sys
                print(f"[NAMES] ‚úÖ Using edited name: '{car_name_clean}' ‚Üí '{car_name_final}'", file=sys.stderr, flush=True)
        
        # Se n√£o tiver grupo definido, usar Admin Vehicles (caracter√≠sticas)
        # IMPORTANTE: match baseado em categoria, transmiss√£o, brand/model
        group_code = it.get("group", "")
        if not group_code:
            from match_helper import match_vehicle_group_by_characteristics
            group_code = match_vehicle_group_by_characteristics(
                it.get("category", ""),
                car_name_final,  # Usar nome editado para matching
                it.get("transmission", ""),
                vehicle_groups,
                map_category_to_group
            )
        
        # DEBUG: Log primeiro item
        if len(detailed) == 0 and len(summary) == 0:
            import sys
            print(f"[DEBUG] Primeiro item: cat={it.get('category')}, car_final={car_name_final}, group={group_code}", file=sys.stderr, flush=True)
        
        # NOVA PRIORIDADE DE FOTOS:
        # 1. vehicle_photos (por nome de carro) - CONTROLADO PELO USER
        # 2. car_groups.photo_url (por c√≥digo de grupo) - ADMIN
        # 3. Scraping CarJet (fallback)
        photo_url = ""
        
        # 1¬∫: Tentar buscar foto por nome de carro (vehicle_photos)
        photo_url = _get_vehicle_photo_by_name(car_name_final)
        if photo_url:
            if len(detailed) == 0 and len(summary) == 0:
                import sys
                print(f"[PHOTOS] ‚úÖ Using vehicle_photos for '{car_name_final}': {photo_url[:80]}", file=sys.stderr, flush=True)
        else:
            # 2¬∫: Tentar buscar foto por c√≥digo de grupo (car_groups)
            if group_code and group_code in group_photos:
                photo_url = group_photos[group_code]
                if len(detailed) == 0 and len(summary) == 0:
                    import sys
                    print(f"[PHOTOS] ‚öôÔ∏è Using car_groups photo for group {group_code}: {photo_url[:80]}", file=sys.stderr, flush=True)
            else:
                # 3¬∫: Fallback para foto do scraping
                photo_url = it.get("photo", "")
                if len(detailed) == 0 and len(summary) == 0 and photo_url:
                    import sys
                    print(f"[PHOTOS] ‚ö†Ô∏è Fallback to scraped photo: {photo_url[:80]}", file=sys.stderr, flush=True)
        
        # Use car_groups category if group is known, otherwise use scraped category
        category_display = it.get("category", "")
        if group_code and group_code in vehicle_groups:
            category_display = vehicle_groups[group_code].get('category', category_display)
        
        row = {
            "supplier": it.get("supplier", ""),
            "car": car_name_final,  # Usar nome editado final
            "price": price_text_in,
            "price_num": price_num,
            "currency": price_curr or it.get("currency", ""),
            "category": category_display,
            "group": group_code,
            "category_code": it.get("category_code", ""),
            "transmission": it.get("transmission", ""),
            "photo": photo_url,
            "link": it.get("link", ""),
        }
        
        # DEBUG: Verificar se group est√° no row antes de append
        if len(detailed) == 0 and len(summary) == 0:
            import sys
            print(f"[DEBUG] row tem 'group'? {'group' in row}, valor={row.get('group')}", file=sys.stderr, flush=True)
        
        if (row["car"] or "").strip():
            detailed.append(row)
        else:
            summary.append(row)

    def _sort(lst: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        lst.sort(key=lambda x: (
            0 if supplier_priority and supplier_priority.lower() in (x.get("supplier") or "").lower() else 1,
            (x.get("category") or ""),
            x.get("price_num") or 1e15,
        ))
        return lst

    # Prefer detailed per-car rows; if none found, fall back to provider-summary rows
    if detailed:
        return _sort(detailed)
    return _sort(summary)


def extract_price_number(price_str: str) -> Optional[float]:
    """
    Extract price number from string, handling European format (1.234,56) and US format (1,234.56)
    """
    if not price_str:
        return None
    
    # Use _parse_amount which already handles thousands separators correctly
    return _parse_amount(price_str)


@app.post("/api/track-carjet")
async def track_carjet(request: Request):
    require_auth(request)
    body = await request.json()
    pickup_date: str = body.get("pickupDate")  # YYYY-MM-DD
    pickup_time: str = body.get("pickupTime", "10:00")  # HH:mm
    durations: List[int] = body.get("durations", [1,2,3,4,5,6,7,8,9,14,22,31,60])
    locations: List[Dict[str, Any]] = body.get("locations", [])  # [{name, template?}]
    supplier_priority: Optional[str] = body.get("supplier_priority")
    lang: str = body.get("lang", "en")
    currency: str = body.get("currency", "EUR")

    if not pickup_date or not locations:
        return JSONResponse({"ok": False, "error": "pickupDate and locations are required"}, status_code=400)

    try:
        from datetime import datetime, timedelta
        from playwright.async_api import async_playwright
        # random j√° importado globalmente
        
        # Load date rotation settings from database
        date_rotation_enabled = True  # Enabled by default
        date_rotation_max_days = 4
        
        with _db_lock:
            con = _db_connect()
            try:
                row = con.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = ?",
                    ("date_rotation_enabled",)
                ).fetchone()
                if row:
                    date_rotation_enabled = row[0].lower() in ('true', '1', 'yes')
                
                row = con.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = ?",
                    ("date_rotation_max_days",)
                ).fetchone()
                if row:
                    date_rotation_max_days = int(row[0])
            finally:
                con.close()

        async def run():
            results: List[Dict[str, Any]] = []
            
            # Referrer options for natural traffic simulation
            referrers = [
                "https://www.google.com/",
                "https://www.google.pt/",
                "https://www.bing.com/",
                "https://www.google.com/search?q=rent+car+portugal",
                None  # Direct access
            ]
            
            # Timezone options for location diversity
            timezones = [
                "Europe/Lisbon",
                "Europe/London", 
                "Europe/Madrid",
                "Europe/Paris"
            ]
            
            # Device rotation for better WAF evasion
            devices = [
                {
                    "name": "iPhone 13 Pro",
                    "user_agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
                    "viewport": {"width": 390, "height": 844},
                    "scale": 3
                },
                {
                    "name": "iPhone 14",
                    "user_agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
                    "viewport": {"width": 390, "height": 844},
                    "scale": 3
                },
                {
                    "name": "Samsung Galaxy S21",
                    "user_agent": "Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
                    "viewport": {"width": 360, "height": 800},
                    "scale": 3
                },
                {
                    "name": "iPad Air",
                    "user_agent": "Mozilla/5.0 (iPad; CPU OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
                    "viewport": {"width": 820, "height": 1180},
                    "scale": 2
                }
            ]
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                
                for loc_index, loc in enumerate(locations):
                    name = loc.get("name", "")
                    template = loc.get("template", "")
                    
                    # Select random device for this location
                    device = random.choice(devices)
                    print(f"[DEVICE_ROTATION] Location: {name}, Device: {device['name']}")
                    
                    # Select random timezone
                    timezone = random.choice(timezones)
                    print(f"[TIMEZONE_ROTATION] Location: {name}, Timezone: {timezone}")
                    
                    # Select random referrer
                    referrer = random.choice(referrers)
                    referrer_display = referrer if referrer else "Direct"
                    print(f"[REFERRER_ROTATION] Location: {name}, Referrer: {referrer_display}")
                    
                    # Create new context with random device (clears cache/history)
                    context = await browser.new_context(
                        user_agent=device["user_agent"],
                        viewport=device["viewport"],
                        device_scale_factor=device["scale"],
                        is_mobile=True,
                        has_touch=True,
                        locale="pt-PT",
                        timezone_id=timezone
                    )
                    
                    # Mobile headers with random referrer
                    default_headers = {
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                        "Accept-Language": "pt-PT,pt;q=0.9,en;q=0.8",
                        "Accept-Encoding": "gzip, deflate, br",
                    }
                    if referrer:
                        default_headers["Referer"] = referrer
                    await context.set_extra_http_headers(default_headers)
                    await context.route("**/*", lambda route: route.continue_())
                    page = await context.new_page()
                    page.set_default_navigation_timeout(10000)
                    page.set_default_timeout(8000)
                    
                    # Apply date rotation per location for more variation
                    rotated_pickup_date = pickup_date
                    if date_rotation_enabled and date_rotation_max_days > 0:
                        base_date = datetime.strptime(pickup_date, "%Y-%m-%d")
                        days_offset = random.randint(0, date_rotation_max_days)
                        rotated_date = base_date + timedelta(days=days_offset)
                        rotated_pickup_date = rotated_date.strftime("%Y-%m-%d")
                        print(f"[DATE_ROTATION] Location: {name}, Original: {pickup_date}, Rotated: {rotated_pickup_date} (+{days_offset} days)")
                    else:
                        print(f"[DATE_ROTATION] Location: {name}, Disabled, using original date: {pickup_date}")
                    
                    # Randomize pickup time between 14:30 and 17:00
                    hour = random.randint(14, 16)
                    if hour == 14:
                        minute = random.choice([30, 45])
                    elif hour == 16:
                        minute = random.choice([0, 15, 30, 45])
                    else:  # hour == 15
                        minute = random.choice([0, 15, 30, 45])
                    
                    # If hour is 16 and minute > 0, cap at 17:00
                    if hour == 16 and minute > 0:
                        if random.random() < 0.5:
                            hour = 17
                            minute = 0
                    
                    rotated_pickup_time = f"{hour:02d}:{minute:02d}"
                    print(f"[TIME_ROTATION] Location: {name}, Original: {pickup_time}, Rotated: {rotated_pickup_time}")
                    
                    loc_block = {"location": name, "durations": []}
                    for d in durations:
                        try:
                            # Random delay between duration searches (0.5-2 seconds)
                            delay = random.uniform(0.5, 2.0)
                            print(f"[DELAY] Waiting {delay:.2f}s before next search...")
                            await asyncio.sleep(delay)
                            
                            start_dt = datetime.fromisoformat(rotated_pickup_date + "T" + rotated_pickup_time)
                            end_dt = start_dt + timedelta(days=int(d))
                            # Try direct POST to CarJet first (faster, no headless)
                            html = try_direct_carjet(name, start_dt, end_dt, lang=lang, currency=currency)
                            final_url = "https://www.carjet.com/do/list"
                            # Fallback to Playwright if direct returned empty or no prices
                            if not html or len(parse_prices(html, final_url)) == 0:
                                html, final_url = await fetch_carjet_results(page, name, start_dt, end_dt, lang, currency, template)
                                
                                # Simulate human scroll behavior after page load
                                try:
                                    scroll_amount = random.randint(200, 500)
                                    await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                                    await asyncio.sleep(random.uniform(0.3, 0.8))
                                    print(f"[SCROLL] Scrolled {scroll_amount}px")
                                except:
                                    pass  # Ignore scroll errors
                                    
                            items = parse_prices(html, final_url)
                            items = normalize_and_sort(items, supplier_priority)
                            save_snapshots(name, start_dt, d, items, currency)
                            
                            # Save to search history
                            try:
                                prices = [float(item.get('price_num', 0)) for item in items if item.get('price_num')]
                                min_price = min(prices) if prices else None
                                max_price = max(prices) if prices else None
                                avg_price = sum(prices) / len(prices) if prices else None
                                
                                save_search_to_history(
                                    location=name,
                                    start_date=start_dt.strftime("%Y-%m-%d"),
                                    end_date=end_dt.strftime("%Y-%m-%d"),
                                    days=d,
                                    results_count=len(items),
                                    min_price=min_price,
                                    max_price=max_price,
                                    avg_price=avg_price,
                                    user=request.session.get('username', 'admin'),
                                    search_params=f"lang={lang}, currency={currency}, time={rotated_pickup_time}"
                                )
                            except Exception as hist_err:
                                logging.warning(f"Failed to save search history: {hist_err}")
                            
                            loc_block["durations"].append({
                                "days": d,
                                "count": len(items),
                                "items": items,
                            })
                        except Exception as e:
                            loc_block["durations"].append({
                                "days": d,
                                "error": str(e),
                                "items": [],
                            })
                    results.append(loc_block)
                    
                    # Close context to clear cache/cookies/history before next location
                    await page.close()
                    await context.close()
                    print(f"[CACHE_CLEAR] Location: {name}, Context closed - cache/history cleared")
                    
                    # Random delay between locations (2-5 seconds) - simulate human behavior
                    if loc_index < len(locations) - 1:  # Not the last location
                        location_delay = random.uniform(2.0, 5.0)
                        print(f"[LOCATION_DELAY] Waiting {location_delay:.2f}s before next location...")
                        await asyncio.sleep(location_delay)
                    
                await browser.close()
            return results

        results = await run()
        return JSONResponse({"ok": True, "results": results})
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


async def fetch_carjet_results(page, location_name, start_dt, end_dt, lang: str, currency: str, template: str):
    try:
        captured_html: Optional[str] = None
        captured_url: Optional[str] = None
        captured_post: Optional[Dict[str, Any]] = None

        async def on_response(resp):
            nonlocal captured_html, captured_url
            try:
                url = resp.url
                if ("/do/list" in url or "/carList.asp" in url) and resp.status == 200 and captured_html is None:
                    text = await resp.text()
                    if text:
                        captured_html = text
                        captured_url = url
            except Exception:
                pass

        # register response listener (use asyncio.create_task for awaiting inside handler)
        page.on("response", lambda r: asyncio.create_task(on_response(r)))
        # capture the first POST payload to /do/list to replay if needed
        def _on_request(req):
            nonlocal captured_post
            try:
                if ("/do/list" in req.url or "/carList.asp" in req.url) and req.method == "POST" and captured_post is None:
                    captured_post = {"url": req.url, "post": req.post_data or ""}
            except Exception:
                pass
        page.on("request", _on_request)
        if template:
            url = (
                template
                .replace("{pickup_date}", start_dt.strftime("%Y-%m-%d"))
                .replace("{pickup_time}", start_dt.strftime("%H:%M"))
                .replace("{dropoff_date}", end_dt.strftime("%Y-%m-%d"))
                .replace("{dropoff_time}", end_dt.strftime("%H:%M"))
                .replace("{location}", location_name)
            )
            await page.goto(url, wait_until="domcontentloaded")
        else:
            # Prefer PT site always for consistency with parsing/selectors
            lang = (lang or "pt").lower()
            if lang not in ("pt", "en", "es", "fr", "de", "it", "nl"):
                lang = "pt"
            base = f"https://www.carjet.com/{lang}/"
            await page.goto(base, wait_until="domcontentloaded")
            try:
                await page.wait_for_timeout(700)
                # Try to accept cookie banner if present
                try:
                    cookies_btn = page.get_by_role("button", name=re.compile("accept|agree|aceitar|ok", re.I)).first
                    if await cookies_btn.count() > 0:
                        await cookies_btn.click()
                        await page.wait_for_timeout(300)
                except Exception:
                    pass
                # Ensure PT language is active to set correct cookies/session
                try:
                    # If not already on /pt/ path, click Portuguese link
                    if not re.search(r"/pt/", page.url):
                        lang_link = page.locator("a[hreflang='pt']").first
                        if await lang_link.count() == 0:
                            lang_link = page.get_by_role("link", name=re.compile("Portugu[e√™]s", re.I)).first
                        if await lang_link.count() == 0:
                            lang_link = page.locator("a[href*='/pt/']").first
                        if await lang_link.count() > 0:
                            await lang_link.click()
                            try:
                                await page.wait_for_url(re.compile(r"/pt/"), timeout=4000)
                            except Exception:
                                pass
                            await page.wait_for_timeout(400)
                except Exception:
                    pass
                # Try location input
                loc_input = page.get_by_placeholder("Pick-up location")
                if await loc_input.count() == 0:
                    loc_input = page.locator("input[name*='pickup']")
                if await loc_input.count() == 0:
                    loc_input = page.locator("#pickup")
                await loc_input.click()
                await loc_input.fill(location_name)
                await page.wait_for_timeout(900)
                # Prefer clicking the first autocomplete option if available
                try:
                    # CarJet PT uses #recogida_lista li for suggestions
                    first_opt = page.locator("#recogida_lista li").first
                    if await first_opt.count() == 0:
                        first_opt = page.locator("[role='listbox'] [role='option']").first
                    if await first_opt.count() > 0:
                        await first_opt.click()
                        # extract data attributes from option and populate hidden fields if present
                        try:
                            data = await first_opt.evaluate("(el)=>{const d=el.dataset||{};return {id:d.id||d.dstId||d.zoneId||'', zone:d.zone||d.zoneCode||''};}")
                            await page.evaluate("(vals)=>{const set=(id,val)=>{const el=document.getElementById(id); if(el){ el.value=val; }}; set('dst_id', vals.id); set('zoneCode', vals.zone); set('pickupId', vals.id); }", data)
                        except Exception:
                            pass
                    else:
                        await page.keyboard.press("Enter")
                except Exception:
                    await page.keyboard.press("Enter")
            except Exception:
                pass
            # Force known internal codes for target locations if inputs exist
            try:
                code_map = {
                    # Albufeira: ABF01 n√£o funciona - deixar CarJet descobrir
                    # "Albufeira": "ABF01",
                    # "Albufeira Cidade": "ABF01",
                    "Faro Airport": "FAO02",
                    "Faro Aeroporto": "FAO02",
                }
                dst_code = code_map.get(location_name, "")
                if dst_code:
                    await page.evaluate(
                        "(dst)=>{\n"
                        "  const setVal=(id,val)=>{const el=document.getElementById(id); if(el){ el.value=val; el.dispatchEvent(new Event('change',{bubbles:true})); }};\n"
                        "  setVal('pickupId', dst); setVal('dst_id', dst); setVal('zoneCode', dst);\n"
                        "}",
                        dst_code,
                    )
            except Exception:
                pass
            # Pickup date
            try:
                # Try to set date inputs directly if present (various IDs)
                pickup_str_dmY = start_dt.strftime("%d/%m/%Y")
                dropoff_str_dmY = end_dt.strftime("%d/%m/%Y")
                await page.evaluate("(ids, val) => { for (const id of ids){ const el=document.getElementById(id) || document.querySelector('[name='+id+']'); if(el){ el.removeAttribute && el.removeAttribute('readonly'); el.value=val; el.dispatchEvent && el.dispatchEvent(new Event('input',{bubbles:true})); el.dispatchEvent && el.dispatchEvent(new Event('change',{bubbles:true})); } } }", ["fechaRecogida","pickupDate","date_from"], pickup_str_dmY)
            except Exception:
                pass
            try:
                await page.evaluate("(ids, val) => { for (const id of ids){ const el=document.getElementById(id) || document.querySelector('[name='+id+']'); if(el){ el.removeAttribute && el.removeAttribute('readonly'); el.value=val; el.dispatchEvent && el.dispatchEvent(new Event('input',{bubbles:true})); el.dispatchEvent && el.dispatchEvent(new Event('change',{bubbles:true})); } } }", ["fechaEntrega","fechaDevolucion","dropoffDate","date_to"], dropoff_str_dmY)
            except Exception:
                pass
            # Pickup/Dropoff time
            try:
                await page.evaluate("(ids, val) => { for (const id of ids){ const el=document.getElementById(id) || document.querySelector('[name='+id+']'); if(el){ el.value=val; el.dispatchEvent && el.dispatchEvent(new Event('change',{bubbles:true})); } } }", ["fechaRecogidaSelHour","h-recogida","pickupTime","time_from"], start_dt.strftime("%H:%M"))
            except Exception:
                pass
            try:
                await page.evaluate("(ids, val) => { for (const id of ids){ const el=document.getElementById(id) || document.querySelector('[name='+id+']'); if(el){ el.value=val; el.dispatchEvent && el.dispatchEvent(new Event('change',{bubbles:true})); } } }", ["fechaEntregaSelHour","h-devolucion","dropoffTime","time_to"], end_dt.strftime("%H:%M"))
            except Exception:
                pass
            # Submit search
            try:
                # Prefer submitting the main search form if present
                form = page.locator("form[name='menu_tarifas']")
                if await form.count() > 0:
                    # some sites rely on JS; try clicking the booking form button
                    btn = page.locator("#booking_form .btn-search").first
                    if await btn.count() > 0:
                        await btn.click()
                    else:
                        # fallback to form submit button
                        btn = form.locator("button[type='submit'], input[type='submit']").first
                        if await btn.count() > 0:
                            await btn.click()
                        else:
                            await page.evaluate("sel=>{const f=document.querySelector(sel); f && f.submit();}", "form[name='menu_tarifas']")
                else:
                    # If the page defines submit_fechas(action) use it to ensure s/b tokens are added
                    used_native = await page.evaluate("() => { try { if (typeof submit_fechas === 'function') { submit_fechas('/do/list/pt'); return true; } } catch(e){} return false; }")
                    if not used_native:
                        btn = page.get_by_role("button", name=re.compile("search|continue|find|atualizar|update", re.I))
                        if await btn.count() == 0:
                            btn = page.locator("button[type='submit']")
                        await btn.click()
            except Exception:
                pass
            # As a final nudge, try native submit one more time
            try:
                await page.evaluate("() => { try { if (typeof submit_fechas === 'function') { submit_fechas('/do/list/pt'); } } catch(e){} }")
            except Exception:
                pass
            # If still no request fired, serialize and submit the form directly
            try:
                await page.evaluate("""
                () => {
                  const form = document.querySelector("form[name='menu_tarifas']") || document.querySelector("#booking_form");
                  if (form) {
                    try { form.dispatchEvent(new Event('submit', {bubbles:true,cancelable:true})); } catch(e){}
                    try { form.submit(); } catch(e){}
                  }
                }
                """)
            except Exception:
                pass
            # Wait for results list
            try:
                # Prefer waiting for the actual network response and capture its body
                resp = await page.wait_for_response(lambda r: ("/do/list" in r.url or "/carList.asp" in r.url) and r.status == 200, timeout=25000)
                try:
                    body = await resp.text()
                    if body:
                        captured_html = body
                        captured_url = resp.url
                except Exception:
                    pass
                # Also ensure URL change if applicable
                await page.wait_for_url(re.compile(r"(/do/list|/carList\.asp)"), timeout=5000)
            except Exception:
                pass
            # Wait for any price-like selector quickly
            try:
                await page.wait_for_selector(".price, .amount, [class*='price']", timeout=15000)
            except Exception:
                pass
        # Prefer captured network HTML if present
        if captured_html:
            html = captured_html
            current_url = captured_url or page.url
        else:
            # If we saw the POST payload but didn't capture body, fetch via in-page fetch using same cookies/session
            if captured_post and captured_post.get("url"):
                try:
                    js = """
                    async (u, body) => {
                      const resp = await fetch(u, { method: 'POST', headers: { 'Content-Type': 'application/x-www-form-urlencoded' }, body: body });
                      return await resp.text();
                    }
                    """
                    html = await page.evaluate(js, captured_post["url"], captured_post.get("post", ""))
                    current_url = captured_post["url"]
                except Exception:
                    html = await page.content()
                    current_url = page.url
            else:
                html = await page.content()
                current_url = page.url
        return html, current_url
    finally:
        pass


@app.post("/api/discovercars-search")
async def discovercars_search(request: Request):
    """
    DiscoverCars AI Price Comparison Endpoint
    Scrapes prices from discovercars.com for competitive analysis
    """
    require_auth(request)
    
    try:
        body = await request.json()
        
        pickup_location = body.get("pickup_location")
        dropoff_location = body.get("dropoff_location", pickup_location)
        pickup_date = body.get("pickup_date")  # DD/MM/YYYY
        dropoff_date = body.get("dropoff_date")
        pickup_time = body.get("pickup_time", "10:00")
        dropoff_time = body.get("dropoff_time", "10:00")
        headless = body.get("headless", True)
        
        if not pickup_location or not pickup_date or not dropoff_date:
            return JSONResponse({
                "success": False,
                "error": "Missing required parameters: pickup_location, pickup_date, dropoff_date"
            }, status_code=400)
        
        # Import scraper
        import discovercars_scraper
        
        # Run scraper
        result = await discovercars_scraper.scrape_discovercars(
            pickup_location=pickup_location,
            dropoff_location=dropoff_location,
            pickup_date=pickup_date,
            dropoff_date=dropoff_date,
            pickup_time=pickup_time,
            dropoff_time=dropoff_time,
            headless=headless
        )
        
        return JSONResponse(result)
    
    except Exception as e:
        logger.error(f"Error in discovercars_search: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, status_code=500)


@app.get("/api/debug_html")
async def debug_html(request: Request):
    params = request.query_params
    location = params.get("location", "Albufeira")
    pickup_date = params.get("date")
    pickup_time = params.get("time", "10:00")
    days = int(params.get("days", 1))
    lang = params.get("lang", "en")
    currency = params.get("currency", "EUR")
    if not pickup_date:
        return JSONResponse({"ok": False, "error": "Missing date (YYYY-MM-DD)"}, status_code=400)

    try:
        from datetime import datetime, timedelta
        from playwright.async_api import async_playwright

        async def run_once():
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                await context.set_extra_http_headers({"User-Agent": "Mozilla/5.0 (compatible; PriceTracker/1.0)"})
                # block heavy resources for speed
                await context.route("**/*", lambda route: (
                    route.abort() if route.request.resource_type in {"image", "media", "font"} else route.continue_()
                ))
                page = await context.new_page()
                page.set_default_navigation_timeout(10000)
                page.set_default_timeout(8000)
                start_dt = datetime.fromisoformat(pickup_date + "T" + pickup_time)
                end_dt = start_dt + timedelta(days=days)
                html, final_url = await fetch_carjet_results(page, location, start_dt, end_dt, lang, currency, template="")
                await browser.close()
                return html, final_url

        html, final_url = await run_once()
        # Save to debug file
        from datetime import datetime as _dt
        stamp = _dt.utcnow().strftime("%Y%m%dT%H%M%S")
        filename = f"debug-{location.replace(' ', '-')}-{pickup_date}-{days}d.html"
        out_path = DEBUG_DIR / filename
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(html)

        # Quick selector counts and dataMap presence
        soup = BeautifulSoup(html, "lxml")
        counts = {
            ".price": len(soup.select(".price")),
            ".amount": len(soup.select(".amount")),
            "[class*='price']": len(soup.select("[class*='price']")),
            "a[href]": len(soup.select("a[href]")),
        }
        try:
            import json as _json
            m = re.search(r"var\s+dataMap\s*=\s*(\[.*?\]);", html, re.S)
            if m:
                arr = _json.loads(m.group(1))
                counts["has_dataMap"] = True
                counts["dataMap_len"] = len(arr)
            else:
                counts["has_dataMap"] = False
                counts["dataMap_len"] = 0
        except Exception:
            counts["has_dataMap"] = False
            counts["dataMap_len"] = 0
        return JSONResponse({
            "ok": True,
            "url": final_url,
            "debug_file": f"/static/debug/{filename}",
            "counts": counts,
        })
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


def save_snapshots(location: str, start_dt, days: int, items: List[Dict[str, Any]], currency: str):
    from datetime import datetime
    ts = datetime.utcnow().isoformat(timespec="seconds")
    with _db_lock:
        conn = _db_connect()
        try:
            for it in items:
                conn.execute(
                    """
                    INSERT INTO price_snapshots (ts, location, pickup_date, pickup_time, days, supplier, car, price_text, price_num, currency, link)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        ts,
                        location,
                        start_dt.strftime("%Y-%m-%d"),
                        start_dt.strftime("%H:%M"),
                        int(days),
                        (it.get("supplier") or "").strip(),
                        (it.get("car") or "").strip(),
                        (it.get("price") or "").strip(),
                        it.get("price_num"),
                        currency or (it.get("currency") or ""),
                        (it.get("link") or "").strip(),
                    ),
                )
        finally:
            conn.commit()
            conn.close()


@app.get("/api/history")
async def get_history(request: Request):
    require_auth(request)
    params = request.query_params
    location = params.get("location")
    supplier = params.get("supplier")
    days = params.get("days")
    since = params.get("from")
    until = params.get("to")
    limit = int(params.get("limit", 200))

    q = "SELECT ts, location, pickup_date, pickup_time, days, supplier, car, price_text, price_num, currency, link FROM price_snapshots WHERE 1=1"
    args: List[Any] = []
    if location:
        q += " AND location = ?"
        args.append(location)
    if supplier:
        q += " AND supplier LIKE ?"
        args.append(f"%{supplier}%")
    if days:
        q += " AND days = ?"
        args.append(int(days))
    if since:
        q += " AND ts >= ?"
        args.append(since)
    if until:
        q += " AND ts <= ?"
        args.append(until)
    q += " ORDER BY ts DESC LIMIT ?"
    args.append(limit)

    with _db_lock:
        conn = _db_connect()
        try:
            rows = conn.execute(q, tuple(args)).fetchall()
        finally:
            conn.close()

    items = [
        {
            "ts": r[0],
            "location": r[1],
            "pickup_date": r[2],
            "pickup_time": r[3],
            "days": r[4],
            "supplier": r[5],
            "car": r[6],
            "price": r[7],
            "price_num": r[8],
            "currency": r[9],
            "link": r[10],
        }
        for r in rows
    ]
    return JSONResponse({"ok": True, "count": len(items), "items": items})

@app.post("/api/price-automation/upload")
async def upload_price_automation(request: Request, file: UploadFile = File(...)):
    """Upload e processamento de ficheiro Excel para automa√ß√£o de pre√ßos"""
    require_auth(request)
    
    try:
        # Ler conte√∫do do ficheiro
        contents = await file.read()
        
        # Processar Excel
        import pandas as pd
        import io
        
        df = pd.read_excel(io.BytesIO(contents))
        
        # Converter para lista de dicion√°rios
        data = []
        for _, row in df.iterrows():
            data.append({
                'categoria': str(row.get('Categoria', '')),
                'localizacao': str(row.get('Localiza√ß√£o', row.get('Localizacao', ''))),
                'dias': int(row.get('Dias', 0)),
                'preco_base': float(row.get('Pre√ßo Base', row.get('Preco Base', 0))),
                'margem': float(row.get('Margem (%)', row.get('Margem', 0))),
                'preco_final': float(row.get('Pre√ßo Final', row.get('Preco Final', 0)))
            })
        
        return JSONResponse({
            "ok": True,
            "message": f"Ficheiro processado: {len(data)} linhas",
            "filename": file.filename,
            "data": data
        })
        
    except Exception as e:
        import traceback
        return JSONResponse({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, status_code=400)

@app.get("/api/price-history")
async def get_price_history(request: Request):
    """API para dados de gr√°ficos de hist√≥rico de pre√ßos"""
    require_auth(request)
    params = request.query_params
    location = params.get("location", "")
    days = params.get("days", "")
    category = params.get("category", "")
    
    with _db_lock:
        conn = _db_connect()
        try:
            # Detect database type
            import psycopg2
            is_postgres = isinstance(conn, psycopg2.extensions.connection)
            
            # Evolu√ß√£o de pre√ßos ao longo do tempo (√∫ltimos 30 dias) - MIN, AVG, MAX
            evolution_query = """
                SELECT DATE(ts) as date, 
                       MIN(price_num) as min_price,
                       AVG(price_num) as avg_price,
                       MAX(price_num) as max_price
                FROM price_snapshots
                WHERE price_num IS NOT NULL AND price_num > 0
            """
            evolution_query = _adapt_query_for_db(evolution_query, is_postgres)
            evolution_args = []
            placeholder = "%s" if is_postgres else "?"
            if location:
                evolution_query += f" AND location = {placeholder}"
                evolution_args.append(location)
            if days:
                evolution_query += f" AND days = {placeholder}"
                evolution_args.append(int(days))
            # Nota: categoria n√£o est√° na tabela, filtrar por car name como aproxima√ß√£o
            # if category:
            #     evolution_query += f" AND car LIKE {placeholder}"
            #     evolution_args.append(f"%{category}%")
            group_by_clause = " GROUP BY DATE(ts::timestamp) ORDER BY DATE(ts::timestamp) DESC LIMIT 30" if is_postgres else " GROUP BY DATE(ts) ORDER BY DATE(ts) DESC LIMIT 30"
            evolution_query += group_by_clause
            
            evolution_rows = conn.execute(evolution_query, tuple(evolution_args)).fetchall()
            evolution_labels = [r[0] for r in reversed(evolution_rows)]
            evolution_min = [round(r[1], 2) if r[1] else 0 for r in reversed(evolution_rows)]
            evolution_avg = [round(r[2], 2) if r[2] else 0 for r in reversed(evolution_rows)]
            evolution_max = [round(r[3], 2) if r[3] else 0 for r in reversed(evolution_rows)]
            
            # Compara√ß√£o por localiza√ß√£o (sempre Faro vs Albufeira)
            comparison_query = """
                SELECT location, AVG(price_num) as avg_price
                FROM price_snapshots
                WHERE price_num IS NOT NULL AND price_num > 0
                  AND location IN ('Aeroporto de Faro', 'Albufeira')
            """
            comparison_args = []
            if days:
                comparison_query += f" AND days = {placeholder}"
                comparison_args.append(int(days))
            comparison_query += " GROUP BY location ORDER BY location"
            
            comparison_rows = conn.execute(comparison_query, tuple(comparison_args)).fetchall()
            
            # Garantir que sempre temos Faro e Albufeira (mesmo sem dados)
            comparison_dict = {r[0]: round(r[1], 2) for r in comparison_rows}
            comparison_labels = ['Albufeira', 'Aeroporto de Faro']
            comparison_values = [
                comparison_dict.get('Albufeira', 0),
                comparison_dict.get('Aeroporto de Faro', 0)
            ]
            
            # Pre√ßos m√©dios por m√™s do ano
            if is_postgres:
                monthly_query = """
                    SELECT EXTRACT(MONTH FROM ts::timestamp)::INTEGER as month, AVG(price_num) as avg_price
                    FROM price_snapshots
                    WHERE price_num IS NOT NULL AND price_num > 0
                """
            else:
                monthly_query = """
                    SELECT CAST(strftime('%m', ts) AS INTEGER) as month, AVG(price_num) as avg_price
                    FROM price_snapshots
                    WHERE price_num IS NOT NULL AND price_num > 0
                """
            monthly_args = []
            if location:
                monthly_query += f" AND location = {placeholder}"
                monthly_args.append(location)
            if days:
                monthly_query += f" AND days = {placeholder}"
                monthly_args.append(int(days))
            monthly_query += " GROUP BY month ORDER BY month"
            
            monthly_rows = conn.execute(monthly_query, tuple(monthly_args)).fetchall()
            monthly_values = [0] * 12
            for r in monthly_rows:
                month_idx = r[0] - 1  # 1-12 -> 0-11
                if 0 <= month_idx < 12:
                    monthly_values[month_idx] = round(r[1], 2)
            
        finally:
            conn.close()
    
    return JSONResponse({
        "ok": True,
        "evolution": {
            "labels": evolution_labels,
            "min": evolution_min,
            "avg": evolution_avg,
            "max": evolution_max
        },
        "comparison": {
            "labels": comparison_labels,
            "values": comparison_values
        },
        "monthly": {
            "values": monthly_values
        }
    })

# ============================================================
# VEHICLES MANAGEMENT ENDPOINTS
# ============================================================

@app.get("/api/vehicles")
async def get_vehicles(request: Request):
    """Retorna todos os ve√≠culos mapeados no dicion√°rio VEHICLES"""
    require_auth(request)
    try:
        from carjet_direct import VEHICLES
        
        # Organizar por categoria
        by_category = {}
        for car, category in VEHICLES.items():
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(car)
        
        # Ordenar categorias e carros
        for cat in by_category:
            by_category[cat] = sorted(by_category[cat])
        
        return _no_store_json({
            "ok": True,
            "total": len(VEHICLES),
            "vehicles": dict(sorted(VEHICLES.items())),
            "by_category": dict(sorted(by_category.items())),
            "categories": sorted(set(VEHICLES.values()))
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)


@app.get("/api/vehicles/search")
async def search_vehicles(request: Request, q: str = ""):
    """Busca ve√≠culos no dicion√°rio VEHICLES"""
    require_auth(request)
    try:
        from carjet_direct import VEHICLES, detect_category_from_car
        
        query = q.lower().strip()
        if not query:
            return _no_store_json({"ok": False, "error": "Query parameter 'q' is required"}, 400)
        
        # Buscar matches
        matches = {}
        for car, category in VEHICLES.items():
            if query in car:
                matches[car] = category
        
        # Testar categoria usando a fun√ß√£o
        detected_category = detect_category_from_car(q, '')
        
        return _no_store_json({
            "ok": True,
            "query": q,
            "matches": matches,
            "detected_category": detected_category,
            "in_vehicles": q.lower() in VEHICLES
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vehicles/save")
async def save_vehicle(request: Request):
    """Salva ou atualiza um ve√≠culo no sistema e atualiza carjet_direct.py automaticamente"""
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        body = await request.json()
        
        original_name = body.get('original_name', '').strip()
        clean_name = body.get('clean_name', '').lower().strip()
        category = body.get('category', '').strip()
        
        if not clean_name or not category:
            return _no_store_json({"ok": False, "error": "clean_name and category are required"}, 400)
        
        # Salvar na tabela vehicle_name_overrides
        with _db_lock:
            con = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = con.__class__.__module__ == 'psycopg2.extensions'
                param_placeholder = "%s" if is_postgres else "?"
                
                # Verificar se j√° existe
                if is_postgres:
                    with con.cursor() as cur:
                        cur.execute(f"SELECT edited_name FROM vehicle_name_overrides WHERE original_name = {param_placeholder}", (original_name,))
                        existing = cur.fetchone()
                else:
                    existing = con.execute(
                        f"SELECT edited_name FROM vehicle_name_overrides WHERE original_name = {param_placeholder}",
                        (original_name,)
                    ).fetchone()
                
                if existing:
                    # Atualizar
                    if is_postgres:
                        with con.cursor() as cur:
                            cur.execute(
                                f"UPDATE vehicle_name_overrides SET edited_name = {param_placeholder}, updated_at = NOW() WHERE original_name = {param_placeholder}",
                                (clean_name, original_name)
                            )
                    else:
                        con.execute(
                            f"UPDATE vehicle_name_overrides SET edited_name = {param_placeholder}, updated_at = datetime('now') WHERE original_name = {param_placeholder}",
                            (clean_name, original_name)
                        )
                else:
                    # Inserir novo
                    if is_postgres:
                        with con.cursor() as cur:
                            cur.execute(
                                f"INSERT INTO vehicle_name_overrides (original_name, edited_name, updated_at) VALUES ({param_placeholder}, {param_placeholder}, NOW())",
                                (original_name, clean_name)
                            )
                    else:
                        con.execute(
                            f"INSERT INTO vehicle_name_overrides (original_name, edited_name, updated_at) VALUES ({param_placeholder}, {param_placeholder}, datetime('now'))",
                            (original_name, clean_name)
                        )
                
                con.commit()
            finally:
                con.close()
        
        # Atualizar carjet_direct.py automaticamente
        try:
            import carjet_direct
            import importlib
            import re
            
            # Ler o arquivo atual
            carjet_path = os.path.join(os.path.dirname(__file__), 'carjet_direct.py')
            with open(carjet_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Encontrar o in√≠cio e fim do dicion√°rio VEHICLES
            vehicles_start = -1
            vehicles_end = -1
            
            for i, line in enumerate(lines):
                if 'VEHICLES = {' in line:
                    vehicles_start = i
                elif vehicles_start > 0 and line.strip() == '}' and vehicles_end == -1:
                    vehicles_end = i
                    break
            
            if vehicles_start == -1 or vehicles_end == -1:
                raise Exception("Could not find VEHICLES dictionary")
            
            # Verificar se o ve√≠culo j√° existe
            vehicle_pattern = f"    '{re.escape(clean_name)}':"
            vehicle_exists = False
            vehicle_line_idx = -1
            
            for i in range(vehicles_start, vehicles_end):
                if vehicle_pattern in lines[i]:
                    vehicle_exists = True
                    vehicle_line_idx = i
                    break
            
            if vehicle_exists:
                # Atualizar entrada existente
                lines[vehicle_line_idx] = f"    '{clean_name}': '{category}',\n"
            else:
                # Adicionar nova entrada antes do }
                new_entry = f"    '{clean_name}': '{category}',\n"
                lines.insert(vehicles_end, new_entry)
            
            # Escrever de volta
            with open(carjet_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            
            # Recarregar o m√≥dulo
            importlib.reload(carjet_direct)
            
            message = "Vehicle saved and carjet_direct.py updated automatically!"
        except Exception as e:
            import traceback
            message = f"Vehicle saved but failed to update carjet_direct.py: {str(e)}\n{traceback.format_exc()}"
        
        # Calcular grupo baseado na categoria
        group = map_category_to_group(category, clean_name)
        
        # Gerar c√≥digo Python
        code = f"    '{clean_name}': '{category}',"
        
        # Invalidar cache do frontend para atualizar pesquisa imediatamente
        global _vehicles_last_update
        _vehicles_last_update = datetime.utcnow().isoformat()
        
        return _no_store_json({
            "ok": True,
            "message": message,
            "clean_name": clean_name,
            "category": category,
            "group": group,
            "code": code,
            "cache_invalidated": True,
            "updated_at": _vehicles_last_update
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# ADMIN - CAR GROUPS MANAGEMENT
# ============================================================


# ============================================================
# ADMIN - CAR GROUPS MANAGEMENT (Fichas Individuais)
# ============================================================

@app.get("/admin/car-groups", response_class=HTMLResponse)
async def admin_car_groups(request: Request):
    """P√°gina de administra√ß√£o dos grupos de carros - NOVA vers√£o com abas e cria√ß√£o de categorias"""
    require_auth(request)
    
    # Ler o ficheiro HTML NOVO (vehicle_editor.html)
    html_path = os.path.join(os.path.dirname(__file__), "vehicle_editor.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: Ficheiro vehicle_editor.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/vehicles-editor", response_class=HTMLResponse)
async def admin_vehicles_editor(request: Request):
    """Editor avan√ßado de ve√≠culos com nome original vs editado"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "vehicle_editor.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: vehicle_editor.html n√£o encontrado</h1>", status_code=500)

@app.get("/vehicle-editor", response_class=HTMLResponse)
async def vehicle_editor(request: Request):
    """Vehicle Editor - Alias para /admin/car-groups"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "vehicle_editor.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: vehicle_editor.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/price-validation", response_class=HTMLResponse)
async def admin_price_validation(request: Request):
    """P√°gina de configura√ß√£o de regras de valida√ß√£o de pre√ßos"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "price_validation_rules.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: price_validation_rules.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/price-automation-settings", response_class=HTMLResponse)
async def admin_price_automation_settings(request: Request):
    """P√°gina de parametriza√ß√µes para automa√ß√£o de pre√ßos"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "price_automation_settings.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: price_automation_settings.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/damage-report", response_class=HTMLResponse)
async def admin_damage_report(request: Request):
    """P√°gina de administra√ß√£o do Damage Report"""
    require_auth(request)
    
    # Contar campos mapeados
    mapped_fields = 0
    try:
        import json
        if os.path.exists('damage_report_coordinates.json'):
            with open('damage_report_coordinates.json', 'r') as f:
                coordinates = json.load(f)
                mapped_fields = len(coordinates)
    except Exception as e:
        logging.error(f"Error counting mapped fields: {e}")
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "admin_damage_report.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # Replace template variables
            content = content.replace('{{ mapped_fields }}', str(mapped_fields))
            return HTMLResponse(content=content)
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: admin_damage_report.html n√£o encontrado</h1>", status_code=500)

@app.post("/api/damage-reports/upload-template")
async def upload_damage_report_template(request: Request):
    """Upload de novo template PDF"""
    require_auth(request)
    
    try:
        from datetime import datetime
        from PyPDF2 import PdfReader
        
        # Garantir tabelas existem
        _ensure_damage_report_tables()
        
        # Receber form data
        form = await request.form()
        file = form.get('file')
        notes = form.get('notes', '')
        
        if not file:
            return {"ok": False, "error": "Nenhum ficheiro enviado"}
        
        # Ler conte√∫do do PDF
        pdf_content = await file.read()
        filename = file.filename
        
        # Obter n√∫mero de p√°ginas
        from io import BytesIO
        pdf_reader = PdfReader(BytesIO(pdf_content))
        num_pages = len(pdf_reader.pages)
        
        # Obter usu√°rio atual
        username = request.session.get('username', 'system')
        
        # Obter pr√≥xima vers√£o
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar PostgreSQL vs SQLite
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                
                placeholder = "%s" if is_postgres else "?"
                
                cursor = conn.execute("SELECT MAX(version) FROM damage_report_templates")
                row = cursor.fetchone()
                next_version = (row[0] or 0) + 1
                
                # Desativar templates anteriores
                conn.execute("UPDATE damage_report_templates SET is_active = 0")
                
                # Inserir novo template
                placeholders = ", ".join([placeholder] * 8)
                insert_query = f"""
                    INSERT INTO damage_report_templates 
                    (version, filename, file_data, num_pages, uploaded_by, uploaded_at, is_active, notes)
                    VALUES ({placeholders})
                """
                conn.execute(insert_query, (
                    next_version,
                    filename,
                    pdf_content,
                    num_pages,
                    username,
                    datetime.now().isoformat(),
                    1,  # is_active
                    notes
                ))
                
                conn.commit()
                
                # Guardar tamb√©m como ficheiro
                with open('Damage Report.pdf', 'wb') as f:
                    f.write(pdf_content)
                
                logging.info(f"‚úÖ Template v{next_version} carregado por {username}")
                
                return {
                    "ok": True, 
                    "version": next_version, 
                    "num_pages": num_pages,
                    "filename": filename
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error uploading template: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/get-active-template")
async def get_active_damage_report_template(request: Request):
    """Obter o template PDF ativo para o mapeador"""
    require_auth(request)
    
    try:
        from starlette.responses import Response
        
        # Garantir tabelas existem
        _ensure_damage_report_tables()
        
        # Buscar template ativo da BD
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar PostgreSQL vs SQLite
                is_postgres = False
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    is_postgres = True
                
                if is_postgres:
                    # PostgreSQL
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT file_data, filename 
                        FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                    cursor.close()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT file_data, filename 
                        FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                
                if not row or not row[0]:
                    # Fallback para arquivo f√≠sico se n√£o houver template na BD
                    try:
                        with open('Damage Report.pdf', 'rb') as f:
                            pdf_data = f.read()
                        logging.info("üìÑ Serving template from file (no active template in DB)")
                        return Response(content=pdf_data, media_type="application/pdf")
                    except FileNotFoundError:
                        return Response(
                            content=b"No template available",
                            status_code=404,
                            media_type="text/plain"
                        )
                
                pdf_data = row[0]
                filename = row[1] if len(row) > 1 else "template.pdf"
                
                # Convert memoryview to bytes if needed (PostgreSQL returns memoryview)
                if isinstance(pdf_data, memoryview):
                    pdf_data = bytes(pdf_data)
                
                logging.info(f"üìÑ Serving active template: {filename}")
                
                return Response(
                    content=pdf_data,
                    media_type="application/pdf",
                    headers={
                        "Content-Disposition": f'inline; filename="{filename}"'
                    }
                )
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting active template: {e}")
        return Response(
            content=str(e).encode(),
            status_code=500,
            media_type="text/plain"
        )

@app.get("/api/damage-reports/get-coordinates")
async def get_damage_report_coordinates(request: Request):
    """Obter coordenadas dos campos do PDF"""
    require_auth(request)
    
    try:
        _ensure_damage_report_tables()
        
        with _db_lock:
            conn = _db_connect()
            try:
                rows = []
                
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                    cursor.close()
                else:
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                # Agrupar coordenadas por field_id (permite m√∫ltiplas entradas por campo)
                coordinates = {}
                template_version = 1
                for row in rows:
                    field_id = row[0]
                    coord_data = {
                        'x': row[1],
                        'y': row[2],
                        'width': row[3],
                        'height': row[4],
                        'page': row[5] if row[5] else 1
                    }
                    
                    # Se field j√° existe, adicionar ao array; sen√£o criar novo array
                    if field_id in coordinates:
                        # J√° √© array, adicionar nova entrada
                        if isinstance(coordinates[field_id], list):
                            coordinates[field_id].append(coord_data)
                        else:
                            # Converter para array (compatibilidade com formato antigo)
                            coordinates[field_id] = [coordinates[field_id], coord_data]
                    else:
                        # Primeira entrada - manter compatibilidade com frontend antigo
                        coordinates[field_id] = coord_data
                    
                    if len(row) > 7 and row[7]:
                        template_version = row[7]
                
                logging.info(f"üìä GET Coordinates: {len(coordinates)} fields (some may have multiple pages)")
                return {"ok": True, "coordinates": coordinates, "version": template_version}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting coordinates: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/get-coordinates-public")
async def get_damage_report_coordinates_public():
    """
    Endpoint TEMPOR√ÅRIO p√∫blico para sincronizar coordenadas
    TODO: REMOVER ap√≥s sincroniza√ß√£o!
    """
    try:
        _ensure_damage_report_tables()
        
        with _db_lock:
            conn = _db_connect()
            try:
                rows = []
                
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                    cursor.close()
                else:
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                coordinates = {}
                template_version = 1
                for row in rows:
                    field_id = row[0]
                    coordinates[field_id] = {
                        'x': row[1],
                        'y': row[2],
                        'width': row[3],
                        'height': row[4],
                        'page': row[5] if row[5] else 1
                    }
                    if len(row) > 7 and row[7]:
                        template_version = row[7]
                
                logging.info(f"üìä [PUBLIC] GET Coordinates: {len(coordinates)} fields")
                return {"ok": True, "coordinates": coordinates, "version": template_version, "count": len(coordinates)}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting coordinates (public): {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/regenerate-grid")
async def regenerate_damage_report_grid(request: Request):
    """Regenerar PDF com grid a partir do template ativo"""
    require_auth(request)
    
    try:
        from PyPDF2 import PdfReader, PdfWriter
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.units import cm
        from reportlab.lib import colors
        from io import BytesIO
        
        # Garantir tabelas existem
        _ensure_damage_report_tables()
        
        # Obter template ativo da BD
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT file_data, num_pages 
                    FROM damage_report_templates 
                    WHERE is_active = 1 
                    ORDER BY version DESC LIMIT 1
                """)
                row = cursor.fetchone()
                
                if not row:
                    return {"ok": False, "error": "Nenhum template ativo encontrado"}
                
                pdf_data = row[0]
                num_pages = row[1]
            finally:
                conn.close()
        
        # Criar overlay com grid
        def create_grid_overlay():
            packet = BytesIO()
            c = canvas.Canvas(packet, pagesize=A4)
            width, height = A4
            
            # Grid semi-transparente
            c.setStrokeColor(colors.Color(0, 0, 1, alpha=0.3))
            c.setLineWidth(0.5)
            
            # Linhas verticais a cada 1cm
            for x in range(0, int(width/cm) + 1):
                x_pos = x * cm
                c.line(x_pos, 0, x_pos, height)
                if x % 2 == 0:
                    c.setFont('Helvetica', 7)
                    c.setFillColor(colors.Color(0, 0, 1, alpha=0.7))
                    c.drawString(x_pos + 2, height - 12, str(x))
                    c.drawString(x_pos + 2, 8, str(x))
            
            # Linhas horizontais a cada 1cm
            for y in range(0, int(height/cm) + 1):
                y_pos = y * cm
                c.line(0, y_pos, width, y_pos)
                if y % 2 == 0:
                    c.setFont('Helvetica', 7)
                    c.setFillColor(colors.Color(0, 0, 1, alpha=0.7))
                    c.drawString(8, y_pos + 2, str(y))
                    c.drawString(width - 18, y_pos + 2, str(y))
            
            # Legenda
            c.setFont('Helvetica-Bold', 11)
            c.setFillColor(colors.Color(1, 0, 0, alpha=0.9))
            c.drawString(2*cm, height - 0.8*cm, 'GRID DE COORDENADAS (em cm)')
            
            c.save()
            packet.seek(0)
            return packet
        
        # Ler PDF original
        reader = PdfReader(BytesIO(pdf_data))
        writer = PdfWriter()
        
        # Processar cada p√°gina
        for page_num in range(len(reader.pages)):
            original_page = reader.pages[page_num]
            
            # Criar overlay com grid
            grid_overlay = create_grid_overlay()
            overlay_pdf = PdfReader(grid_overlay)
            overlay_page = overlay_pdf.pages[0]
            
            # Merge
            original_page.merge_page(overlay_page)
            writer.add_page(original_page)
        
        # Salvar
        with open('static/damage_report_with_grid.pdf', 'wb') as f:
            writer.write(f)
        
        logging.info(f"‚úÖ Grid regenerado com {len(reader.pages)} p√°ginas")
        
        return {"ok": True, "pages": len(reader.pages)}
    except Exception as e:
        logging.error(f"Error regenerating grid: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/download-coordinates")
async def download_damage_report_coordinates(request: Request):
    """Download do ficheiro de coordenadas (busca da base de dados)"""
    require_auth(request)
    
    try:
        import json
        import tempfile
        
        # Buscar coordenadas da base de dados
        coordinates = {}
        
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                    cursor.close()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                # Converter para dicion√°rio
                for row in rows:
                    field_id = row[0]
                    coordinates[field_id] = {
                        'x': row[1],
                        'y': row[2],
                        'width': row[3],
                        'height': row[4],
                        'page': row[5],
                        'type': row[6],
                        'version': row[7]
                    }
            finally:
                conn.close()
        
        if not coordinates:
            raise HTTPException(status_code=404, detail="No coordinates found")
        
        # Criar ficheiro tempor√°rio
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(coordinates, f, indent=2)
            temp_path = f.name
        
        logging.info(f"üì• Downloading {len(coordinates)} coordinates")
        
        # Retornar ficheiro e apagar ap√≥s envio
        return FileResponse(
            temp_path,
            media_type='application/json',
            filename='damage_report_coordinates.json',
            background=None  # File will be deleted after response
        )
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error downloading coordinates: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/customization/branding", response_class=HTMLResponse)
async def admin_customization_branding(request: Request):
    """P√°gina de configura√ß√£o de branding"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "customization_branding.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: customization_branding.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/customization/appearance", response_class=HTMLResponse)
async def admin_customization_appearance(request: Request):
    """P√°gina de configura√ß√£o de apar√™ncia"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "customization_appearance.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: customization_appearance.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/customization/company-info", response_class=HTMLResponse)
async def admin_customization_company_info(request: Request):
    """P√°gina de informa√ß√µes da empresa"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "customization_company_info.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: customization_company_info.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/customization/language", response_class=HTMLResponse)
async def admin_customization_language(request: Request):
    """P√°gina de configura√ß√£o de idioma"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "customization_language.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: customization_language.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/customization/email", response_class=HTMLResponse)
async def admin_customization_email(request: Request):
    """P√°gina de configura√ß√£o de notifica√ß√µes por email"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "customization_email.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: customization_email.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/customization/automated-reports", response_class=HTMLResponse)
async def admin_customization_automated_reports(request: Request):
    """P√°gina de configura√ß√£o de relat√≥rios autom√°ticos - NOVO DESIGN"""
    require_auth(request)
    
    # Usar novo template redesenhado (sem emojis, √≠cones monocrom√°ticos)
    html_path = os.path.join(os.path.dirname(__file__), "templates", "automated_reports_redesign.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: automated_reports_redesign.html n√£o encontrado</h1>", status_code=500)

@app.get("/admin/migrate-data", response_class=HTMLResponse)
async def admin_migrate_data(request: Request):
    """P√°gina de migra√ß√£o de dados localStorage ‚Üí Database"""
    require_auth(request)
    
    html_path = os.path.join(os.path.dirname(__file__), "templates", "migrate_data.html")
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="<h1>Erro: migrate_data.html n√£o encontrado</h1>", status_code=500)

# ============================================================
# API ENDPOINTS - PRICE AUTOMATION SETTINGS PERSISTENCE
# ============================================================

@app.post("/api/price-automation/settings/save")
async def save_price_automation_settings(request: Request):
    """Salvar configura√ß√µes globais de automa√ß√£o de pre√ßos na base de dados"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üíæ Saving price automation settings: {len(data)} keys")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                # Salvar cada configura√ß√£o
                for key, value in data.items():
                    value_json = json.dumps(value)
                    logging.debug(f"  - {key}: {value_json[:100]}...")
                    
                    query = f"""
                        INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                        VALUES ({placeholder}, {placeholder}, 'json', CURRENT_TIMESTAMP)
                        ON CONFLICT (setting_key) DO UPDATE SET
                            setting_value = EXCLUDED.setting_value,
                            updated_at = CURRENT_TIMESTAMP
                    """
                    
                    conn.execute(query, (key, value_json))
                conn.commit()
                logging.info(f"‚úÖ Price automation settings saved to database (placeholder: {placeholder})")
                return JSONResponse({"ok": True, "message": "Settings saved successfully"})
            except Exception as db_err:
                logging.error(f"‚ùå Database error saving settings: {str(db_err)}")
                raise
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving price automation settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/price-automation/settings/load")
async def load_price_automation_settings(request: Request):
    """Carregar configura√ß√µes globais de automa√ß√£o de pre√ßos da base de dados"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("SELECT setting_key, setting_value FROM price_automation_settings")
                rows = cursor.fetchall()
                
                settings = {}
                for row in rows:
                    try:
                        settings[row[0]] = json.loads(row[1])
                    except:
                        settings[row[0]] = row[1]
                
                return JSONResponse({"ok": True, "settings": settings})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/price-automation/rules/save")
async def save_automated_price_rules(request: Request):
    """Salvar regras automatizadas de pre√ßos na base de dados"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üíæ Saving automated price rules for {len(data)} locations")
        logging.info(f"üì¶ Data structure: {list(data.keys())}")
        
        # üö® PROTE√á√ÉO: Contar quantas regras reais existem
        total_rules = 0
        for location, grupos in data.items():
            for grupo, grupo_data in grupos.items():
                if 'months' in grupo_data:
                    for month, month_data in grupo_data['months'].items():
                        if 'days' in month_data:
                            total_rules += len(month_data['days'])
        
        logging.info(f"üìä Total rules to save: {total_rules}")
        
        # üö® PROTE√á√ÉO: Se tentando salvar 0 regras, verificar se j√° existem regras
        if total_rules == 0:
            with _db_lock:
                conn = _db_connect()
                try:
                    cursor = conn.execute("SELECT COUNT(*) FROM automated_price_rules")
                    existing_count = cursor.fetchone()[0]
                    logging.warning(f"‚ö†Ô∏è Attempting to save 0 rules, but {existing_count} rules exist in database!")
                    
                    if existing_count > 0:
                        logging.error(f"‚ùå BLOCKED: Refusing to delete {existing_count} existing rules with empty save!")
                        return JSONResponse({
                            "ok": False, 
                            "error": f"Cannot save empty rules when {existing_count} rules exist. Use Clear All button explicitly.",
                            "blocked": True
                        }, status_code=400)
                finally:
                    conn.close()
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD - ROBUST METHOD
                is_postgres = False
                conn_type = type(conn).__name__
                
                if 'psycopg' in conn_type.lower() or conn_type == 'connection':
                    is_postgres = True
                elif os.getenv('DATABASE_URL'):
                    is_postgres = True
                else:
                    try:
                        import psycopg2
                        is_postgres = isinstance(conn, psycopg2.extensions.connection)
                    except:
                        pass
                
                placeholder = "%s" if is_postgres else "?"
                logging.info(f"üíæ Saving to {'PostgreSQL' if is_postgres else 'SQLite'} (conn type: {conn_type})")
                
                # Limpar regras antigas
                logging.info("üóëÔ∏è Deleting old rules...")
                conn.execute("DELETE FROM automated_price_rules")
                logging.info("‚úÖ Old rules deleted")
                
                # Salvar novas regras
                rules_count = 0
                for location, grupos in data.items():
                    logging.info(f"  üìç Location: {location} ({len(grupos)} groups)")
                    for grupo, grupo_data in grupos.items():
                        if 'months' in grupo_data:
                            months_count = len(grupo_data['months'])
                            logging.info(f"    üìä Group: {grupo} ({months_count} months)")
                            for month, month_data in grupo_data['months'].items():
                                if 'days' in month_data:
                                    days_count = len(month_data['days'])
                                    logging.info(f"      üìÖ Month {month}: {days_count} days")
                                    for day, day_config in month_data['days'].items():
                                        try:
                                            config_json = json.dumps(day_config)
                                            strategies_count = len(day_config.get('strategies', []))
                                            logging.info(f"        üíæ Saving {location}/{grupo}/M{month}/D{day} ({strategies_count} strategies)")
                                            
                                            conn.execute(
                                                f"""
                                                INSERT INTO automated_price_rules 
                                                (location, grupo, month, day, config, updated_at)
                                                VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, CURRENT_TIMESTAMP)
                                                """,
                                                (location, grupo, int(month), int(day), config_json)
                                            )
                                            rules_count += 1
                                        except Exception as rule_err:
                                            logging.error(f"‚ùå Error saving rule {location}/{grupo}/{month}/{day}: {str(rule_err)}")
                                            raise
                
                conn.commit()
                logging.info(f"‚úÖ Saved {rules_count} automated price rules to database")
                return JSONResponse({"ok": True, "message": "Rules saved successfully"})
            except Exception as db_err:
                logging.error(f"‚ùå Database error saving rules: {str(db_err)}")
                raise
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving automated price rules: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/price-automation/rules/debug")
async def debug_automated_price_rules(request: Request):
    """Debug endpoint to check if automated_price_rules table exists and has data"""
    try:
        # Get database info
        import os
        db_url = os.getenv("DATABASE_URL", "")
        internal_url = os.getenv("INTERNAL_DATABASE_URL", "")
        db_type = "PostgreSQL" if (_USE_NEW_DB and USE_POSTGRES) else "SQLite"
        
        # Show more of URL to identify if it changes
        if db_url:
            # Show host and database name (safe to show)
            import re
            match = re.search(r'postgresql://[^@]+@([^/]+)/(.+?)(\?|$)', db_url)
            if match:
                db_url_preview = f"postgresql://***@{match.group(1)}/{match.group(2)}"
            else:
                db_url_preview = db_url[:60] + "..."
        else:
            db_url_preview = "local.db"
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Check if table exists and count rows
                try:
                    cursor = conn.execute("SELECT COUNT(*) FROM automated_price_rules")
                    count = cursor.fetchone()[0]
                    logging.info(f"‚úÖ Table exists, {count} rules found in {db_type}")
                except Exception as table_err:
                    logging.error(f"‚ùå Table doesn't exist or error: {table_err}")
                    return JSONResponse({
                        "ok": False, 
                        "error": "Table doesn't exist", 
                        "detail": str(table_err),
                        "db_type": db_type,
                        "db_url": db_url_preview
                    })
                
                # Get all data
                cursor = conn.execute("SELECT location, grupo, month, day, config FROM automated_price_rules")
                rows = cursor.fetchall()
                
                # Get unique locations
                locations = {}
                for r in rows:
                    loc = r[0]
                    if loc not in locations:
                        locations[loc] = 0
                    locations[loc] += 1
                
                # Get samples from EACH location (not just first 10)
                samples_by_location = {}
                for loc in locations.keys():
                    cursor = conn.execute(
                        "SELECT location, grupo, month, day, config FROM automated_price_rules WHERE location = %s LIMIT 3" if db_type == "PostgreSQL" else 
                        "SELECT location, grupo, month, day, config FROM automated_price_rules WHERE location = ? LIMIT 3",
                        (loc,)
                    )
                    loc_rows = cursor.fetchall()
                    samples_by_location[loc] = [{"grupo": r[1], "month": r[2], "day": r[3], "config": r[4][:100] if r[4] else None} for r in loc_rows]
                
                return JSONResponse({
                    "ok": True,
                    "count": len(rows),
                    "db_type": db_type,
                    "db_url": db_url_preview,
                    "internal_url": internal_url[:60] + "..." if internal_url else None,
                    "_USE_NEW_DB": _USE_NEW_DB,
                    "USE_POSTGRES": USE_POSTGRES if _USE_NEW_DB else None,
                    "locations": locations,
                    "samples_by_location": samples_by_location,
                    "sample_rows": [{"location": r[0], "grupo": r[1], "month": r[2], "day": r[3], "config": r[4][:100] if r[4] else None} for r in rows[:10]]
                })
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Debug error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/price-automation/rules/load")
async def load_automated_price_rules(request: Request):
    """Carregar regras automatizadas de pre√ßos da base de dados"""
    require_auth(request)
    
    try:
        logging.info("üì• Loading automated price rules from database...")
        
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT location, grupo, month, day, config FROM automated_price_rules ORDER BY location, grupo, month, day"
                )
                rows = cursor.fetchall()
                
                logging.info(f"üì¶ Found {len(rows)} rules in database")
                
                rules = {}
                for row in rows:
                    location, grupo, month, day, config_json = row
                    
                    if location not in rules:
                        rules[location] = {}
                    if grupo not in rules[location]:
                        rules[location][grupo] = {"months": {}}
                    if str(month) not in rules[location][grupo]["months"]:
                        rules[location][grupo]["months"][str(month)] = {"days": {}}
                    
                    try:
                        config = json.loads(config_json)
                        strategies_count = len(config.get('strategies', []))
                        rules[location][grupo]["months"][str(month)]["days"][str(day)] = config
                        logging.info(f"  ‚úÖ Loaded {location}/{grupo}/M{month}/D{day} ({strategies_count} strategies)")
                    except Exception as parse_err:
                        logging.error(f"  ‚ùå Error parsing config for {location}/{grupo}/M{month}/D{day}: {parse_err}")
                        rules[location][grupo]["months"][str(month)]["days"][str(day)] = {}
                
                total_locations = len(rules)
                total_groups = sum(len(g) for g in rules.values())
                logging.info(f"‚úÖ Loaded {len(rows)} rules for {total_locations} locations, {total_groups} groups")
                
                return JSONResponse({"ok": True, "rules": rules})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/price-automation/strategies/save")
async def save_pricing_strategies(request: Request):
    """Salvar estrat√©gias de pricing na base de dados"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üíæ Saving pricing strategies: {len(data)} keys")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD - ROBUST METHOD
                is_postgres = False
                conn_type = type(conn).__name__
                
                if 'psycopg' in conn_type.lower() or conn_type == 'connection':
                    is_postgres = True
                elif os.getenv('DATABASE_URL'):
                    is_postgres = True
                else:
                    try:
                        import psycopg2
                        is_postgres = isinstance(conn, psycopg2.extensions.connection)
                    except:
                        pass
                
                placeholder = "%s" if is_postgres else "?"
                logging.info(f"üíæ Saving strategies to {'PostgreSQL' if is_postgres else 'SQLite'} (conn type: {conn_type})")
                
                # Limpar estrat√©gias antigas
                conn.execute("DELETE FROM pricing_strategies")
                
                # Salvar novas estrat√©gias
                strategies_count = 0
                for key, strategies in data.items():
                    # Parse key: location_grupo_month_day
                    parts = key.split('_')
                    if len(parts) >= 4:
                        location = parts[0]
                        grupo = parts[1]
                        try:
                            month = int(parts[2])
                            day = int(parts[3])
                        except ValueError as ve:
                            logging.error(f"‚ùå Invalid month/day format in key '{key}': {str(ve)}")
                            continue
                        
                        for idx, strategy in enumerate(strategies):
                            try:
                                strategy_json = json.dumps(strategy)
                                conn.execute(
                                    f"""
                                    INSERT INTO pricing_strategies 
                                    (location, grupo, month, day, priority, strategy_type, config, updated_at)
                                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, CURRENT_TIMESTAMP)
                                    """,
                                    (location, grupo, month, day, idx + 1, strategy.get('type', 'unknown'), strategy_json)
                                )
                                strategies_count += 1
                            except Exception as strat_err:
                                logging.error(f"‚ùå Error saving strategy {key}[{idx}]: {str(strat_err)}")
                                raise
                
                conn.commit()
                logging.info(f"‚úÖ Saved {strategies_count} pricing strategies to database")
                return JSONResponse({"ok": True, "message": "Strategies saved successfully"})
            except Exception as db_err:
                logging.error(f"‚ùå Database error saving strategies: {str(db_err)}")
                raise
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving pricing strategies: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/price-automation/strategies/load")
async def load_pricing_strategies(request: Request):
    """Carregar estrat√©gias de pricing da base de dados"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT location, grupo, month, day, priority, config FROM pricing_strategies ORDER BY location, grupo, month, day, priority"
                )
                rows = cursor.fetchall()
                
                strategies = {}
                for row in rows:
                    location, grupo, month, day, priority, config_json = row
                    key = f"{location}_{grupo}_{month}_{day}"
                    
                    if key not in strategies:
                        strategies[key] = []
                    
                    try:
                        strategies[key].append(json.loads(config_json))
                    except:
                        pass
                
                return JSONResponse({"ok": True, "strategies": strategies})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/price-automation/history/save")
async def save_automated_prices_history(request: Request):
    """Salvar hist√≥rico de pre√ßos automatizados"""
    require_auth(request)
    
    try:
        data = await request.json()
        username = request.session.get("username", "unknown")
        source = data.get("source", "manual")  # 'manual' or 'automated'
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Salvar cada entrada do hist√≥rico
                for entry in data.get("entries", []):
                    conn.execute(
                        """
                        INSERT INTO automated_prices_history 
                        (location, grupo, dias, pickup_date, auto_price, real_price, 
                         strategy_used, strategy_details, min_price_applied, created_by, source)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            entry.get("location"),
                            entry.get("grupo"),
                            entry.get("dias"),
                            entry.get("pickup_date"),
                            entry.get("auto_price"),
                            entry.get("real_price"),
                            entry.get("strategy_used"),
                            json.dumps(entry.get("strategy_details", {})),
                            entry.get("min_price_applied"),
                            username,
                            source
                        )
                    )
                
                conn.commit()
                return JSONResponse({"ok": True, "message": "History saved successfully"})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/price-automation/history/load")
async def load_automated_prices_history(request: Request):
    """Carregar hist√≥rico de pre√ßos automatizados"""
    require_auth(request)
    
    try:
        # Par√¢metros opcionais de filtro
        location = request.query_params.get("location")
        grupo = request.query_params.get("grupo")
        limit = int(request.query_params.get("limit", 100))
        
        with _db_lock:
            conn = _db_connect()
            try:
                query = """
                    SELECT id, location, grupo, dias, pickup_date, 
                           auto_price, real_price, strategy_used, strategy_details,
                           min_price_applied, created_at, created_by, source
                    FROM automated_prices_history
                    WHERE 1=1
                """
                params = []
                
                if location:
                    query += " AND location = ?"
                    params.append(location)
                
                if grupo:
                    query += " AND grupo = ?"
                    params.append(grupo)
                
                query += " ORDER BY created_at DESC LIMIT ?"
                params.append(limit)
                
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
                
                history = []
                for row in rows:
                    try:
                        strategy_details = json.loads(row[8]) if row[8] else {}
                    except:
                        strategy_details = {}
                    
                    history.append({
                        "id": row[0],
                        "location": row[1],
                        "grupo": row[2],
                        "dias": row[3],
                        "pickup_date": row[4],
                        "auto_price": row[5],
                        "real_price": row[6],
                        "strategy_used": row[7],
                        "strategy_details": strategy_details,
                        "min_price_applied": row[9],
                        "created_at": row[10],
                        "created_by": row[11],
                        "source": row[12] if len(row) > 12 else 'manual'
                    })
                
                return JSONResponse({"ok": True, "history": history})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/prices/current/save")
async def save_current_prices(request: Request):
    """Guardar Pre√ßos Atuais (Current Prices) no hist√≥rico"""
    require_auth(request)
    
    try:
        data = await request.json()
        location = data.get("location", "")
        year = data.get("year")
        month = data.get("month")
        prices_data = data.get("prices", {})  # {grupo: {dias: price}}
        username = request.session.get('username', 'admin')
        
        if not location or not year or not month or not prices_data:
            return JSONResponse({"ok": False, "error": "Missing required fields"}, status_code=400)
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Guardar na tabela price_history
                prices_json = json.dumps(prices_data)
                conn.execute(
                    """
                    INSERT INTO price_history (history_type, year, month, location, prices_data, saved_by)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    ('current_prices', year, month, location, prices_json, username)
                )
                conn.commit()
                logging.info(f"‚úÖ Current prices saved: {location} {year}-{month:02d} by {username}")
                return JSONResponse({"ok": True, "message": "Current prices saved successfully"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving current prices: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/prices/history/list")
async def list_price_history(request: Request):
    """Listar hist√≥rico de pre√ßos (Current e Automated)"""
    require_auth(request)
    
    try:
        location = request.query_params.get("location")
        history_type = request.query_params.get("type")  # 'current_prices' ou 'automated_prices'
        limit = int(request.query_params.get("limit", 50))
        
        with _db_lock:
            conn = _db_connect()
            try:
                query = """
                    SELECT id, history_type, year, month, location, saved_at, saved_by
                    FROM price_history
                    WHERE 1=1
                """
                params = []
                
                if location:
                    query += " AND location = ?"
                    params.append(location)
                
                if history_type:
                    query += " AND history_type = ?"
                    params.append(history_type)
                
                query += " ORDER BY saved_at DESC LIMIT ?"
                params.append(limit)
                
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
                
                history = []
                for row in rows:
                    history.append({
                        "id": row[0],
                        "type": row[1],
                        "year": row[2],
                        "month": row[3],
                        "location": row[4],
                        "saved_at": row[5],
                        "saved_by": row[6]
                    })
                
                return JSONResponse({"ok": True, "history": history})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error listing price history: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/prices/history/load/{history_id}")
async def load_price_history(request: Request, history_id: int):
    """Carregar pre√ßos de um hist√≥rico espec√≠fico"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT history_type, year, month, location, prices_data, saved_at, saved_by
                    FROM price_history
                    WHERE id = ?
                    """,
                    (history_id,)
                )
                row = cursor.fetchone()
                
                if not row:
                    return JSONResponse({"ok": False, "error": "History not found"}, status_code=404)
                
                prices_data = json.loads(row[4])
                
                return JSONResponse({
                    "ok": True,
                    "data": {
                        "type": row[0],
                        "year": row[1],
                        "month": row[2],
                        "location": row[3],
                        "prices": prices_data,
                        "saved_at": row[5],
                        "saved_by": row[6]
                    }
                })
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error loading price history: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/prices/history/update/{history_id}")
async def update_price_history(request: Request, history_id: int):
    """Atualizar pre√ßos de um hist√≥rico espec√≠fico"""
    require_auth(request)
    
    try:
        data = await request.json()
        prices_data = data.get('prices', {})
        username = request.state.user.get('username', 'admin') if hasattr(request.state, 'user') else 'admin'
        
        logging.info(f"üíæ Updating price history {history_id} by {username}")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Update existing history with new timestamp
                conn.execute(
                    """
                    UPDATE price_history 
                    SET prices_data = ?,
                        saved_at = CURRENT_TIMESTAMP,
                        saved_by = ?
                    WHERE id = ?
                    """,
                    (json.dumps(prices_data), username, history_id)
                )
                conn.commit()
                
                logging.info(f"‚úÖ Price history {history_id} updated successfully")
                return JSONResponse({"ok": True, "message": "History updated successfully"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error updating price history: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# API ENDPOINTS - AI LEARNING DATA & USER SETTINGS
# ============================================================

@app.post("/api/ai/learning/save")
async def save_ai_learning(request: Request):
    """Salvar dados de AI learning (ajustes manuais) na base de dados"""
    require_auth(request)
    
    try:
        data = await request.json()
        adjustment = data.get("adjustment", {})
        
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute(
                    """
                    INSERT INTO ai_learning_data 
                    (grupo, days, location, original_price, new_price, user)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (
                        adjustment.get("group"),
                        adjustment.get("days"),
                        adjustment.get("location"),
                        adjustment.get("originalPrice"),
                        adjustment.get("newPrice"),
                        "admin"  # TODO: pegar do session
                    )
                )
                conn.commit()
                
                logging.info(f"‚úÖ AI Learning saved: {adjustment.get('group')}/{adjustment.get('days')}d = {adjustment.get('newPrice')}‚Ç¨")
                return JSONResponse({"ok": True})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving AI learning: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/ai/learning/load")
async def load_ai_learning(request: Request):
    """Carregar dados de AI learning da base de dados"""
    require_auth(request)
    
    try:
        location = request.query_params.get("location")
        limit = int(request.query_params.get("limit", 100))
        
        with _db_lock:
            conn = _db_connect()
            try:
                query = "SELECT grupo, days, location, original_price, new_price, timestamp FROM ai_learning_data WHERE 1=1"
                params = []
                
                if location:
                    query += " AND location = ?"
                    params.append(location)
                
                query += " ORDER BY timestamp DESC LIMIT ?"
                params.append(limit)
                
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
                
                adjustments = []
                for row in rows:
                    adjustments.append({
                        "group": row[0],
                        "days": row[1],
                        "location": row[2],
                        "originalPrice": row[3],
                        "newPrice": row[4],
                        "timestamp": row[5]
                    })
                
                return JSONResponse({"ok": True, "adjustments": adjustments})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/user-settings/save")
async def save_user_settings(request: Request):
    """Salvar configura√ß√µes do usu√°rio (substituir localStorage)"""
    require_auth(request)
    
    try:
        data = await request.json()
        user_key = data.get("user_key", "default")
        settings = data.get("settings", {})
        
        with _db_lock:
            conn = _db_connect()
            try:
                for key, value in settings.items():
                    # Serializar valor como JSON
                    value_json = json.dumps(value) if not isinstance(value, str) else value
                    
                    query = _convert_query_for_db("""
                        INSERT OR REPLACE INTO user_settings 
                        (user_key, setting_key, setting_value, updated_at)
                        VALUES (?, ?, ?, datetime('now'))
                    """, conn)
                    
                    if conn.__class__.__module__ == 'psycopg2.extensions':
                        with conn.cursor() as cur:
                            cur.execute(query, (user_key, key, value_json))
                    else:
                        conn.execute(query, (user_key, key, value_json))
                
                conn.commit()
                logging.info(f"‚úÖ User settings saved: {len(settings)} keys for {user_key}")
                return JSONResponse({"ok": True})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving user settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/user-settings/load")
async def load_user_settings(request: Request):
    """Carregar configura√ß√µes do usu√°rio da base de dados"""
    require_auth(request)
    
    try:
        user_key = request.query_params.get("user_key", "default")
        
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT setting_key, setting_value 
                    FROM user_settings 
                    WHERE user_key = ?
                    ORDER BY updated_at DESC
                    """,
                    (user_key,)
                )
                rows = cursor.fetchall()
                
                settings = {}
                for row in rows:
                    key = row[0]
                    value_str = row[1]
                    
                    # Tentar deserializar JSON
                    try:
                        settings[key] = json.loads(value_str)
                    except:
                        settings[key] = value_str
                
                return JSONResponse({"ok": True, "settings": settings})
            finally:
                conn.close()
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# API ENDPOINTS - EXTERNAL AI INTEGRATION (Claude/GPT)
# ============================================================

@app.post("/api/ai/external-analysis")
async def external_ai_analysis(request: Request):
    """
    An√°lise de pricing usando AI externa (Claude Sonnet 3.5 ou GPT-4)
    Requer ANTHROPIC_API_KEY ou OPENAI_API_KEY no .env
    """
    require_auth(request)
    
    try:
        data = await request.json()
        
        group = data.get("group")
        days = data.get("days")
        location = data.get("location")
        current_price = data.get("current_price")
        competitors = data.get("competitors", [])
        provider = data.get("provider", "claude")  # claude ou openai
        min_price_day = data.get("min_price_day")  # Pre√ßo m√≠nimo por dia
        min_price_month = data.get("min_price_month")  # Pre√ßo m√≠nimo para ‚â•30 dias
        
        if not all([group, days, location, current_price]):
            return JSONResponse({
                "ok": False,
                "error": "Missing required fields: group, days, location, current_price"
            }, status_code=400)
        
        # Importar AI assistant
        try:
            from ai_pricing_assistant import get_ai_assistant
            
            ai_assistant = get_ai_assistant(provider=provider)
            
            # Verificar se AI est√° dispon√≠vel
            status = ai_assistant.get_status()
            
            if not status['available']:
                return JSONResponse({
                    "ok": False,
                    "error": f"AI provider '{provider}' not available. Please install required libraries and set API key.",
                    "status": status,
                    "fallback_used": True
                }, status_code=503)
            
            # Fazer an√°lise com valida√ß√£o de pre√ßo m√≠nimo
            analysis = ai_assistant.analyze_market_positioning(
                group=group,
                days=int(days),
                location=location,
                current_price=float(current_price),
                competitors=competitors,
                min_price_day=float(min_price_day) if min_price_day else None,
                min_price_month=float(min_price_month) if min_price_month else None
            )
            
            logging.info(f"‚úÖ AI Analysis completed: {group}/{days}d using {analysis.get('ai_provider', 'unknown')}")
            
            return JSONResponse({
                "ok": True,
                "analysis": analysis,
                "provider": provider,
                "status": status
            })
            
        except ImportError as e:
            logging.error(f"‚ùå AI assistant import error: {str(e)}")
            return JSONResponse({
                "ok": False,
                "error": f"AI assistant not available: {str(e)}",
                "hint": "Install required libraries: pip install anthropic openai"
            }, status_code=500)
        
    except Exception as e:
        logging.error(f"‚ùå External AI analysis error: {str(e)}")
        import traceback
        return JSONResponse({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, status_code=500)

@app.get("/api/ai/status")
async def ai_status(request: Request):
    """Verifica status da integra√ß√£o com AI externa"""
    require_auth(request)
    
    try:
        from ai_pricing_assistant import get_ai_assistant
        
        # Testar ambos providers
        claude_assistant = get_ai_assistant(provider="claude")
        claude_status = claude_assistant.get_status()
        
        openai_assistant = get_ai_assistant(provider="openai")
        openai_status = openai_assistant.get_status()
        
        return JSONResponse({
            "ok": True,
            "claude": claude_status,
            "openai": openai_status,
            "recommended": "claude" if claude_status['available'] else ("openai" if openai_status['available'] else "none"),
            "env_keys": {
                "anthropic": "ANTHROPIC_API_KEY" in os.environ,
                "openai": "OPENAI_API_KEY" in os.environ
            }
        })
    except Exception as e:
        return JSONResponse({
            "ok": False,
            "error": str(e),
            "hint": "AI integration not configured. Install: pip install anthropic openai"
        }, status_code=500)

@app.get("/api/vehicles/with-originals")
async def get_vehicles_with_originals(request: Request):
    """Retorna ve√≠culos com nomes originais do scraping"""
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        print("[VEHICLES API] Iniciando...", file=sys.stderr, flush=True)
        
        # Recarregar m√≥dulo para pegar altera√ß√µes mais recentes
        import carjet_direct
        import importlib
        importlib.reload(carjet_direct)
        from carjet_direct import VEHICLES
        
        print(f"[VEHICLES API] VEHICLES importado: {len(VEHICLES)} ve√≠culos", file=sys.stderr, flush=True)
        
        # Buscar nomes originais do hist√≥rico
        rows = []
        try:
            with _db_lock:
                conn = _db_connect()  # Use hybrid connection
                try:
                    # Pegar exemplos recentes de cada carro
                    query = """
                        SELECT DISTINCT car 
                        FROM price_snapshots 
                        WHERE ts >= datetime('now', '-7 days')
                        ORDER BY car
                    """
                    rows = conn.execute(query).fetchall()
                    print(f"[VEHICLES API] Encontrados {len(rows)} carros no hist√≥rico", file=sys.stderr, flush=True)
                finally:
                    conn.close()
        except Exception as e:
            print(f"[VEHICLES API] Aviso: N√£o foi poss√≠vel buscar hist√≥rico: {e}", file=sys.stderr, flush=True)
            print(f"[VEHICLES API] Continuando com lista de ve√≠culos sem dados de scraping...", file=sys.stderr, flush=True)
        
        # Criar mapeamento de originais
        originals_map = {}
        for row in rows:
            original_name = row[0]  # Nome como veio do scraping
            # Limpar para encontrar no VEHICLES
            import re
            clean = original_name.lower().strip()
            clean = re.sub(r'\s+(ou\s*similar|or\s*similar).*$', '', clean, flags=re.IGNORECASE)
            clean = re.sub(r'\s*\|\s*.*$', '', clean)
            clean = re.sub(r'\s+(pequeno|m√©dio|medio|grande|compacto|economico|econ√¥mico).*$', '', clean, flags=re.IGNORECASE)
            clean = re.sub(r'\s+', ' ', clean).strip()
            
            if clean in VEHICLES:
                originals_map[clean] = {
                    'original': original_name,
                    'clean': clean,
                    'category': VEHICLES[clean]
                }
        
        # Adicionar ve√≠culos que n√£o t√™m dados de scraping
        for clean_name, category in VEHICLES.items():
            if clean_name not in originals_map:
                originals_map[clean_name] = {
                    'original': f'{clean_name} (sem dados recentes)',
                    'clean': clean_name,
                    'category': category
                }
        
        print(f"[VEHICLES API] Retornando {len(originals_map)} ve√≠culos", file=sys.stderr, flush=True)
        
        return _no_store_json({
            "ok": True,
            "vehicles": originals_map,
            "total": len(originals_map)
        })
        
    except Exception as e:
        import traceback
        print(f"[VEHICLES API] ERRO: {e}", file=sys.stderr, flush=True)
        traceback.print_exc()
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# VEHICLE PHOTOS MANAGEMENT
# ============================================================

def _ensure_vehicle_photos_table():
    """Garante que a tabela de fotos de ve√≠culos existe"""
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("""
                    CREATE TABLE IF NOT EXISTS vehicle_photos (
                        vehicle_name TEXT PRIMARY KEY,
                        photo_data BLOB,
                        photo_url TEXT,
                        content_type TEXT,
                        uploaded_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Migration: Ensure columns exist (PostgreSQL)
                is_postgres = con.__class__.__module__ == 'psycopg2.extensions'
                if is_postgres:
                    try:
                        with con.cursor() as cur:
                            # Add photo_url if missing
                            cur.execute("""
                                ALTER TABLE vehicle_photos 
                                ADD COLUMN IF NOT EXISTS photo_url TEXT
                            """)
                            # Add uploaded_at if missing
                            cur.execute("""
                                ALTER TABLE vehicle_photos 
                                ADD COLUMN IF NOT EXISTS uploaded_at TEXT DEFAULT CURRENT_TIMESTAMP
                            """)
                    except Exception as e:
                        import logging
                        logging.warning(f"‚ö†Ô∏è Could not add vehicle_photos columns: {e}")
                
                con.commit()
            finally:
                con.close()
    except Exception as e:
        print(f"Erro ao criar tabela vehicle_photos: {e}")

def _ensure_vehicle_name_overrides_table():
    """Garante que a tabela de nomes editados existe"""
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("""
                    CREATE TABLE IF NOT EXISTS vehicle_name_overrides (
                        original_name TEXT PRIMARY KEY,
                        edited_name TEXT NOT NULL,
                        updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                con.commit()
            finally:
                con.close()
    except Exception as e:
        print(f"Erro ao criar tabela vehicle_name_overrides: {e}")

def _ensure_vehicle_images_table():
    """Garante que a tabela de imagens de ve√≠culos existe"""
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("""
                    CREATE TABLE IF NOT EXISTS vehicle_images (
                        vehicle_key TEXT PRIMARY KEY,
                        image_data BLOB NOT NULL,
                        content_type TEXT DEFAULT 'image/jpeg',
                        source_url TEXT,
                        downloaded_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Migration: Add source_url and downloaded_at if not exists (PostgreSQL)
                is_postgres = con.__class__.__module__ == 'psycopg2.extensions'
                if is_postgres:
                    try:
                        with con.cursor() as cur:
                            cur.execute("""
                                ALTER TABLE vehicle_images 
                                ADD COLUMN IF NOT EXISTS source_url TEXT
                            """)
                            cur.execute("""
                                ALTER TABLE vehicle_images 
                                ADD COLUMN IF NOT EXISTS downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            """)
                    except Exception as e:
                        import logging
                        logging.warning(f"‚ö†Ô∏è Could not add vehicle_images columns: {e}")
                
                con.commit()
            finally:
                con.close()
    except Exception as e:
        print(f"Erro ao criar tabela vehicle_images: {e}")

@app.on_event("startup")
async def startup_vehicle_photos():
    """Inicializar tabelas de ve√≠culos na startup"""
    _ensure_vehicle_photos_table()
    _ensure_vehicle_name_overrides_table()
    _ensure_vehicle_images_table()

@app.on_event("startup")
async def startup_migrate_automated_reports():
    """Migrate automated reports settings from user_settings to price_automation_settings"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                
                # Check if old config exists in user_settings
                old_cursor = conn.execute(
                    "SELECT setting_value FROM user_settings WHERE setting_key = 'automated_reports' LIMIT 1"
                )
                old_row = old_cursor.fetchone()
                
                if old_row and old_row[0]:
                    # Check if new config already exists
                    new_cursor = conn.execute(
                        "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                    )
                    new_row = new_cursor.fetchone()
                    
                    if not new_row:
                        # Migrate old config to new location
                        placeholder = "%s" if is_postgres else "?"
                        insert_query = f"""
                            INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                            VALUES ({placeholder}, {placeholder}, 'json', CURRENT_TIMESTAMP)
                        """
                        conn.execute(insert_query, ('automatedReportsSettings', old_row[0]))
                        conn.commit()
                        logging.info("‚úÖ [STARTUP] Migrated automated reports settings from user_settings to price_automation_settings")
                        print("‚úÖ [STARTUP] Migrated automated reports settings successfully", flush=True)
                    else:
                        logging.debug("[STARTUP] Automated reports settings already migrated")
                else:
                    logging.debug("[STARTUP] No automated reports settings found in user_settings")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå [STARTUP] Failed to migrate automated reports settings: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())

# DESATIVADO: Esta fun√ß√£o causava problemas ao sobrescrever numera√ß√£o ap√≥s deploy
# Numera√ß√£o agora √© gerida APENAS manualmente via interface web
# @app.on_event("startup")
# async def startup_update_dr_numbering():
#     """Atualizar configura√ß√£o de numera√ß√£o DR para DR40/2025"""
#     # FUN√á√ÉO DESATIVADA - Numera√ß√£o gerida manualmente
#     pass

@app.on_event("startup")
async def startup_automated_scheduler():
    """ü§ñ Iniciar sistema de agendamento autom√°tico de relat√≥rios"""
    print("="*80, flush=True)
    print("ü§ñ INITIALIZING AUTOMATED SCHEDULER", flush=True)
    print("="*80, flush=True)
    try:
        logging.info("ü§ñ Starting automated reports scheduler...")
        print("üì¶ Importing automated_scheduler module...", flush=True)
        from automated_scheduler import setup_scheduled_tasks
        print("‚úÖ Module imported successfully", flush=True)
        print("üîß Calling setup_scheduled_tasks()...", flush=True)
        setup_scheduled_tasks()
        print("‚úÖ SCHEDULER INITIALIZED SUCCESSFULLY!", flush=True)
        logging.info("‚úÖ Automated scheduler initialized successfully")
    except Exception as e:
        print(f"‚ùå SCHEDULER INITIALIZATION FAILED: {str(e)}", flush=True)
        logging.error(f"‚ùå Failed to initialize automated scheduler: {str(e)}")
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str, flush=True)
        logging.error(traceback_str)

@app.on_event("shutdown")
async def shutdown_automated_scheduler():
    """üõë Desligar scheduler ao parar aplica√ß√£o"""
    try:
        logging.info("üõë Shutting down automated scheduler...")
        from automated_scheduler import shutdown_scheduler
        shutdown_scheduler()
        logging.info("‚úÖ Automated scheduler stopped")
    except Exception as e:
        logging.error(f"‚ùå Error stopping scheduler: {str(e)}")

@app.post("/api/scheduler/reload")
async def reload_scheduler(request: Request):
    """üîÑ Recarregar configura√ß√µes do scheduler (quando user salva settings)"""
    require_auth(request)
    
    try:
        logging.info("üîÑ Reloading scheduler configuration...")
        from automated_scheduler import setup_scheduled_tasks
        setup_scheduled_tasks()
        return JSONResponse({
            "ok": True,
            "message": "Scheduler recarregado com sucesso"
        })
    except Exception as e:
        logging.error(f"‚ùå Error reloading scheduler: {str(e)}")
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

@app.get("/api/scheduler/status")
async def get_scheduler_status(request: Request):
    """üìä Obter status do scheduler e pr√≥ximas execu√ß√µes"""
    require_auth(request)
    
    try:
        from automated_scheduler import scheduler
        
        if scheduler is None:
            return JSONResponse({
                "ok": False,
                "running": False,
                "message": "Scheduler n√£o inicializado"
            })
        
        jobs_info = []
        for job in scheduler.get_jobs():
            jobs_info.append({
                "id": job.id,
                "name": job.name,
                "next_run": str(job.next_run_time) if job.next_run_time else None,
                "trigger": str(job.trigger)
            })
        
        return JSONResponse({
            "ok": True,
            "running": True,
            "job_count": len(jobs_info),
            "jobs": jobs_info,
            "message": f"Scheduler ativo com {len(jobs_info)} jobs agendados"
        })
    except Exception as e:
        logging.error(f"‚ùå Error getting scheduler status: {str(e)}")
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

@app.post("/api/scheduler/test-automated-search")
async def test_automated_search(request: Request):
    """üß™ Testar pesquisa autom√°tica - salva placeholders na BD"""
    require_auth(request)
    
    try:
        print("="*80, flush=True)
        print("üß™ TEST: Manual automated search save", flush=True)
        print("="*80, flush=True)
        
        from automated_scheduler import save_automated_search_placeholder
        
        # Get request body
        body = await request.json()
        location = body.get('location', 'Albufeira')
        days = body.get('days', [1, 3, 7])
        
        print(f"üìç Test Location: {location}", flush=True)
        print(f"üìÖ Test Days: {days}", flush=True)
        
        logging.info(f"üß™ Testing automated search: {location}, days: {days}")
        
        # Execute placeholder save
        success = save_automated_search_placeholder(location, days)
        
        if success:
            print(f"‚úÖ TEST SUCCESSFUL - Searches saved to history!", flush=True)
            print(f"‚Üí Check Automated Pricing ‚Üí Search History", flush=True)
            print("="*80, flush=True)
            return JSONResponse({
                "ok": True,
                "message": f"‚úÖ Pesquisa autom√°tica salva: {location}, {len(days)} dias. Verifica o hist√≥rico!",
                "location": location,
                "days": days
            })
        else:
            print(f"‚ùå TEST FAILED - Could not save to database", flush=True)
            print("="*80, flush=True)
            return JSONResponse({
                "ok": False,
                "error": "Falha ao salvar pesquisa autom√°tica"
            }, status_code=500)
    except Exception as e:
        print(f"‚ùå TEST ERROR: {str(e)}", flush=True)
        logging.error(f"‚ùå Error testing automated search: {str(e)}")
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str, flush=True)
        logging.error(traceback_str)
        print("="*80, flush=True)
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

@app.post("/api/vehicles/{vehicle_name}/photo/upload")
async def upload_vehicle_photo(vehicle_name: str, request: Request, file: UploadFile = File(...)):
    """Upload de foto para um ve√≠culo"""
    require_auth(request)
    try:
        # Ler dados do arquivo
        photo_data = await file.read()
        content_type = file.content_type or 'image/jpeg'
        
        # Salvar no banco (AMBAS as tabelas para compatibilidade)
        _ensure_vehicle_photos_table()
        _ensure_vehicle_images_table()
        
        vehicle_key = vehicle_name.lower().strip()
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Salvar em vehicle_photos
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                            VALUES (%s, %s, %s, NULL)
                            ON CONFLICT (vehicle_name) DO UPDATE SET
                                photo_data = EXCLUDED.photo_data,
                                content_type = EXCLUDED.content_type
                        """, (vehicle_key, photo_data, content_type))
                else:
                    # SQLite
                    conn.execute("""
                        INSERT OR REPLACE INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                        VALUES (?, ?, ?, NULL)
                    """, (vehicle_key, photo_data, content_type))
                
                # Salvar tamb√©m em vehicle_images para compatibilidade
                conn.execute("""
                    INSERT OR REPLACE INTO vehicle_images (vehicle_key, image_data, content_type, source_url)
                    VALUES (?, ?, ?, NULL)
                """, (vehicle_key, photo_data, content_type))
                
                conn.commit()
            finally:
                conn.close()
        
        return _no_store_json({
            "ok": True,
            "message": "Foto enviada com sucesso",
            "vehicle": vehicle_name,
            "size": len(photo_data)
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vehicles/{vehicle_name}/photo/from-url")
async def download_vehicle_photo_from_url(vehicle_name: str, request: Request):
    """Baixar foto de URL e salvar no banco"""
    require_auth(request)
    try:
        body = await request.json()
        photo_url = body.get('url', '').strip()
        
        if not photo_url:
            return _no_store_json({"ok": False, "error": "URL n√£o fornecida"}, 400)
        
        # Baixar imagem
        import httpx
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(photo_url)
            response.raise_for_status()
            
            photo_data = response.content
            content_type = response.headers.get('content-type', 'image/jpeg')
        
        # Salvar no banco (AMBAS as tabelas para compatibilidade)
        _ensure_vehicle_photos_table()
        _ensure_vehicle_images_table()
        
        vehicle_key = vehicle_name.lower().strip()
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Salvar em vehicle_photos
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                            VALUES (%s, %s, %s, %s)
                            ON CONFLICT (vehicle_name) DO UPDATE SET
                                photo_data = EXCLUDED.photo_data,
                                content_type = EXCLUDED.content_type,
                                photo_url = EXCLUDED.photo_url
                        """, (vehicle_key, photo_data, content_type, photo_url))
                else:
                    # SQLite
                    conn.execute("""
                        INSERT OR REPLACE INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                        VALUES (?, ?, ?, ?)
                    """, (vehicle_key, photo_data, content_type, photo_url))
                
                # Salvar tamb√©m em vehicle_images para compatibilidade
                conn.execute("""
                    INSERT OR REPLACE INTO vehicle_images (vehicle_key, image_data, content_type, source_url)
                    VALUES (?, ?, ?, ?)
                """, (vehicle_key, photo_data, content_type, photo_url))
                
                conn.commit()
            finally:
                conn.close()
        
        return _no_store_json({
            "ok": True,
            "message": "Foto baixada e salva com sucesso",
            "vehicle": vehicle_name,
            "url": photo_url,
            "size": len(photo_data)
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/vehicles/uncategorized")
async def get_uncategorized_vehicles(request: Request):
    """Retorna ve√≠culos que n√£o est√£o no dicion√°rio VEHICLES"""
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        from carjet_direct import VEHICLES
        import re
        
        # Buscar carros √∫nicos do hist√≥rico recente
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                
                # Use appropriate date function
                if is_postgres:
                    query = """
                        SELECT DISTINCT car 
                        FROM price_snapshots 
                        WHERE ts >= CURRENT_TIMESTAMP - INTERVAL '30 days'
                        ORDER BY car
                    """
                    with conn.cursor() as cur:
                        cur.execute(query)
                        rows = cur.fetchall()
                else:
                    query = """
                        SELECT DISTINCT car 
                        FROM price_snapshots 
                        WHERE ts >= datetime('now', '-30 days')
                        ORDER BY car
                    """
                    rows = conn.execute(query).fetchall()
            finally:
                conn.close()
        
        uncategorized = []
        for row in rows:
            original_name = row[0]
            
            # Usar a MESMA fun√ß√£o de limpeza que o scraping
            clean = clean_car_name(original_name)
            # VEHICLES est√° em lowercase, converter para compara√ß√£o
            clean_lower = clean.lower()
            
            # Se n√£o est√° no VEHICLES, adicionar √† lista
            if clean and clean_lower not in VEHICLES:
                # Extrair marca
                parts = clean.split(' ')
                brand = parts[0] if parts else ''
                model = ' '.join(parts[1:]) if len(parts) > 1 else ''
                
                uncategorized.append({
                    'original': original_name,
                    'clean': clean,
                    'brand': brand,
                    'model': model,
                    'suggested_category': detect_category_suggestion(clean)
                })
        
        # Remover duplicados
        seen = set()
        unique = []
        for item in uncategorized:
            if item['clean'] not in seen:
                seen.add(item['clean'])
                unique.append(item)
        
        return _no_store_json({
            "ok": True,
            "uncategorized": unique,
            "total": len(unique)
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

def detect_category_suggestion(car_name: str) -> str:
    """Sugere categoria baseado no nome do carro"""
    from carjet_direct import detect_category_from_car
    try:
        return detect_category_from_car(car_name, '')
    except:
        return 'ECONOMY'

# ============================================================
# EXPORT/IMPORT CONFIGURATION
# ============================================================

@app.get("/api/export/config")
async def export_configuration(request: Request):
    """
    Exporta configura√ß√µes COMPLETAS do sistema de Vehicles:
    - VEHICLES (mapeamento carro ‚Üí categoria)
    - vehicle_name_overrides (nomes editados)
    - car_groups (grupos manuais)
    - vehicle_photos (fotos em base64)
    - vehicle_images (imagens em base64)
    - suppliers (fornecedores)
    - users (utilizadores)
    """
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        from carjet_direct import VEHICLES, SUPPLIER_MAP
        import base64
        
        print("[EXPORT] Iniciando exporta√ß√£o completa...")
        
        # 1. Exportar VEHICLES (mapeamento principal)
        vehicles_data = dict(VEHICLES)
        print(f"[EXPORT] VEHICLES: {len(vehicles_data)} carros")
        
        # 2. Exportar vehicle_name_overrides (nomes editados)
        name_overrides_data = []
        try:
            _ensure_vehicle_name_overrides_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    rows = conn.execute("""
                        SELECT original_name, edited_name, updated_at
                        FROM vehicle_name_overrides
                    """).fetchall()
                    
                    for row in rows:
                        updated_at = row[2]
                        # Convert datetime to ISO string for JSON serialization
                        updated_at_str = updated_at if isinstance(updated_at, str) else (updated_at.isoformat() if updated_at else None)
                        name_overrides_data.append({
                            "original_name": row[0],
                            "edited_name": row[1],
                            "updated_at": updated_at_str
                        })
                finally:
                    conn.close()
            print(f"[EXPORT] Name Overrides: {len(name_overrides_data)} registos")
        except Exception as e:
            print(f"[EXPORT] Warning: Could not export name_overrides: {e}")
        
        # 3. Exportar car_groups (grupos manuais)
        car_groups_data = []
        try:
            with _db_lock:
                conn = _db_connect()
                try:
                    rows = conn.execute("""
                        SELECT code, name, model, brand, category, doors, seats, 
                               transmission, luggage, photo_url, enabled
                        FROM car_groups
                    """).fetchall()
                    
                    for row in rows:
                        car_groups_data.append({
                            "code": row[0],
                            "name": row[1],
                            "model": row[2],
                            "brand": row[3],
                            "category": row[4],
                            "doors": row[5],
                            "seats": row[6],
                            "transmission": row[7],
                            "luggage": row[8],
                            "photo_url": row[9],
                            "enabled": row[10]
                        })
                finally:
                    conn.close()
            print(f"[EXPORT] Car Groups: {len(car_groups_data)} grupos")
        except Exception as e:
            print(f"[EXPORT] Warning: Could not export car_groups: {e}")
        
        # 4. Exportar vehicle_photos (fotos em base64)
        photos_data = {}
        try:
            _ensure_vehicle_photos_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    rows = conn.execute("""
                        SELECT vehicle_name, photo_data, content_type, photo_url, uploaded_at
                        FROM vehicle_photos
                    """).fetchall()
                    
                    for row in rows:
                        vehicle_name = row[0]
                        photo_data = row[1]
                        content_type = row[2] or "image/jpeg"
                        photo_url = row[3]
                        uploaded_at = row[4]
                        
                        if photo_data:
                            # Converter BLOB para base64
                            photo_base64 = base64.b64encode(photo_data).decode('utf-8')
                            # Convert datetime to ISO string for JSON serialization
                            uploaded_at_str = uploaded_at if isinstance(uploaded_at, str) else (uploaded_at.isoformat() if uploaded_at else None)
                            photos_data[vehicle_name] = {
                                "data": photo_base64,
                                "content_type": content_type,
                                "url": photo_url,
                                "uploaded_at": uploaded_at_str,
                                "size": len(photo_data)
                            }
                finally:
                    conn.close()
            print(f"[EXPORT] Photos: {len(photos_data)} fotos")
        except Exception as e:
            print(f"[EXPORT] Warning: Could not export photos: {e}")
        
        # 5. Exportar vehicle_images (imagens em base64)
        images_data = {}
        try:
            _ensure_vehicle_images_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    rows = conn.execute("""
                        SELECT vehicle_key, image_data, source_url, downloaded_at
                        FROM vehicle_images
                    """).fetchall()
                    
                    for row in rows:
                        vehicle_key = row[0]
                        image_data = row[1]
                        source_url = row[2]
                        downloaded_at = row[3]
                        
                        if image_data:
                            # Converter BLOB para base64
                            image_base64 = base64.b64encode(image_data).decode('utf-8')
                            # Convert datetime to ISO string for JSON serialization
                            downloaded_at_str = downloaded_at if isinstance(downloaded_at, str) else (downloaded_at.isoformat() if downloaded_at else None)
                            images_data[vehicle_key] = {
                                "data": image_base64,
                                "source_url": source_url,
                                "downloaded_at": downloaded_at_str,
                                "size": len(image_data)
                            }
                finally:
                    conn.close()
            print(f"[EXPORT] Images: {len(images_data)} imagens")
        except Exception as e:
            print(f"[EXPORT] Warning: Could not export images: {e}")
        
        # 6. Exportar suppliers
        suppliers_data = dict(SUPPLIER_MAP)
        print(f"[EXPORT] Suppliers: {len(suppliers_data)} fornecedores")
        
        # 7. Exportar users
        users_data = []
        try:
            with _db_lock:
                conn = _db_connect()
                try:
                    rows = conn.execute("SELECT username, password_hash FROM users").fetchall()
                    users_data = [{"username": r[0], "password_hash": r[1]} for r in rows]
                finally:
                    conn.close()
            print(f"[EXPORT] Users: {len(users_data)} utilizadores")
        except Exception as e:
            print(f"[EXPORT] Warning: Could not export users: {e}")
        
        # Fun√ß√£o auxiliar para converter datetime recursivamente
        def convert_datetime_recursive(obj):
            """Converte recursivamente todos os datetime objects para ISO strings"""
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, dict):
                return {key: convert_datetime_recursive(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_datetime_recursive(item) for item in obj]
            else:
                return obj
        
        # Criar estrutura de export completa
        config = {
            "version": "2.0",  # Nova vers√£o com dados completos
            "exported_at": datetime.utcnow().isoformat(),
            "export_type": "vehicles_complete",
            "statistics": {
                "vehicles_count": len(vehicles_data),
                "name_overrides_count": len(name_overrides_data),
                "car_groups_count": len(car_groups_data),
                "photos_count": len(photos_data),
                "images_count": len(images_data),
                "suppliers_count": len(suppliers_data),
                "users_count": len(users_data),
                "total_photo_size_mb": sum(p.get("size", 0) for p in photos_data.values()) / (1024 * 1024),
                "total_image_size_mb": sum(i.get("size", 0) for i in images_data.values()) / (1024 * 1024)
            },
            "data": {
                "vehicles": vehicles_data,
                "name_overrides": name_overrides_data,
                "car_groups": car_groups_data,
                "photos": photos_data,
                "images": images_data,
                "suppliers": suppliers_data,
                "users": users_data
            }
        }
        
        # Converter todos os datetime recursivamente antes de serializar
        config = convert_datetime_recursive(config)
        
        # Retornar como JSON para download
        from fastapi.responses import Response
        import json
        
        json_content = json.dumps(config, indent=2, ensure_ascii=False)
        
        print(f"[EXPORT] Export completo! Tamanho: {len(json_content) / (1024 * 1024):.2f} MB")
        
        return Response(
            content=json_content,
            media_type="application/json",
            headers={
                "Content-Disposition": f"attachment; filename=vehicles_complete_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
            }
        )
        
    except Exception as e:
        import traceback
        print(f"[EXPORT] ERRO: {e}")
        print(traceback.format_exc())
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/import/config")
async def import_configuration(request: Request, file: UploadFile = File(...)):
    """
    Importa configura√ß√µes COMPLETAS do sistema de Vehicles:
    - VEHICLES (atualiza carjet_direct.py)
    - vehicle_name_overrides (nomes editados)
    - car_groups (grupos manuais)
    - vehicle_photos (fotos)
    - vehicle_images (imagens)
    - suppliers (fornecedores)
    - users (utilizadores)
    """
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        import json
        import base64
        
        print("[IMPORT] Iniciando importa√ß√£o completa...")
        
        # Ler ficheiro
        content = await file.read()
        config = json.loads(content)
        
        print(f"[IMPORT] Vers√£o do ficheiro: {config.get('version', 'unknown')}")
        
        # Detectar formato (v1.x ou v2.0)
        is_v2 = config.get("version", "").startswith("2.") and "data" in config
        
        # Extrair dados conforme vers√£o
        if is_v2:
            data = config["data"]
            vehicles_data = data.get("vehicles", {})
            name_overrides_data = data.get("name_overrides", [])
            car_groups_data = data.get("car_groups", [])
            photos_data = data.get("photos", {})
            images_data = data.get("images", {})
            suppliers_data = data.get("suppliers", {})
            users_data = data.get("users", [])
        else:
            # Formato antigo (v1.x)
            vehicles_data = config.get("vehicles", {})
            name_overrides_data = []
            car_groups_data = []
            photos_data = config.get("photos", {})
            images_data = {}
            suppliers_data = config.get("suppliers", {})
            users_data = config.get("users", [])
        
        # Validar
        if not vehicles_data:
            return _no_store_json({"ok": False, "error": "Ficheiro inv√°lido: falta 'vehicles'"}, 400)
        
        print(f"[IMPORT] Importando {len(vehicles_data)} ve√≠culos...")
        
        # 1. Importar VEHICLES (gerar c√≥digo Python)
        vehicles_code = "VEHICLES = {\n"
        for car, category in sorted(vehicles_data.items()):
            vehicles_code += f"    '{car}': '{category}',\n"
        vehicles_code += "}\n"
        
        # 2. Importar SUPPLIER_MAP (gerar c√≥digo Python)
        suppliers_code = ""
        if suppliers_data:
            suppliers_code = "\nSUPPLIER_MAP = {\n"
            for code, name in sorted(suppliers_data.items()):
                suppliers_code += f"    '{code}': '{name}',\n"
            suppliers_code += "}\n"
        
        # 3. Importar vehicle_name_overrides
        imported_overrides = 0
        if name_overrides_data:
            _ensure_vehicle_name_overrides_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    for override in name_overrides_data:
                        conn.execute("""
                            INSERT OR REPLACE INTO vehicle_name_overrides 
                            (original_name, edited_name, updated_at)
                            VALUES (?, ?, ?)
                        """, (
                            override["original_name"],
                            override["edited_name"],
                            override.get("updated_at", datetime.utcnow().isoformat())
                        ))
                        imported_overrides += 1
                    conn.commit()
                finally:
                    conn.close()
            print(f"[IMPORT] Name Overrides: {imported_overrides} importados")
        
        # 4. Importar car_groups
        imported_groups = 0
        if car_groups_data:
            with _db_lock:
                conn = _db_connect()
                try:
                    for group in car_groups_data:
                        query = _convert_query_for_db("""
                            INSERT OR REPLACE INTO car_groups 
                            (code, name, model, brand, category, doors, seats, 
                             transmission, luggage, photo_url, enabled)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, conn)
                        
                        params = (
                            group["code"],
                            group["name"],
                            group.get("model"),
                            group.get("brand"),
                            group.get("category"),
                            group.get("doors"),
                            group.get("seats"),
                            group.get("transmission"),
                            group.get("luggage"),
                            group.get("photo_url"),
                            group.get("enabled", 1)
                        )
                        
                        if conn.__class__.__module__ == 'psycopg2.extensions':
                            with conn.cursor() as cur:
                                cur.execute(query, params)
                        else:
                            conn.execute(query, params)
                        imported_groups += 1
                    conn.commit()
                finally:
                    conn.close()
            print(f"[IMPORT] Car Groups: {imported_groups} importados")
        
        # 5. Importar vehicle_photos
        imported_photos = 0
        if photos_data:
            _ensure_vehicle_photos_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    for vehicle_name, photo_info in photos_data.items():
                        # Converter base64 de volta para BLOB
                        photo_data = base64.b64decode(photo_info["data"])
                        content_type = photo_info.get("content_type", "image/jpeg")
                        photo_url = photo_info.get("url")
                        
                        if conn.__class__.__module__ == 'psycopg2.extensions':
                            # PostgreSQL
                            with conn.cursor() as cur:
                                cur.execute("""
                                    INSERT INTO vehicle_photos 
                                    (vehicle_name, photo_data, content_type, photo_url, updated_at)
                                    VALUES (%s, %s, %s, %s, NOW())
                                    ON CONFLICT (vehicle_name) DO UPDATE SET
                                        photo_data = EXCLUDED.photo_data,
                                        content_type = EXCLUDED.content_type,
                                        photo_url = EXCLUDED.photo_url,
                                        updated_at = NOW()
                                """, (vehicle_name, photo_data, content_type, photo_url))
                        else:
                            # SQLite
                            conn.execute("""
                                INSERT OR REPLACE INTO vehicle_photos 
                                (vehicle_name, photo_data, content_type, photo_url, updated_at)
                                VALUES (?, ?, ?, ?, datetime('now'))
                            """, (vehicle_name, photo_data, content_type, photo_url))
                        imported_photos += 1
                    conn.commit()
                finally:
                    conn.close()
            print(f"[IMPORT] Photos: {imported_photos} importadas")
        
        # 6. Importar vehicle_images
        imported_images = 0
        if images_data:
            _ensure_vehicle_images_table()
            with _db_lock:
                conn = _db_connect()
                try:
                    for vehicle_name, image_info in images_data.items():
                        # Converter base64 de volta para BLOB
                        image_data = base64.b64decode(image_info["data"])
                        source_url = image_info.get("source_url")
                        
                        query = _convert_query_for_db("""
                            INSERT OR REPLACE INTO vehicle_images 
                            (vehicle_name, image_data, source_url, updated_at)
                            VALUES (?, ?, ?, datetime('now'))
                        """, conn)
                        
                        if conn.__class__.__module__ == 'psycopg2.extensions':
                            with conn.cursor() as cur:
                                cur.execute(query, (vehicle_name, image_data, source_url))
                        else:
                            conn.execute(query, (vehicle_name, image_data, source_url))
                        imported_images += 1
                    conn.commit()
                finally:
                    conn.close()
            print(f"[IMPORT] Images: {imported_images} importadas")
        
        # 7. Importar users
        imported_users = 0
        if users_data:
            with _db_lock:
                conn = _db_connect()
                try:
                    for user in users_data:
                        password_hash = user.get("password_hash") or user.get("password")
                        query = _convert_query_for_db(
                            "INSERT OR REPLACE INTO users (username, password_hash) VALUES (?, ?)",
                            conn
                        )
                        
                        if conn.__class__.__module__ == 'psycopg2.extensions':
                            with conn.cursor() as cur:
                                cur.execute(query, (user["username"], password_hash))
                        else:
                            conn.execute(query, (user["username"], password_hash))
                        imported_users += 1
                    conn.commit()
                finally:
                    conn.close()
            print(f"[IMPORT] Users: {imported_users} importados")
        
        # Invalidar cache
        global _vehicles_last_update
        _vehicles_last_update = datetime.utcnow().isoformat()
        
        print(f"[IMPORT] Importa√ß√£o completa!")
        
        return _no_store_json({
            "ok": True,
            "message": "Configura√ß√£o importada com sucesso!",
            "imported": {
                "vehicles": len(vehicles_data),
                "name_overrides": imported_overrides,
                "car_groups": imported_groups,
                "photos": imported_photos,
                "images": imported_images,
                "suppliers": len(suppliers_data),
                "users": imported_users
            },
            "vehicles_code": vehicles_code,
            "suppliers_code": suppliers_code,
            "cache_invalidated": True,
            "updated_at": _vehicles_last_update,
            "instructions": "‚úÖ Dados importados! Copie o c√≥digo gerado e cole em carjet_direct.py se necess√°rio."
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# REAL-TIME UPDATE SYSTEM
# ============================================================

# Timestamp de √∫ltima atualiza√ß√£o de VEHICLES
_vehicles_last_update = datetime.utcnow().isoformat()

@app.post("/api/vehicles/notify-update")
async def notify_vehicles_update(request: Request):
    """Notifica que VEHICLES foi atualizado (para invalidar cache do frontend)"""
    require_auth(request)
    global _vehicles_last_update
    _vehicles_last_update = datetime.utcnow().isoformat()
    
    return _no_store_json({
        "ok": True,
        "updated_at": _vehicles_last_update,
        "message": "Cache invalidado. Frontend ser√° atualizado."
    })

@app.get("/api/vehicles/last-update")
async def get_vehicles_last_update():
    """Retorna timestamp da √∫ltima atualiza√ß√£o de VEHICLES"""
    return _no_store_json({
        "ok": True,
        "last_update": _vehicles_last_update
    })

@app.post("/api/vehicles/refresh")
async def refresh_vehicles(request: Request):
    """
    Faz scraping em Albufeira + Faro para verificar carros novos/atualizados
    Retorna lista de carros novos encontrados
    """
    require_auth(request)
    try:
        from datetime import datetime, timedelta
        from carjet_direct import scrape_carjet_direct, VEHICLES
        import random
        
        # Datas ALEAT√ìRIAS para scraping (3-10 dias no futuro)
        days_offset = random.randint(3, 10)
        start_date = datetime.now() + timedelta(days=days_offset)
        end_date = start_date + timedelta(days=7)
        
        print(f"[REFRESH] Usando datas aleat√≥rias: {start_date.strftime('%Y-%m-%d')} a {end_date.strftime('%Y-%m-%d')}")
        
        new_cars = []
        updated_cars = []
        total_scraped = 0
        
        # Scraping em Albufeira (SEM quick mode para scraping completo)
        print("[REFRESH] Fazendo scraping COMPLETO em Albufeira...")
        albufeira_results = scrape_carjet_direct("Albufeira", start_date, end_date, quick=0)
        total_scraped += len(albufeira_results)
        print(f"[REFRESH] Albufeira: {len(albufeira_results)} carros encontrados")
        
        # Scraping em Faro (SEM quick mode para scraping completo)
        print("[REFRESH] Fazendo scraping COMPLETO em Faro...")
        faro_results = scrape_carjet_direct("Faro", start_date, end_date, quick=0)
        total_scraped += len(faro_results)
        print(f"[REFRESH] Faro: {len(faro_results)} carros encontrados")
        
        # Combinar resultados
        all_results = albufeira_results + faro_results
        
        # Verificar carros novos
        for item in all_results:
            car_name = item.get('car', '').strip()
            if not car_name:
                continue
            
            car_clean = clean_car_name(car_name).lower()
            
            # Verificar se est√° no VEHICLES
            if car_clean not in VEHICLES:
                # Carro novo!
                category = item.get('category', '')
                photo_url = item.get('photo', '')
                
                new_cars.append({
                    'original_name': car_name,
                    'clean_name': car_clean,
                    'category': category,
                    'photo_url': photo_url,
                    'location': item.get('location', ''),
                    'price': item.get('price', '')
                })
        
        # Remover duplicados
        seen = set()
        unique_new_cars = []
        for car in new_cars:
            if car['clean_name'] not in seen:
                seen.add(car['clean_name'])
                unique_new_cars.append(car)
        
        return _no_store_json({
            "ok": True,
            "total_scraped": total_scraped,
            "new_cars_count": len(unique_new_cars),
            "new_cars": unique_new_cars,
            "message": f"Scraping completo! {total_scraped} carros encontrados, {len(unique_new_cars)} novos."
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, 500)

@app.post("/api/vehicles/download-all-photos")
async def download_all_photos_from_carjet(request: Request):
    """
    Faz scraping em Albufeira + Faro e baixa TODAS as fotos dos carros
    Mostra progresso em tempo real
    """
    require_auth(request)
    try:
        from datetime import datetime, timedelta
        from carjet_direct import scrape_carjet_direct
        import httpx
        
        # Datas aleat√≥rias (hoje + 3 a 10 dias)
        import random
        days_offset = random.randint(3, 10)
        start_date = datetime.now() + timedelta(days=days_offset)
        end_date = start_date + timedelta(days=7)
        
        print(f"[DOWNLOAD ALL PHOTOS] Iniciando scraping para {start_date.strftime('%Y-%m-%d')}...", flush=True)
        
        photos_downloaded = 0
        photos_failed = 0
        total_cars = 0
        
        # Scraping APENAS em Faro (mais r√°pido e suficiente para fotos)
        print("[DOWNLOAD ALL PHOTOS] Fazendo scraping COMPLETO em Faro...", flush=True)
        all_results = scrape_carjet_direct("Faro", start_date, end_date, quick=0)
        total_cars = len(all_results)
        print(f"[DOWNLOAD ALL PHOTOS] Faro: {total_cars} carros encontrados", flush=True)
        
        print(f"[DOWNLOAD ALL PHOTOS] Total de carros encontrados: {total_cars}", flush=True)
        
        # Baixar fotos
        with _db_lock:
            conn = _db_connect()
            try:
                for idx, item in enumerate(all_results, 1):
                    car_name = item.get('car', '').strip()
                    photo_url = item.get('photo', '').strip()
                    
                    if not car_name:
                        photos_failed += 1
                        print(f"[DOWNLOAD ALL PHOTOS] [{idx}/{total_cars}] ‚ùå Sem nome de carro", flush=True)
                        continue
                    
                    car_clean = clean_car_name(car_name).lower()
                    
                    if not photo_url:
                        photos_failed += 1
                        print(f"[DOWNLOAD ALL PHOTOS] [{idx}/{total_cars}] ‚ùå Sem URL de foto: {car_clean}", flush=True)
                        continue
                    
                    # IGNORAR placeholders (loading-car.png)
                    if 'loading-car.png' in photo_url:
                        photos_failed += 1
                        print(f"[DOWNLOAD ALL PHOTOS] [{idx}/{total_cars}] ‚è≠Ô∏è  Placeholder ignorado: {car_clean}", flush=True)
                        continue
                    
                    print(f"[DOWNLOAD ALL PHOTOS] [{idx}/{total_cars}] Baixando foto: {car_clean}", flush=True)
                    print(f"                      URL: {photo_url}", flush=True)
                    
                    try:
                        # Baixar foto
                        async with httpx.AsyncClient(timeout=30.0) as client:
                            photo_response = await client.get(photo_url)
                            if photo_response.status_code == 200:
                                photo_data = photo_response.content
                                
                                # Salvar na tabela vehicle_photos
                                if conn.__class__.__module__ == 'psycopg2.extensions':
                                    # PostgreSQL
                                    with conn.cursor() as cur:
                                        cur.execute("""
                                            INSERT INTO vehicle_photos (vehicle_name, photo_data, photo_url, updated_at)
                                            VALUES (%s, %s, %s, %s)
                                            ON CONFLICT (vehicle_name) DO UPDATE SET
                                                photo_data = EXCLUDED.photo_data,
                                                photo_url = EXCLUDED.photo_url,
                                                updated_at = EXCLUDED.updated_at
                                        """, (car_clean, photo_data, photo_url, datetime.now().isoformat()))
                                else:
                                    # SQLite
                                    conn.execute("""
                                        INSERT OR REPLACE INTO vehicle_photos (vehicle_name, photo_data, photo_url, updated_at)
                                        VALUES (?, ?, ?, ?)
                                    """, (car_clean, photo_data, photo_url, datetime.now().isoformat()))
                                
                                # Salvar na tabela vehicle_images tamb√©m
                                conn.execute("""
                                    INSERT OR REPLACE INTO vehicle_images (vehicle_name, image_data, image_url, updated_at)
                                    VALUES (?, ?, ?, ?)
                                """, (car_clean, photo_data, photo_url, datetime.now().isoformat()))
                                
                                conn.commit()
                                photos_downloaded += 1
                                
                                print(f"[DOWNLOAD ALL PHOTOS] ‚úÖ Foto salva: {car_clean} ({len(photo_data)} bytes)", flush=True)
                            else:
                                photos_failed += 1
                                print(f"[DOWNLOAD ALL PHOTOS] ‚ùå Erro HTTP {photo_response.status_code}: {car_clean}", flush=True)
                    except Exception as e:
                        photos_failed += 1
                        print(f"[DOWNLOAD ALL PHOTOS] ‚ùå Erro ao baixar {car_clean}: {e}", flush=True)
                        continue
            finally:
                conn.close()
        
        return _no_store_json({
            "ok": True,
            "total_cars": total_cars,
            "photos_downloaded": photos_downloaded,
            "photos_failed": photos_failed,
            "message": f"Download completo! {photos_downloaded} fotos baixadas, {photos_failed} falharam."
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, 500)

@app.post("/api/vehicles/{vehicle_name}/download-photo")
async def download_vehicle_photo_from_carjet(vehicle_name: str, request: Request):
    """
    Baixa a foto do CarJet para um ve√≠culo espec√≠fico e atualiza a ficha
    Faz scraping r√°pido para encontrar a foto mais recente
    """
    require_auth(request)
    try:
        from datetime import datetime, timedelta
        from carjet_direct import scrape_carjet_direct
        import httpx
        
        # Limpar nome do ve√≠culo
        car_clean = clean_car_name(vehicle_name).lower()
        
        # Fazer scraping r√°pido em Faro para encontrar o carro
        start_date = datetime.now()
        end_date = start_date + timedelta(days=7)
        
        print(f"[DOWNLOAD PHOTO] Procurando foto para: {car_clean}")
        
        # Tentar Faro primeiro
        results = scrape_carjet_direct("Faro", start_date, end_date, quick=1)
        
        # Se n√£o encontrar, tentar Albufeira
        if not results:
            results = scrape_carjet_direct("Albufeira", start_date, end_date, quick=1)
        
        # Procurar o carro nos resultados
        photo_url = None
        for item in results:
            car_name = item.get('car', '').strip()
            if not car_name:
                continue
            
            item_clean = clean_car_name(car_name).lower()
            
            if item_clean == car_clean or car_clean in item_clean or item_clean in car_clean:
                photo_url = item.get('photo', '')
                if photo_url:
                    print(f"[DOWNLOAD PHOTO] Foto encontrada: {photo_url}")
                    break
        
        if not photo_url:
            return _no_store_json({
                "ok": False,
                "error": f"Foto n√£o encontrada para '{vehicle_name}' no scraping do CarJet"
            }, 404)
        
        # Baixar a foto
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(photo_url)
            response.raise_for_status()
            photo_data = response.content
        
        # Salvar no banco de dados
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                now_func = "NOW()" if is_postgres else "datetime('now')"
                param_placeholder = "%s" if is_postgres else "?"
                
                # Verificar se j√° existe
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute(f"SELECT id FROM vehicle_photos WHERE vehicle_name = {param_placeholder}", (car_clean,))
                        existing = cur.fetchone()
                else:
                    existing = conn.execute(
                        f"SELECT id FROM vehicle_photos WHERE vehicle_name = {param_placeholder}",
                        (car_clean,)
                    ).fetchone()
                
                if existing:
                    # Atualizar
                    if is_postgres:
                        with conn.cursor() as cur:
                            cur.execute(
                                f"UPDATE vehicle_photos SET photo_data = {param_placeholder}, photo_url = {param_placeholder}, updated_at = {now_func} WHERE vehicle_name = {param_placeholder}",
                                (photo_data, photo_url, car_clean)
                            )
                    else:
                        conn.execute(
                            f"UPDATE vehicle_photos SET photo_data = {param_placeholder}, photo_url = {param_placeholder}, updated_at = {now_func} WHERE vehicle_name = {param_placeholder}",
                            (photo_data, photo_url, car_clean)
                        )
                else:
                    # Inserir novo
                    if is_postgres:
                        with conn.cursor() as cur:
                            cur.execute(
                                f"INSERT INTO vehicle_photos (vehicle_name, photo_data, photo_url, updated_at) VALUES ({param_placeholder}, {param_placeholder}, {param_placeholder}, {now_func})",
                                (car_clean, photo_data, photo_url)
                            )
                    else:
                        conn.execute(
                            f"INSERT INTO vehicle_photos (vehicle_name, photo_data, photo_url, updated_at) VALUES ({param_placeholder}, {param_placeholder}, {param_placeholder}, {now_func})",
                            (car_clean, photo_data, photo_url)
                        )
                
                conn.commit()
            finally:
                conn.close()
        
        # Tamb√©m salvar em vehicle_images para compatibilidade
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                now_func = "NOW()" if is_postgres else "datetime('now')"
                param_placeholder = "%s" if is_postgres else "?"
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute(f"SELECT id FROM vehicle_images WHERE vehicle_name = {param_placeholder}", (car_clean,))
                        existing = cur.fetchone()
                else:
                    existing = conn.execute(
                        f"SELECT id FROM vehicle_images WHERE vehicle_name = {param_placeholder}",
                        (car_clean,)
                    ).fetchone()
                
                if existing:
                    if is_postgres:
                        with conn.cursor() as cur:
                            cur.execute(
                                f"UPDATE vehicle_images SET image_data = {param_placeholder}, source_url = {param_placeholder}, updated_at = {now_func} WHERE vehicle_name = {param_placeholder}",
                                (photo_data, photo_url, car_clean)
                            )
                    else:
                        conn.execute(
                            f"UPDATE vehicle_images SET image_data = {param_placeholder}, source_url = {param_placeholder}, updated_at = {now_func} WHERE vehicle_name = {param_placeholder}",
                            (photo_data, photo_url, car_clean)
                        )
                else:
                    if is_postgres:
                        with conn.cursor() as cur:
                            cur.execute(
                                f"INSERT INTO vehicle_images (vehicle_name, image_data, source_url, updated_at) VALUES ({param_placeholder}, {param_placeholder}, {param_placeholder}, {now_func})",
                                (car_clean, photo_data, photo_url)
                            )
                    else:
                        conn.execute(
                            f"INSERT INTO vehicle_images (vehicle_name, image_data, source_url, updated_at) VALUES ({param_placeholder}, {param_placeholder}, {param_placeholder}, {now_func})",
                            (car_clean, photo_data, photo_url)
                        )
                
                conn.commit()
            finally:
                conn.close()
        
        return _no_store_json({
            "ok": True,
            "message": f"Foto baixada e salva com sucesso para '{vehicle_name}'!",
            "photo_url": photo_url,
            "photo_size": len(photo_data)
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, 500)

# ============================================================
# VEHICLE NAME MAPPING FOR FRONTEND
# ============================================================

@app.get("/api/vehicles/name-mapping")
async def get_vehicle_name_mapping():
    """Retorna mapeamento de nomes originais para clean names para usar na frontend
    INCLUI nomes editados guardados na base de dados"""
    try:
        from carjet_direct import VEHICLES
        import re
        
        # 1. Criar mapeamento base do VEHICLES
        name_mapping = {}
        
        for clean_name, category in VEHICLES.items():
            name_mapping[clean_name] = clean_name
            
            parts = clean_name.split()
            if len(parts) >= 2:
                brand = parts[0]
                model = ' '.join(parts[1:])
                
                variations = [
                    f"{brand} {model}",
                    f"{brand.upper()} {model}",
                    f"{brand.capitalize()} {model.capitalize()}",
                    f"{brand.upper()} {model.upper()}",
                    f"{brand.capitalize()} {model}",
                ]
                
                for var in variations:
                    name_mapping[var.lower()] = clean_name
        
        # 2. Aplicar OVERRIDES da base de dados (nomes editados pelo utilizador)
        try:
            with _db_lock:
                con = _db_connect()
                try:
                    rows = con.execute("SELECT original_name, edited_name FROM vehicle_name_overrides").fetchall()
                    for original, edited in rows:
                        # Normalizar chave
                        key = original.lower().strip()
                        # Aplicar override
                        name_mapping[key] = edited
                        print(f"[NAME MAPPING] Override aplicado: '{original}' ‚Üí '{edited}'")
                finally:
                    con.close()
        except Exception as db_err:
            print(f"[NAME MAPPING] Aviso: N√£o foi poss√≠vel carregar overrides da BD: {db_err}")
        
        return _no_store_json({
            "ok": True,
            "mapping": name_mapping,
            "total": len(name_mapping)
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vehicles/name-overrides")
async def save_vehicle_name_override(request: Request):
    """Salva ou atualiza um nome editado de ve√≠culo"""
    require_auth(request)
    try:
        body = await request.json()
        original_name = body.get("original_name", "").strip()
        edited_name = body.get("edited_name", "").strip()
        
        if not original_name or not edited_name:
            return _no_store_json({"ok": False, "error": "original_name e edited_name s√£o obrigat√≥rios"}, 400)
        
        with _db_lock:
            con = _db_connect()
            try:
                is_postgres = con.__class__.__module__ == 'psycopg2.extensions'
                now_func = "NOW()" if is_postgres else "datetime('now')"
                param_placeholder = "%s" if is_postgres else "?"
                
                if is_postgres:
                    with con.cursor() as cur:
                        cur.execute(f"""
                            INSERT INTO vehicle_name_overrides (original_name, edited_name, updated_at)
                            VALUES ({param_placeholder}, {param_placeholder}, {now_func})
                            ON CONFLICT(original_name) DO UPDATE SET
                                edited_name = excluded.edited_name,
                                updated_at = {now_func}
                        """, (original_name, edited_name))
                else:
                    con.execute(f"""
                        INSERT INTO vehicle_name_overrides (original_name, edited_name, updated_at)
                        VALUES ({param_placeholder}, {param_placeholder}, {now_func})
                        ON CONFLICT(original_name) DO UPDATE SET
                            edited_name = excluded.edited_name,
                            updated_at = {now_func}
                    """, (original_name, edited_name))
                con.commit()
            finally:
                con.close()
        
        return _no_store_json({
            "ok": True,
            "message": f"Nome editado salvo: '{original_name}' ‚Üí '{edited_name}'"
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.delete("/api/vehicles/name-overrides/{original_name}")
async def delete_vehicle_name_override(original_name: str, request: Request):
    """Remove um override de nome de ve√≠culo"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("DELETE FROM vehicle_name_overrides WHERE original_name = ?", (original_name,))
                con.commit()
            finally:
                con.close()
        
        return _no_store_json({
            "ok": True,
            "message": f"Override removido: '{original_name}'"
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/vehicles/name-overrides")
async def get_vehicle_name_overrides(request: Request):
    """Lista todos os overrides de nomes de ve√≠culos"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                rows = con.execute("SELECT original_name, edited_name, updated_at FROM vehicle_name_overrides ORDER BY updated_at DESC").fetchall()
                overrides = [
                    {
                        "original_name": row[0],
                        "edited_name": row[1],
                        "updated_at": row[2]
                    }
                    for row in rows
                ]
            finally:
                con.close()
        
        return _no_store_json({
            "ok": True,
            "overrides": overrides,
            "total": len(overrides)
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# VEHICLE IMAGES - Download e Storage
# ============================================================

@app.post("/api/vehicles/images/download")
async def download_vehicle_images(request: Request):
    """Download autom√°tico de todas as imagens de ve√≠culos dos URLs do scraping"""
    # N√£o requer autentica√ß√£o para funcionar em iframes
    try:
        from carjet_direct import VEHICLES
        import httpx
        import re
        
        downloaded = 0
        skipped = 0
        errors = []
        
        # PRIMEIRO: Buscar URLs de fotos da tabela dedicada car_images
        photo_urls_from_scraping = {}
        try:
            # Usar car_images.db (DB dedicada de fotos)
            from pathlib import Path
            car_images_db = str(Path(__file__).resolve().parent / "car_images.db")
            
            if os.path.exists(car_images_db):
                with _db_lock:
                    conn = sqlite3.connect(car_images_db)
                    try:
                        # Buscar fotos do car_images (tabela dedicada de fotos)
                        query = """
                            SELECT DISTINCT model_key, photo_url 
                            FROM car_images 
                            WHERE photo_url IS NOT NULL
                            AND photo_url != ''
                        """
                        rows = conn.execute(query).fetchall()
                        
                        for row in rows:
                            model_key = row[0]
                            photo_url = row[1]
                            
                            # Limpar nome para encontrar no VEHICLES
                            clean = model_key.lower().strip()
                            clean = re.sub(r'\s+(ou\s*similar|or\s*similar).*$', '', clean, flags=re.IGNORECASE)
                            clean = re.sub(r'\s*\|\s*.*$', '', clean)
                            clean = re.sub(r'\s+(pequeno|m√©dio|medio|grande|compacto|economico|econ√¥mico).*$', '', clean, flags=re.IGNORECASE)
                            clean = re.sub(r'\s+', ' ', clean).strip()
                            
                            if clean in VEHICLES and clean not in photo_urls_from_scraping:
                                photo_urls_from_scraping[clean] = photo_url
                                
                    finally:
                        conn.close()
        except Exception as e:
            print(f"[PHOTOS] Erro ao buscar fotos do car_images.db: {e}", file=sys.stderr, flush=True)
        
        print(f"[PHOTOS] Encontradas {len(photo_urls_from_scraping)} fotos no hist√≥rico de scraping", file=sys.stderr, flush=True)
        
        # SEGUNDO: Mapeamento manual para ve√≠culos sem dados recentes
        image_mappings = {
            # MINI / B1 / B2
            'fiat 500 cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L154.jpg',
            'fiat 500': 'https://www.carjet.com/cdn/img/cars/M/car_C25.jpg',
            'fiat 500x': 'https://www.carjet.com/cdn/img/cars/M/car_A112.jpg',
            'fiat 500x auto': 'https://www.carjet.com/cdn/img/cars/M/car_A112.jpg',
            'fiat 500l': 'https://www.carjet.com/cdn/img/cars/M/car_C43.jpg',
            'fiat panda': 'https://www.carjet.com/cdn/img/cars/M/car_C30.jpg',
            'citroen c1': 'https://www.carjet.com/cdn/img/cars/M/car_C96.jpg',
            'citro√´n c1': 'https://www.carjet.com/cdn/img/cars/M/car_C96.jpg',
            'toyota aygo': 'https://www.carjet.com/cdn/img/cars/M/car_C29.jpg',
            'toyota aygo x': 'https://www.carjet.com/cdn/img/cars/M/car_F408.jpg',
            'volkswagen up': 'https://www.carjet.com/cdn/img/cars/M/car_C66.jpg',
            'vw up': 'https://www.carjet.com/cdn/img/cars/M/car_C66.jpg',
            'peugeot 108': 'https://www.carjet.com/cdn/img/cars/M/car_C15.jpg',
            'peugeot 108 cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L41.jpg',
            'hyundai i10': 'https://www.carjet.com/cdn/img/cars/M/car_C32.jpg',
            'kia picanto': 'https://www.carjet.com/cdn/img/cars/M/car_C59.jpg',
            'opel adam': 'https://www.carjet.com/cdn/img/cars/M/car_C50.jpg',
            'mitsubishi space star': 'https://www.carjet.com/cdn/img/cars/M/car_C190.jpg',
            'mitsubishi spacestar': 'https://www.carjet.com/cdn/img/cars/M/car_C190.jpg',
            'nissan micra': 'https://www.carjet.com/cdn/img/cars/M/car_C13.jpg',
            'renault twingo': 'https://www.carjet.com/cdn/img/cars/M/car_C61.jpg',
            'dacia sandero': 'https://www.carjet.com/cdn/img/cars/M/car_C75.jpg',
            'skoda scala': 'https://www.carjet.com/cdn/img/cars/M/car_C166.jpg',
            
            # ECONOMY / D / E2
            'renault clio': 'https://www.carjet.com/cdn/img/cars/M/car_C04.jpg',
            'renault clio sw': 'https://www.carjet.com/cdn/img/cars/M/car_C54.jpg',
            'peugeot 208': 'https://www.carjet.com/cdn/img/cars/M/car_C60.jpg',
            'ford fiesta': 'https://www.carjet.com/cdn/img/cars/M/car_C17.jpg',
            'ford ka': 'https://www.carjet.com/cdn/img/cars/M/car_N07.jpg',
            'volkswagen polo': 'https://www.carjet.com/cdn/img/cars/M/car_C27.jpg',
            'vw polo': 'https://www.carjet.com/cdn/img/cars/M/car_C27.jpg',
            'hyundai i20': 'https://www.carjet.com/cdn/img/cars/M/car_C52.jpg',
            'seat ibiza': 'https://www.carjet.com/cdn/img/cars/M/car_C01.jpg',
            'seat ibiza auto': 'https://www.carjet.com/cdn/img/cars/M/car_C01.jpg',
            'citroen c3': 'https://www.carjet.com/cdn/img/cars/M/car_C06.jpg',
            'citro√´n c3': 'https://www.carjet.com/cdn/img/cars/M/car_C06.jpg',
            'citroen c4 cactus': 'https://www.carjet.com/cdn/img/cars/M/car_C51.jpg',
            'citro√´n c4 cactus': 'https://www.carjet.com/cdn/img/cars/M/car_C51.jpg',
            'opel corsa': 'https://www.carjet.com/cdn/img/cars/M/car_A03.jpg',
            'opel corsa auto': 'https://www.carjet.com/cdn/img/cars/M/car_A03.jpg',
            'toyota yaris': 'https://www.carjet.com/cdn/img/cars/M/car_C64.jpg',
            
            # COMPACT / F
            'volkswagen golf': 'https://www.carjet.com/cdn/img/cars/M/car_F12.jpg',
            'vw golf': 'https://www.carjet.com/cdn/img/cars/M/car_F12.jpg',
            'audi a1': 'https://www.carjet.com/cdn/img/cars/M/car_C42.jpg',
            'ford focus': 'https://www.carjet.com/cdn/img/cars/M/car_F02.jpg',
            'renault megane': 'https://www.carjet.com/cdn/img/cars/M/car_F05.jpg',
            'renault m√©gane': 'https://www.carjet.com/cdn/img/cars/M/car_F05.jpg',
            'peugeot 308': 'https://www.carjet.com/cdn/img/cars/M/car_F22.jpg',
            'hyundai i30': 'https://www.carjet.com/cdn/img/cars/M/car_C41.jpg',
            'kia ceed': 'https://www.carjet.com/cdn/img/cars/M/car_C21.jpg',
            'kia ceed auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1023.jpg',
            'seat leon': 'https://www.carjet.com/cdn/img/cars/M/car_F39.jpg',
            'seat le√≥n': 'https://www.carjet.com/cdn/img/cars/M/car_F39.jpg',
            'toyota corolla auto': 'https://www.carjet.com/cdn/img/cars/M/car_A623.jpg',
            'opel astra': 'https://www.carjet.com/cdn/img/cars/M/car_F73.jpg',
            'citroen c4': 'https://www.carjet.com/cdn/img/cars/M/car_A17.jpg',
            'citro√´n c4': 'https://www.carjet.com/cdn/img/cars/M/car_A17.jpg',
            'peugeot 508': 'https://www.carjet.com/cdn/img/cars/M/car_F65.jpg',
            
            # SUV / F / L1
            'nissan juke': 'https://www.carjet.com/cdn/img/cars/M/car_F29.jpg',
            'peugeot 2008': 'https://www.carjet.com/cdn/img/cars/M/car_F91.jpg',
            'peugeot 3008': 'https://www.carjet.com/cdn/img/cars/M/car_A132.jpg',
            'peugeot 3008 auto': 'https://www.carjet.com/cdn/img/cars/M/car_A132.jpg',
            'renault captur': 'https://www.carjet.com/cdn/img/cars/M/car_F44.jpg',
            'volkswagen t-cross': 'https://www.carjet.com/cdn/img/cars/M/car_F252.jpg',
            'vw t-cross': 'https://www.carjet.com/cdn/img/cars/M/car_F252.jpg',
            'volkswagen tcross': 'https://www.carjet.com/cdn/img/cars/M/car_F252.jpg',
            'ford kuga': 'https://www.carjet.com/cdn/img/cars/M/car_F41.jpg',
            'kia stonic': 'https://www.carjet.com/cdn/img/cars/M/car_F119.jpg',
            'citroen c3 aircross': 'https://www.carjet.com/cdn/img/cars/M/car_A782.jpg',
            'citro√´n c3 aircross': 'https://www.carjet.com/cdn/img/cars/M/car_A782.jpg',
            'citroen c5 aircross': 'https://www.carjet.com/cdn/img/cars/M/car_A640.jpg',
            'citro√´n c5 aircross': 'https://www.carjet.com/cdn/img/cars/M/car_A640.jpg',
            'citroen c5 aircross auto': 'https://www.carjet.com/cdn/img/cars/M/car_A640.jpg',
            'citro√´n c5 aircross auto': 'https://www.carjet.com/cdn/img/cars/M/car_A640.jpg',
            'jeep avenger': 'https://www.carjet.com/cdn/img/cars/M/car_L164.jpg',
            'jeep renegade': 'https://www.carjet.com/cdn/img/cars/M/car_A222.jpg',
            'jeep renegade auto': 'https://www.carjet.com/cdn/img/cars/M/car_A222.jpg',
            'volkswagen taigo': 'https://www.carjet.com/cdn/img/cars/M/car_F352.jpg',
            'vw taigo': 'https://www.carjet.com/cdn/img/cars/M/car_F352.jpg',
            'hyundai kauai': 'https://www.carjet.com/cdn/img/cars/M/car_F44.jpg',
            'hyundai kaua√≠': 'https://www.carjet.com/cdn/img/cars/M/car_F44.jpg',
            'mitsubishi asx': 'https://www.carjet.com/cdn/img/cars/M/car_F178.jpg',
            'hyundai kona': 'https://www.carjet.com/cdn/img/cars/M/car_F191.jpg',
            'toyota c-hr': 'https://www.carjet.com/cdn/img/cars/M/car_A301.jpg',
            'toyota chr': 'https://www.carjet.com/cdn/img/cars/M/car_A301.jpg',
            'toyota c-hr auto': 'https://www.carjet.com/cdn/img/cars/M/car_A301.jpg',
            'toyota chr auto': 'https://www.carjet.com/cdn/img/cars/M/car_A301.jpg',
            'ford ecosport': 'https://www.carjet.com/cdn/img/cars/M/car_A606.jpg',
            'ford eco sport': 'https://www.carjet.com/cdn/img/cars/M/car_A606.jpg',
            'ford ecosport auto': 'https://www.carjet.com/cdn/img/cars/M/car_A606.jpg',
            'opel crossland x': 'https://www.carjet.com/cdn/img/cars/M/car_A444.jpg',
            'opel crossland x auto': 'https://www.carjet.com/cdn/img/cars/M/car_A444.jpg',
            'volkswagen tiguan': 'https://www.carjet.com/cdn/img/cars/M/car_A830.jpg',
            'vw tiguan': 'https://www.carjet.com/cdn/img/cars/M/car_A830.jpg',
            'volkswagen tiguan auto': 'https://www.carjet.com/cdn/img/cars/M/car_A830.jpg',
            'vw tiguan auto': 'https://www.carjet.com/cdn/img/cars/M/car_A830.jpg',
            'skoda karoq': 'https://www.carjet.com/cdn/img/cars/M/car_A822.jpg',
            'skoda karoq auto': 'https://www.carjet.com/cdn/img/cars/M/car_A822.jpg',
            'kia sportage': 'https://www.carjet.com/cdn/img/cars/M/car_F43.jpg',
            'nissan qashqai': 'https://www.carjet.com/cdn/img/cars/M/car_F24.jpg',
            'skoda kamiq': 'https://www.carjet.com/cdn/img/cars/M/car_F310.jpg',
            'hyundai tucson': 'https://www.carjet.com/cdn/img/cars/M/car_F310.jpg',
            'renault austral': 'https://www.carjet.com/cdn/img/cars/M/car_F430.jpg',
            'seat ateca': 'https://www.carjet.com/cdn/img/cars/M/car_F154.jpg',
            'seat arona': 'https://www.carjet.com/cdn/img/cars/M/car_F194.jpg',
            'seat arona auto': 'https://www.carjet.com/cdn/img/cars/M/car_F194.jpg',
            'ford puma': 'https://www.carjet.com/cdn/img/cars/M/car_A999.jpg',
            'ford puma auto': 'https://www.carjet.com/cdn/img/cars/M/car_A999.jpg',
            'mazda cx-3': 'https://www.carjet.com/cdn/img/cars/M/car_F179.jpg',
            'mazda cx 3': 'https://www.carjet.com/cdn/img/cars/M/car_F179.jpg',
            'renault arkana': 'https://www.carjet.com/cdn/img/cars/M/car_A1159.jpg',
            'renault arkana auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1159.jpg',
            'toyota rav 4': 'https://www.carjet.com/cdn/img/cars/M/car_A1000.jpg',
            'toyota rav4': 'https://www.carjet.com/cdn/img/cars/M/car_A1000.jpg',
            'toyota rav 4 4x4': 'https://www.carjet.com/cdn/img/cars/M/car_A1000.jpg',
            'toyota rav 4 auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1000.jpg',
            'toyota hilux': 'https://www.carjet.com/cdn/img/cars/M/car_F326.jpg',
            'toyota hilux 4x4': 'https://www.carjet.com/cdn/img/cars/M/car_F326.jpg',
            
            # PREMIUM / G
            'mini cooper countryman': 'https://www.carjet.com/cdn/img/cars/M/car_F209.jpg',
            'miny cooper countryman': 'https://www.carjet.com/cdn/img/cars/M/car_F209.jpg',
            'mini countryman': 'https://www.carjet.com/cdn/img/cars/M/car_F209.jpg',
            'mini cooper countryman auto': 'https://www.carjet.com/cdn/img/cars/M/car_F209.jpg',
            'mini cooper cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L118.jpg',
            'mini one cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L118.jpg',
            'volkswagen beetle cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L44.jpg',
            'vw beetle cabrio': 'https://www.carjet.com/cdn/img/cars/M/car_L44.jpg',
            'cupra formentor': 'https://www.carjet.com/cdn/img/cars/M/car_A1185.jpg',
            'cupra formentor auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1185.jpg',
            'ds 4': 'https://www.carjet.com/cdn/img/cars/M/car_A1637.jpg',
            'ds 4 auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1637.jpg',
            
            # STATION WAGON / J2 / L2
            'peugeot 308 sw': 'https://www.carjet.com/cdn/img/cars/M/car_S06.jpg',
            'peugeot 308 sw auto': 'https://www.carjet.com/cdn/img/cars/M/car_S06.jpg',
            'opel astra sw': 'https://www.carjet.com/cdn/img/cars/M/car_S10.jpg',
            'cupra leon sw': 'https://www.carjet.com/cdn/img/cars/M/car_A1426.jpg',
            'cupra leon st': 'https://www.carjet.com/cdn/img/cars/M/car_A1426.jpg',
            'cupra leon estate': 'https://www.carjet.com/cdn/img/cars/M/car_A1426.jpg',
            'cupra leon sport tourer': 'https://www.carjet.com/cdn/img/cars/M/car_A1426.jpg',
            'cupra leon sw auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1426.jpg',
            'toyota corolla sw': 'https://www.carjet.com/cdn/img/cars/M/car_A590.jpg',
            'toyota corolla touring sports': 'https://www.carjet.com/cdn/img/cars/M/car_A590.jpg',
            'toyota corolla estate': 'https://www.carjet.com/cdn/img/cars/M/car_A590.jpg',
            'toyota corolla sw auto': 'https://www.carjet.com/cdn/img/cars/M/car_A590.jpg',
            'skoda octavia': 'https://www.carjet.com/cdn/img/cars/M/car_I12.jpg',
            'skoda octavia sw': 'https://www.carjet.com/cdn/img/cars/M/car_I12.jpg',
            'skoda octavia combi': 'https://www.carjet.com/cdn/img/cars/M/car_I12.jpg',
            'skoda octavia estate': 'https://www.carjet.com/cdn/img/cars/M/car_I12.jpg',
            'skoda fabia sw': 'https://www.carjet.com/cdn/img/cars/M/car_S34.jpg',
            'skoda fabia combi': 'https://www.carjet.com/cdn/img/cars/M/car_S34.jpg',
            'skoda fabia estate': 'https://www.carjet.com/cdn/img/cars/M/car_S34.jpg',
            'volkswagen passat': 'https://www.carjet.com/cdn/img/cars/M/car_I11.jpg',
            'vw passat': 'https://www.carjet.com/cdn/img/cars/M/car_I11.jpg',
            'volkswagen passat variant': 'https://www.carjet.com/cdn/img/cars/M/car_I11.jpg',
            'volkswagen passat estate': 'https://www.carjet.com/cdn/img/cars/M/car_I11.jpg',
            'volkswagen passat sw': 'https://www.carjet.com/cdn/img/cars/M/car_I11.jpg',
            'fiat tipo sw': 'https://www.carjet.com/cdn/img/cars/M/car_F72.jpg',
            'fiat tipo estate': 'https://www.carjet.com/cdn/img/cars/M/car_F72.jpg',
            'seat leon sw': 'https://www.carjet.com/cdn/img/cars/M/car_F46.jpg',
            'seat leon st': 'https://www.carjet.com/cdn/img/cars/M/car_F46.jpg',
            'seat leon estate': 'https://www.carjet.com/cdn/img/cars/M/car_F46.jpg',
            'seat leon sport tourer': 'https://www.carjet.com/cdn/img/cars/M/car_F46.jpg',
            
            # 7 SEATER / M1 / M2
            'dacia lodgy': 'https://www.carjet.com/cdn/img/cars/M/car_M117.jpg',
            'dacia jogger': 'https://www.carjet.com/cdn/img/cars/M/car_M162.jpg',
            'opel zafira': 'https://www.carjet.com/cdn/img/cars/M/car_M05.jpg',
            'peugeot 5008': 'https://www.carjet.com/cdn/img/cars/M/car_M27.jpg',
            'renault grand scenic': 'https://www.carjet.com/cdn/img/cars/M/car_M15.jpg',
            'renault grand scenic auto': 'https://www.carjet.com/cdn/img/cars/M/car_M15.jpg',
            'citroen grand picasso': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citro√´n grand picasso': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citroen c4 grand picasso': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citro√´n c4 grand picasso': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citroen c4 grand picasso auto': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citro√´n c4 grand picasso auto': 'https://www.carjet.com/cdn/img/cars/M/car_A219.jpg',
            'citroen c4 picasso auto': 'https://www.carjet.com/cdn/img/cars/M/car_A522.jpg',
            'citro√´n c4 picasso auto': 'https://www.carjet.com/cdn/img/cars/M/car_A522.jpg',
            'citroen c4 grand spacetourer': 'https://www.carjet.com/cdn/img/cars/M/car_A1430.jpg',
            'citro√´n c4 grand spacetourer': 'https://www.carjet.com/cdn/img/cars/M/car_A1430.jpg',
            'citroen c4 grand spacetourer auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1430.jpg',
            'volkswagen caddy': 'https://www.carjet.com/cdn/img/cars/M/car_A295.jpg',
            'vw caddy': 'https://www.carjet.com/cdn/img/cars/M/car_A295.jpg',
            'volkswagen caddy auto': 'https://www.carjet.com/cdn/img/cars/M/car_A295.jpg',
            'peugeot rifter': 'https://www.carjet.com/cdn/img/cars/M/car_M124.jpg',
            'peugeot rifter auto': 'https://www.carjet.com/cdn/img/cars/M/car_M124.jpg',
            'mercedes glb': 'https://www.carjet.com/cdn/img/cars/M/car_GZ399.jpg',
            'mercedes glb auto': 'https://www.carjet.com/cdn/img/cars/M/car_GZ399.jpg',
            
            # 9 SEATER / N
            'ford tourneo': 'https://www.carjet.com/cdn/img/cars/M/car_M44.jpg',
            'ford transit': 'https://www.carjet.com/cdn/img/cars/M/car_M02.jpg',
            'ford galaxy': 'https://www.carjet.com/cdn/img/cars/M/car_M03.jpg',
            'volkswagen sharan': 'https://www.carjet.com/cdn/img/cars/M/car_M56.jpg',
            'vw sharan': 'https://www.carjet.com/cdn/img/cars/M/car_M56.jpg',
            'volkswagen multivan': 'https://www.carjet.com/cdn/img/cars/M/car_A406.jpg',
            'vw multivan': 'https://www.carjet.com/cdn/img/cars/M/car_A406.jpg',
            'volkswagen multivan auto': 'https://www.carjet.com/cdn/img/cars/M/car_A406.jpg',
            'vw multivan auto': 'https://www.carjet.com/cdn/img/cars/M/car_A406.jpg',
            'citroen spacetourer': 'https://www.carjet.com/cdn/img/cars/M/car_A261.jpg',
            'citro√´n spacetourer': 'https://www.carjet.com/cdn/img/cars/M/car_A261.jpg',
            'citroen spacetourer auto': 'https://www.carjet.com/cdn/img/cars/M/car_A261.jpg',
            'renault trafic': 'https://www.carjet.com/cdn/img/cars/M/car_A581.jpg',
            'renault trafic auto': 'https://www.carjet.com/cdn/img/cars/M/car_A581.jpg',
            'peugeot traveller': 'https://www.carjet.com/cdn/img/cars/M/car_M86.jpg',
            'volkswagen transporter': 'https://www.carjet.com/cdn/img/cars/M/car_M08.jpg',
            'vw transporter': 'https://www.carjet.com/cdn/img/cars/M/car_M08.jpg',
            'mercedes vito': 'https://www.carjet.com/cdn/img/cars/M/car_A230.jpg',
            'mercedes benz vito': 'https://www.carjet.com/cdn/img/cars/M/car_A230.jpg',
            'mercedes vito auto': 'https://www.carjet.com/cdn/img/cars/M/car_A230.jpg',
            'volkswagen caravelle': 'https://www.carjet.com/cdn/img/cars/M/car_M63.jpg',
            'vw caravelle': 'https://www.carjet.com/cdn/img/cars/M/car_M63.jpg',
            'mercedes v class': 'https://www.carjet.com/cdn/img/cars/M/car_A1336.jpg',
            'mercedes benz v class': 'https://www.carjet.com/cdn/img/cars/M/car_A1336.jpg',
            'mercedes v class auto': 'https://www.carjet.com/cdn/img/cars/M/car_A1336.jpg',
            'fiat talento': 'https://www.carjet.com/cdn/img/cars/M/car_M49.jpg',
            'opel vivaro': 'https://www.carjet.com/cdn/img/cars/M/car_M34.jpg',
            'toyota proace': 'https://www.carjet.com/cdn/img/cars/M/car_M136.jpg',
        }
        
        for vehicle_key in VEHICLES.keys():
            try:
                # Verificar se j√° existe
                with _db_lock:
                    con = _db_connect()
                    try:
                        existing = con.execute("SELECT vehicle_name FROM vehicle_photos WHERE vehicle_name = ?", (vehicle_key,)).fetchone()
                        if existing:
                            skipped += 1
                            continue
                    finally:
                        con.close()
                
                # Buscar URL da imagem - PRIORIDADE: scraping recente
                image_url = photo_urls_from_scraping.get(vehicle_key) or image_mappings.get(vehicle_key)
                if not image_url:
                    errors.append(f"{vehicle_key}: No photo URL found")
                    continue
                
                print(f"[PHOTOS] Downloading {vehicle_key} from {image_url[:80]}...", file=sys.stderr, flush=True)
                
                # Download da imagem
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(image_url)
                    if response.status_code == 200:
                        image_data = response.content
                        content_type = response.headers.get('content-type', 'image/jpeg')
                        
                        # Salvar na BD
                        with _db_lock:
                            con = _db_connect()
                            try:
                                if con.__class__.__module__ == 'psycopg2.extensions':
                                    # PostgreSQL
                                    with con.cursor() as cur:
                                        cur.execute("""
                                            INSERT INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                                            VALUES (%s, %s, %s, %s)
                                            ON CONFLICT (vehicle_name) DO UPDATE SET
                                                photo_data = EXCLUDED.photo_data,
                                                content_type = EXCLUDED.content_type,
                                                photo_url = EXCLUDED.photo_url
                                        """, (vehicle_key, image_data, content_type, image_url))
                                else:
                                    # SQLite
                                    con.execute("""
                                        INSERT OR REPLACE INTO vehicle_photos (vehicle_name, photo_data, content_type, photo_url)
                                        VALUES (?, ?, ?, ?)
                                    """, (vehicle_key, image_data, content_type, image_url))
                                con.commit()
                                downloaded += 1
                            finally:
                                con.close()
            except Exception as e:
                errors.append(f"{vehicle_key}: {str(e)}")
        
        return _no_store_json({
            "ok": True,
            "downloaded": downloaded,
            "skipped": skipped,
            "errors": errors[:10]
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/vehicles/{vehicle_name}/photo")
async def get_vehicle_photo(vehicle_name: str):
    """Retorna a foto de um ve√≠culo espec√≠fico"""
    # N√£o requer autentica√ß√£o para permitir que as tags <img> funcionem
    from fastapi.responses import Response
    
    try:
        # Normalizar nome do ve√≠culo
        vehicle_key = vehicle_name.lower().strip()
        
        with _db_lock:
            con = _db_connect()
            try:
                row = None
                
                # PostgreSQL or SQLite
                if con.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL - usar cursor
                    cursor = con.cursor()
                    
                    # Tentar buscar foto exata em vehicle_images primeiro
                    cursor.execute(
                        "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key = %s",
                        (vehicle_key,)
                    )
                    row = cursor.fetchone()
                    
                    # Se n√£o encontrar, tentar em vehicle_photos
                    if not row:
                        cursor.execute(
                            "SELECT photo_data, content_type FROM vehicle_photos WHERE vehicle_name = %s",
                            (vehicle_key,)
                        )
                        row = cursor.fetchone()
                    
                    # Se n√£o encontrar, tentar buscar varia√ß√µes do mesmo modelo
                    if not row:
                        # Detectar se √© Station Wagon (SW) - s√£o modelos diferentes!
                        is_sw = ' sw' in vehicle_key or 'station wagon' in vehicle_key or 'estate' in vehicle_key
                        
                        # Extrair modelo base (ex: "citroen c1" de "citroen c1 auto")
                        base_model = vehicle_key
                        for suffix in [' auto', ' automatic', ' hybrid', ' electric', ' diesel', ' 4x4', ', hybrid', ', electric', ', diesel', ', automatic']:
                            base_model = base_model.replace(suffix, '')
                        base_model = base_model.strip()
                        
                        search_pattern = base_model + '%'
                        
                        if is_sw:
                            cursor.execute(
                                "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key LIKE %s AND (vehicle_key LIKE %s OR vehicle_key LIKE %s OR vehicle_key LIKE %s) LIMIT 1",
                                (search_pattern, '%sw%', '%station wagon%', '%estate%')
                            )
                            row = cursor.fetchone()
                        else:
                            cursor.execute(
                                "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key LIKE %s AND vehicle_key NOT LIKE %s AND vehicle_key NOT LIKE %s AND vehicle_key NOT LIKE %s LIMIT 1",
                                (search_pattern, '%sw%', '%station wagon%', '%estate%')
                            )
                            row = cursor.fetchone()
                    
                    cursor.close()
                else:
                    # SQLite - usar execute direto
                    # Tentar buscar foto exata em vehicle_images primeiro
                    row = con.execute(
                        "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key = ?",
                        (vehicle_key,)
                    ).fetchone()
                    
                    # Se n√£o encontrar, tentar em vehicle_photos
                    if not row:
                        row = con.execute(
                            "SELECT photo_data, content_type FROM vehicle_photos WHERE vehicle_name = ?",
                            (vehicle_key,)
                        ).fetchone()
                    
                    # Se n√£o encontrar, tentar buscar varia√ß√µes do mesmo modelo
                    if not row:
                        is_sw = ' sw' in vehicle_key or 'station wagon' in vehicle_key or 'estate' in vehicle_key
                        base_model = vehicle_key
                        for suffix in [' auto', ' automatic', ' hybrid', ' electric', ' diesel', ' 4x4', ', hybrid', ', electric', ', diesel', ', automatic']:
                            base_model = base_model.replace(suffix, '')
                        base_model = base_model.strip()
                        
                        search_pattern = base_model + '%'
                        
                        if is_sw:
                            row = con.execute(
                                "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key LIKE ? AND (vehicle_key LIKE ? OR vehicle_key LIKE ? OR vehicle_key LIKE ?) LIMIT 1",
                                (search_pattern, '%sw%', '%station wagon%', '%estate%')
                            ).fetchone()
                        else:
                            row = con.execute(
                                "SELECT image_data, content_type FROM vehicle_images WHERE vehicle_key LIKE ? AND vehicle_key NOT LIKE ? AND vehicle_key NOT LIKE ? AND vehicle_key NOT LIKE ? LIMIT 1",
                                (search_pattern, '%sw%', '%station wagon%', '%estate%')
                            ).fetchone()
                
                if row:
                    image_data = row[0]
                    content_type = row[1] or 'image/jpeg'
                    
                    # Convert memoryview to bytes (PostgreSQL compatibility)
                    if isinstance(image_data, memoryview):
                        image_data = bytes(image_data)
                    
                    return Response(
                        content=image_data,
                        media_type=content_type,
                        headers={
                            "Cache-Control": "public, max-age=86400",
                            "Content-Disposition": f"inline; filename={vehicle_key}.jpg"
                        }
                    )
                else:
                    # Retornar imagem placeholder SVG
                    svg_placeholder = '''<svg xmlns="http://www.w3.org/2000/svg" width="60" height="40">
                        <rect width="60" height="40" fill="#e5e7eb"/>
                        <text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#999" font-size="12">üöó</text>
                    </svg>'''
                    return Response(
                        content=svg_placeholder,
                        media_type="image/svg+xml"
                    )
            finally:
                con.close()
    except Exception as e:
        import traceback
        print(f"Erro ao buscar foto: {traceback.format_exc()}")
        # Retornar placeholder em caso de erro
        svg_placeholder = '''<svg xmlns="http://www.w3.org/2000/svg" width="60" height="40">
            <rect width="60" height="40" fill="#e5e7eb"/>
            <text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#999" font-size="12">üöó</text>
        </svg>'''
        return Response(
            content=svg_placeholder,
            media_type="image/svg+xml"
        )

@app.get("/api/vehicles/{vehicle_name}/photo/metadata")
async def get_vehicle_photo_metadata(vehicle_name: str, request: Request):
    """Retorna metadata da foto de um ve√≠culo (URL original, etc)"""
    require_auth(request)
    try:
        vehicle_key = vehicle_name.lower().strip()
        
        with _db_lock:
            con = _db_connect()
            try:
                # Try vehicle_photos first (has photo_url)
                if con.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with con.cursor() as cur:
                        cur.execute(
                            "SELECT photo_url, uploaded_at, content_type FROM vehicle_photos WHERE vehicle_name = %s",
                            (vehicle_key,)
                        )
                        row = cur.fetchone()
                else:
                    # SQLite
                    row = con.execute(
                        "SELECT photo_url, uploaded_at, content_type FROM vehicle_photos WHERE vehicle_name = ?",
                        (vehicle_key,)
                    ).fetchone()
                
                if row and row[0]:  # Has photo_url
                    return _no_store_json({
                        "ok": True,
                        "source_url": row[0],  # photo_url
                        "downloaded_at": row[1],
                        "content_type": row[2]
                    })
                
                # Fallback: try vehicle_images (has source_url)
                if con.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with con.cursor() as cur:
                        cur.execute(
                            "SELECT source_url, downloaded_at, content_type FROM vehicle_images WHERE vehicle_key = %s",
                            (vehicle_key,)
                        )
                        row = cur.fetchone()
                else:
                    # SQLite
                    row = con.execute(
                        "SELECT source_url, downloaded_at, content_type FROM vehicle_images WHERE vehicle_key = ?",
                        (vehicle_key,)
                    ).fetchone()
                
                if row and row[0]:  # Has source_url
                    return _no_store_json({
                        "ok": True,
                        "source_url": row[0],
                        "downloaded_at": row[1],
                        "content_type": row[2]
                    })
                
                # No metadata found
                return _no_store_json({"ok": False, "error": "Photo metadata not found"}, 404)
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vehicles/{vehicle_name}/photo/from-url")
async def download_vehicle_photo_from_url(vehicle_name: str, request: Request):
    """Baixa e salva a foto de um ve√≠culo a partir de uma URL"""
    require_auth(request)
    try:
        import httpx
        
        body = await request.json()
        url = body.get('url', '').strip()
        
        if not url:
            return _no_store_json({"ok": False, "error": "URL √© obrigat√≥ria"}, 400)
        
        # Normalizar nome do ve√≠culo
        vehicle_key = vehicle_name.lower().strip()
        
        # Baixar imagem
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url)
            response.raise_for_status()
            
            image_data = response.content
            content_type = response.headers.get('content-type', 'image/jpeg')
            
            # Salvar na base de dados
            with _db_lock:
                con = _db_connect()
                try:
                    query = _convert_query_for_db(
                        """INSERT OR REPLACE INTO vehicle_images 
                           (vehicle_key, image_data, content_type, source_url, downloaded_at)
                           VALUES (?, ?, ?, ?, datetime('now'))""", con)
                    
                    if con.__class__.__module__ == 'psycopg2.extensions':
                        with con.cursor() as cur:
                            cur.execute(query, (vehicle_key, image_data, content_type, url))
                    else:
                        con.execute(query, (vehicle_key, image_data, content_type, url))
                    con.commit()
                finally:
                    con.close()
            
            return _no_store_json({
                "ok": True,
                "message": f"Foto baixada e salva para {vehicle_name}",
                "size": len(image_data),
                "content_type": content_type
            })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vehicles/{vehicle_name}/photo/upload")
async def upload_vehicle_photo(vehicle_name: str, request: Request, file: UploadFile = File(...)):
    """Upload de foto de um ve√≠culo"""
    require_auth(request)
    try:
        # Normalizar nome do ve√≠culo
        vehicle_key = vehicle_name.lower().strip()
        
        # Ler conte√∫do do ficheiro
        image_data = await file.read()
        content_type = file.content_type or 'image/jpeg'
        
        # Salvar na base de dados
        with _db_lock:
            con = _db_connect()
            try:
                query = _convert_query_for_db(
                    """INSERT OR REPLACE INTO vehicle_images 
                       (vehicle_key, image_data, content_type, source_url, downloaded_at)
                       VALUES (?, ?, ?, ?, datetime('now'))""", con)
                
                if con.__class__.__module__ == 'psycopg2.extensions':
                    with con.cursor() as cur:
                        cur.execute(query, (vehicle_key, image_data, content_type, 'uploaded'))
                else:
                    con.execute(query, (vehicle_key, image_data, content_type, 'uploaded'))
                con.commit()
            finally:
                con.close()
        
        return _no_store_json({
            "ok": True,
            "message": f"Foto enviada com sucesso para {vehicle_name}",
            "size": len(image_data),
            "content_type": content_type
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# EXPORT/IMPORT - Base de Dados Completa
# ============================================================

@app.get("/api/export/config")
async def export_config(request: Request):
    """Exporta configura√ß√£o completa: ve√≠culos, imagens, overrides"""
    require_auth(request)
    try:
        import base64
        from datetime import datetime
        
        export_data = {
            "version": "1.0",
            "exported_at": datetime.utcnow().isoformat(),
            "vehicles": {},
            "name_overrides": [],
            "images": {}
        }
        
        # 1. Exportar VEHICLES
        try:
            from carjet_direct import VEHICLES
            export_data["vehicles"] = dict(VEHICLES)
        except Exception as e:
            print(f"Aviso: n√£o foi poss√≠vel exportar VEHICLES: {e}")
        
        # 2. Exportar name overrides
        with _db_lock:
            con = _db_connect()
            try:
                rows = con.execute("SELECT original_name, edited_name, updated_at FROM vehicle_name_overrides").fetchall()
                export_data["name_overrides"] = [
                    {
                        "original_name": row[0],
                        "edited_name": row[1],
                        "updated_at": row[2]
                    }
                    for row in rows
                ]
            finally:
                con.close()
        
        # 3. Exportar imagens (como Base64)
        with _db_lock:
            con = _db_connect()
            try:
                rows = con.execute("SELECT vehicle_key, image_data, content_type, source_url FROM vehicle_images").fetchall()
                for row in rows:
                    vehicle_key = row[0]
                    image_data = row[1]
                    content_type = row[2]
                    source_url = row[3]
                    
                    # Converter para Base64
                    image_base64 = base64.b64encode(image_data).decode('utf-8')
                    
                    export_data["images"][vehicle_key] = {
                        "data": image_base64,
                        "content_type": content_type,
                        "source_url": source_url
                    }
            finally:
                con.close()
        
        # Retornar como JSON para download
        return JSONResponse(
            content=export_data,
            headers={
                "Content-Disposition": f"attachment; filename=carrental_config_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
            }
        )
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/import/config")
async def import_config(request: Request):
    """Importa configura√ß√£o completa de um ficheiro JSON"""
    require_auth(request)
    try:
        import base64
        
        body = await request.json()
        
        if not body or "version" not in body:
            return _no_store_json({"ok": False, "error": "Ficheiro de configura√ß√£o inv√°lido"}, 400)
        
        imported = {
            "name_overrides": 0,
            "images": 0
        }
        
        # 1. Importar name overrides
        if "name_overrides" in body:
            with _db_lock:
                con = _db_connect()
                try:
                    for override in body["name_overrides"]:
                        con.execute("""
                            INSERT INTO vehicle_name_overrides (original_name, edited_name, updated_at)
                            VALUES (?, ?, ?)
                            ON CONFLICT(original_name) DO UPDATE SET
                                edited_name = excluded.edited_name,
                                updated_at = excluded.updated_at
                        """, (override["original_name"], override["edited_name"], override.get("updated_at", "now")))
                        imported["name_overrides"] += 1
                    con.commit()
                finally:
                    con.close()
        
        # 2. Importar imagens
        if "images" in body:
            with _db_lock:
                con = _db_connect()
                try:
                    for vehicle_key, image_info in body["images"].items():
                        # Converter de Base64 para bytes
                        image_data = base64.b64decode(image_info["data"])
                        content_type = image_info.get("content_type", "image/jpeg")
                        source_url = image_info.get("source_url", "")
                        
                        query = _convert_query_for_db("""
                            INSERT INTO vehicle_images (vehicle_key, image_data, content_type, source_url, downloaded_at)
                            VALUES (?, ?, ?, ?, datetime('now'))
                            ON CONFLICT(vehicle_key) DO UPDATE SET
                                image_data = excluded.image_data,
                                content_type = excluded.content_type,
                                source_url = excluded.source_url,
                                downloaded_at = excluded.downloaded_at
                        """, con)
                        
                        if con.__class__.__module__ == 'psycopg2.extensions':
                            with con.cursor() as cur:
                                cur.execute(query, (vehicle_key, image_data, content_type, source_url))
                        else:
                            con.execute(query, (vehicle_key, image_data, content_type, source_url))
                        imported["images"] += 1
                    con.commit()
                finally:
                    con.close()
        
        return _no_store_json({
            "ok": True,
            "message": "Configura√ß√£o importada com sucesso",
            "imported": imported
        })
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# DAMAGE REPORTS
# ============================================================

@app.post("/api/damage-reports/upload-template")
async def upload_damage_report_template(request: Request, file: UploadFile = File(...)):
    """Upload do template de Damage Report (PDF/Word)"""
    require_auth(request)
    
    try:
        contents = await file.read()
        filename = file.filename
        
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS damage_report_templates (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        file_data BLOB NOT NULL,
                        content_type TEXT,
                        uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        uploaded_by TEXT
                    )
                """)
                
                conn.execute("""
                    INSERT INTO damage_report_templates (filename, file_data, content_type, uploaded_by)
                    VALUES (?, ?, ?, ?)
                """, (filename, contents, file.content_type, request.session.get('username', 'unknown')))
                
                conn.commit()
                
                return {"ok": True, "message": "Template uploaded successfully", "filename": filename}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error uploading template: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/extract-from-ra")
async def extract_from_rental_agreement(request: Request, file: UploadFile = File(...)):
    """Extrai campos do Rental Agreement PDF - Usa coordenadas mapeadas se dispon√≠veis"""
    require_auth(request)
    
    try:
        import PyPDF2
        import re
        from io import BytesIO
        
        contents = await file.read()
        pdf_file = BytesIO(contents)
        
        # M√âTODO 1: Tentar usar coordenadas mapeadas (se existirem)
        # ‚úÖ REATIVADO - Usar coordenadas configuradas manualmente como PRIORIDADE
        fields_from_mapping = {}
        coords_rows = []
        
        try:
            import fitz  # PyMuPDF
            
            # Carregar coordenadas mapeadas do RA
            print("\n" + "="*80)
            print("üö® EXTRA√á√ÉO POR COORDENADAS - IN√çCIO")
            print("="*80)
            logging.info("üìç Procurando coordenadas mapeadas...")
            with _db_lock:
                conn = _db_connect()
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("SELECT field_id, x, y, width, height, page FROM rental_agreement_coordinates")
                        coords_rows = cur.fetchall()
                else:
                    cursor = conn.execute("SELECT field_id, x, y, width, height, page FROM rental_agreement_coordinates")
                    coords_rows = cursor.fetchall()
            
            print(f"üîç Coordenadas encontradas: {len(coords_rows)}")
            
            if coords_rows:  # ‚úÖ ATIVADO
                print(f"‚úÖ USANDO {len(coords_rows)} COORDENADAS MAPEADAS!")
                logging.info(f"üìç Found {len(coords_rows)} mapped RA coordinates, extracting...")
                
                # Abrir PDF com PyMuPDF
                pdf_doc = fitz.open(stream=contents, filetype="pdf")
                
                logging.info(f"\n{'='*60}")
                logging.info(f"üîç DIAGN√ìSTICO DE COORDENADAS")
                logging.info(f"{'='*60}")
                
                for row in coords_rows:
                    field_id, x, y, width, height, page = row[0], row[1], row[2], row[3], row[4], row[5]
                    
                    try:
                        # Ajustar para √≠ndice 0-based
                        page_num = int(page) - 1 if page else 0
                        
                        if page_num < len(pdf_doc):
                            pdf_page = pdf_doc[page_num]
                            
                            print(f"\nüìç Campo: {field_id}")
                            print(f"   Coords: x={x:.1f}, y={y:.1f}, w={width:.1f}, h={height:.1f}")
                            
                            # TESTAR 4 VARIA√á√ïES COMUNS DE COORDENADAS
                            page_height = pdf_page.rect.height
                            
                            methods = {
                                "DIRETO": (x, y, x + width, y + height),
                                "INVERTIDO_Y": (x, page_height - y - height, x + width, page_height - y),
                                "ESCALA_2": (x/2, y/2, (x + width)/2, (y + height)/2),
                                "ESCALA_INV": (x/2, page_height - y/2 - height/2, (x + width)/2, page_height - y/2),
                            }
                            
                            best_text = ""
                            best_method = "DIRETO"
                            
                            # ‚úÖ PRIORIDADE: Testar DIRETO primeiro
                            for method_name, coords in methods.items():
                                rect_test = fitz.Rect(*coords)
                                text_test = pdf_page.get_text("text", clip=rect_test).strip()
                                text_clean = ' '.join(text_test.split()) if text_test else ""
                                
                                if text_clean:
                                    print(f"   {method_name}: '{text_clean[:40]}'")
                                
                                # Se DIRETO tiver texto, usar! S√≥ tentar outros se DIRETO estiver vazio
                                if method_name == "DIRETO" and text_clean:
                                    best_text = text_clean
                                    best_method = "DIRETO"
                                    break  # ‚úÖ USAR DIRETO E PARAR
                                
                                # Se DIRETO est√° vazio, testar outros m√©todos
                                if not best_text and text_clean:
                                    # Valida√ß√£o b√°sica: se campo √© location, deve ter letras
                                    if 'Location' in field_id or 'location' in field_id:
                                        if any(c.isalpha() for c in text_clean) and not any(char in text_clean for char in ['J5V', 'H4H', '08 -']):
                                            best_text = text_clean
                                            best_method = method_name
                                    else:
                                        best_text = text_clean
                                        best_method = method_name
                            
                            text_extracted = best_text
                            print(f"   ‚úÖ Escolhido: {best_method} ‚Üí '{text_extracted}'")
                            logging.info(f"üìç {field_id}: '{text_extracted}' ({best_method})")
                            
                            # Se n√£o extraiu texto, tentar OCR
                            if not text_extracted:
                                try:
                                    import pytesseract
                                    from PIL import Image
                                    import io
                                    
                                    # Renderizar √°rea como imagem
                                    rect_ocr = fitz.Rect(x, y, x + width, y + height)
                                    zoom = 2  # Aumentar resolu√ß√£o para OCR
                                    mat = fitz.Matrix(zoom, zoom)
                                    pix = pdf_page.get_pixmap(matrix=mat, clip=rect_ocr)
                                    
                                    # Converter para PIL Image
                                    img_data = pix.tobytes("png")
                                    img = Image.open(io.BytesIO(img_data))
                                    
                                    # Aplicar OCR
                                    text_extracted = pytesseract.image_to_string(img, lang='por+eng', config='--psm 6').strip()
                                    
                                    if text_extracted:
                                        logging.info(f"   üîç OCR extraiu {field_id}: {text_extracted[:50]}")
                                except Exception as ocr_error:
                                    logging.debug(f"   ‚ö†Ô∏è  OCR falhou para {field_id}: {ocr_error}")
                            
                            if text_extracted:
                                fields_from_mapping[field_id] = text_extracted
                                logging.info(f"   ‚úÖ {field_id}: {text_extracted[:50]}")
                    except Exception as e:
                        logging.warning(f"   ‚ö†Ô∏è  Error extracting {field_id}: {e}")
                        continue
                
                pdf_doc.close()
                
                # Se extraiu campos usando coordenadas, processar e retornar
                if fields_from_mapping:
                    logging.info(f"‚úÖ Extra√≠dos {len(fields_from_mapping)} campos usando coordenadas mapeadas")
                    logging.info(f"   Campos: {list(fields_from_mapping.keys())}")
                    
                    # Combinar campos para Damage Report
                    if fields_from_mapping.get('postalCode') or fields_from_mapping.get('city'):
                        postal_code_city = ' / '.join(filter(None, [
                            fields_from_mapping.get('postalCode'),
                            fields_from_mapping.get('city')
                        ]))
                        if postal_code_city:
                            fields_from_mapping['postalCodeCity'] = postal_code_city
                    
                    if fields_from_mapping.get('vehicleBrand') or fields_from_mapping.get('vehicleModel'):
                        brand_model = ' / '.join(filter(None, [
                            fields_from_mapping.get('vehicleBrand'),
                            fields_from_mapping.get('vehicleModel')
                        ]))
                        if brand_model:
                            fields_from_mapping['vehicleBrandModel'] = brand_model
                    
                    # IMPORTANTE: Tamb√©m verificar campo combinado j√° mapeado
                    if fields_from_mapping.get('postalCodeCity') and not fields_from_mapping.get('postalCode'):
                        # Se mapeou o campo combinado, dividir
                        pcc = fields_from_mapping.get('postalCodeCity', '')
                        if ' / ' in pcc:
                            parts = pcc.split(' / ', 1)
                            fields_from_mapping['postalCode'] = parts[0].strip()
                            fields_from_mapping['city'] = parts[1].strip()
                    
                    if fields_from_mapping.get('vehicleBrandModel') and not fields_from_mapping.get('vehicleBrand'):
                        # Se mapeou o campo combinado, dividir
                        vbm = fields_from_mapping.get('vehicleBrandModel', '')
                        if ' / ' in vbm:
                            parts = vbm.split(' / ', 1)
                            fields_from_mapping['vehicleBrand'] = parts[0].strip()
                            fields_from_mapping['vehicleModel'] = parts[1].strip()
                    
                    # Calcular pa√≠s automaticamente baseado no c√≥digo postal
                    postal_code = fields_from_mapping.get('postalCode') or (fields_from_mapping.get('postalCodeCity', '').split(' / ')[0] if ' / ' in fields_from_mapping.get('postalCodeCity', '') else None)
                    
                    if postal_code and not fields_from_mapping.get('country'):
                        # Detectar pa√≠s pelo c√≥digo postal
                        country_detected = None
                        
                        # Portugal: 1000-001, 4000-123
                        if re.match(r'^\d{4}-\d{3}$', postal_code):
                            if postal_code[0] in '123456789':
                                country_detected = 'PORTUGAL'
                            else:
                                country_detected = 'ESPANHA'
                        # Espanha: 01234, 28001
                        elif re.match(r'^0\d{4}$', postal_code):
                            country_detected = 'ESPANHA'
                        # Reino Unido: SW1A 1AA, W1A 0AX
                        elif re.match(r'^[A-Z]{1,2}\d{1,2}[A-Z]?\s?\d[A-Z]{2}$', postal_code):
                            country_detected = 'UNITED KINGDOM'
                        # USA: 12345, 12345-6789
                        elif re.match(r'^\d{5}(?:-\d{4})?$', postal_code):
                            country_detected = 'USA'
                        # Fran√ßa: 75001, 69000, 13000, 31000
                        elif re.match(r'^\d{5}$', postal_code) and postal_code[0:2] in ['75', '69', '13', '31', '44', '33', '34', '35', '06', '76']:
                            country_detected = 'FRANCE'
                        # Alemanha: 10115, 80331
                        elif re.match(r'^\d{5}$', postal_code) and postal_code[0:2] in ['10', '20', '30', '40', '50', '60', '70', '80', '90', '01', '02', '03', '04', '14', '15']:
                            country_detected = 'GERMANY'
                        # It√°lia: 00100, 20100
                        elif re.match(r'^\d{5}$', postal_code) and postal_code[0:2] in ['00', '20', '10', '50', '40', '16', '70', '80', '90']:
                            country_detected = 'ITALY'
                        # Holanda: 1012 AB, 1012AB
                        elif re.match(r'^\d{4}\s?[A-Z]{2}$', postal_code):
                            country_detected = 'NETHERLANDS'
                        # B√©lgica: 1000, 2000
                        elif re.match(r'^[1-9]\d{3}$', postal_code):
                            country_detected = 'BELGIUM'
                        # Su√≠√ßa: 8001, 1200
                        elif re.match(r'^\d{4}$', postal_code) and postal_code[0] in ['1', '2', '3', '4', '5', '6', '7', '8', '9']:
                            country_detected = 'SWITZERLAND'
                        # Canad√°: K1A 0B1
                        elif re.match(r'^[A-Z]\d[A-Z]\s?\d[A-Z]\d$', postal_code):
                            country_detected = 'CANADA'
                        
                        if country_detected:
                            fields_from_mapping['country'] = country_detected
                            logging.info(f"   üåç Pa√≠s detectado automaticamente: {country_detected} (de c√≥digo postal: {postal_code})")
                    
                    # ‚úÖ SEMPRE retornar se extraiu QUALQUER campo das coordenadas
                    # N√£O usar fallback de padr√µes se coordenadas est√£o configuradas
                    print("\n" + "="*80)
                    print(f"‚úÖ EXTRA√á√ÉO CONCLU√çDA: {len(fields_from_mapping)} campos extra√≠dos")
                    print("="*80)
                    for field_id, value in fields_from_mapping.items():
                        print(f"   ‚Ä¢ {field_id}: {value[:50] if len(value) > 50 else value}")
                    print("="*80)
                    
                    # MAPEAR CAMPOS DO RA PARA DAMAGE REPORT (nomes corretos)
                    # O frontend espera estes nomes exatos de campos
                    dr_fields = {}
                    
                    # Mapear cada campo extra√≠do para o nome correto do DR
                    field_mapping = {
                        'contractNumber': 'contractNumber',      # N√∫mero do Contrato ‚Üí DR N¬∫ Contrato (= RA Number)
                        'clientName': 'clientName',              # Nome do Cliente
                        'clientEmail': 'clientEmail',            # Email do Cliente  
                        'clientPhone': 'clientPhone',            # Telefone do Cliente
                        'address': 'address',                    # Morada
                        'city': 'city',                          # Cidade
                        'postalCode': 'postalCode',              # C√≥digo Postal
                        'postalCodeCity': 'postalCodeCity',      # C√≥digo Postal / Cidade (combinado)
                        'country': 'country',                    # Pa√≠s
                        'vehiclePlate': 'vehiclePlate',          # Matr√≠cula
                        'vehicleBrand': 'vehicleBrand',          # Marca
                        'vehicleModel': 'vehicleModel',          # Modelo
                        'vehicleBrandModel': 'vehicleBrandModel', # Marca / Modelo (combinado)
                        'pickupDate': 'pickupDate',              # Data de Levantamento
                        'pickupTime': 'pickupTime',              # Hora de Levantamento
                        'pickupLocation': 'pickupLocation',      # Local de Levantamento
                        'pickupFuel': 'pickupFuel',              # Combust√≠vel Levantamento
                        'returnDate': 'returnDate',              # Data de Devolu√ß√£o
                        'returnTime': 'returnTime',              # Hora de Devolu√ß√£o
                        'returnLocation': 'returnLocation',      # Local de Devolu√ß√£o
                        'returnFuel': 'returnFuel',              # Combust√≠vel Devolu√ß√£o
                    }
                    
                    # Copiar campos mapeados
                    for ra_field, dr_field in field_mapping.items():
                        if ra_field in fields_from_mapping and fields_from_mapping[ra_field]:
                            dr_fields[dr_field] = fields_from_mapping[ra_field]
                            logging.info(f"   ‚úÖ Mapeado: {ra_field} ‚Üí {dr_field} = {fields_from_mapping[ra_field][:50]}")
                    
                    # Limpar matr√≠cula: remover espa√ßos entre n√∫meros e letras
                    # "3 0 - X Q - 9 7" ‚Üí "30-XQ-97"
                    if 'vehiclePlate' in dr_fields:
                        plate = dr_fields['vehiclePlate']
                        # Remover espa√ßos mas manter h√≠fens
                        plate_clean = plate.replace(' ', '')
                        dr_fields['vehiclePlate'] = plate_clean
                        logging.info(f"   üöó Matr√≠cula limpa: '{plate}' ‚Üí '{plate_clean}'")
                    
                    # Se tiver postalCodeCity combinado, dividir
                    if 'postalCodeCity' in dr_fields and ' / ' in dr_fields['postalCodeCity']:
                        parts = dr_fields['postalCodeCity'].split(' / ', 1)
                        if not dr_fields.get('postalCode'):
                            dr_fields['postalCode'] = parts[0].strip()
                        if not dr_fields.get('city') and len(parts) > 1:
                            dr_fields['city'] = parts[1].strip()
                    
                    # Se tiver vehicleBrandModel combinado, dividir
                    if 'vehicleBrandModel' in dr_fields and ' / ' in dr_fields['vehicleBrandModel']:
                        parts = dr_fields['vehicleBrandModel'].split(' / ', 1)
                        if not dr_fields.get('vehicleBrand'):
                            dr_fields['vehicleBrand'] = parts[0].strip()
                        if not dr_fields.get('vehicleModel') and len(parts) > 1:
                            dr_fields['vehicleModel'] = parts[1].strip()
                    
                    # ‚úÖ COPIAR CONTRACT NUMBER PARA RA NUMBER (s√£o o mesmo campo!)
                    # Contract Number = Rental Agreement Number = ex: 06424-09
                    if 'contractNumber' in dr_fields and dr_fields['contractNumber']:
                        dr_fields['raNumber'] = dr_fields['contractNumber']
                        logging.info(f"   ‚úÖ RA Number = Contract Number: {dr_fields['raNumber']}")
                    
                    logging.info(f"‚úÖ SUCESSO: {len(dr_fields)} campos mapeados para Damage Report")
                    logging.info("   ‚ö° Retornando campos prontos para inserir no DR")
                    return {"ok": True, "fields": dr_fields, "method": "mapped_coordinates"}
        
            else:
                # N√£o tinha coordenadas mapeadas
                print("\n‚ö†Ô∏è  NENHUMA COORDENADA MAPEADA ENCONTRADA!")
                print("   üëâ Acesse /rental-agreement-mapper para configurar\n")
                logging.info("‚ö†Ô∏è  Nenhuma coordenada mapeada encontrada no banco")
                logging.info("   üëâ Acesse /rental-agreement-mapper para configurar")
        
        except Exception as e:
            print("\n" + "="*80)
            print(f"‚ùå ERRO NA EXTRA√á√ÉO POR COORDENADAS: {e}")
            print("="*80)
            logging.error(f"‚ùå Erro ao extrair usando coordenadas: {e}")
            import traceback
            logging.error(traceback.format_exc())
            # Continuar para m√©todo fallback (padr√µes)
        
        # M√âTODO 2: Extra√ß√£o INTELIGENTE por PADR√ïES (robusta para tamanhos vari√°veis)
        print("\n" + "="*80)
        print("‚ö†Ô∏è  USANDO FALLBACK: EXTRA√á√ÉO POR PADR√ïES")
        print("="*80)
        logging.info("üìÑ Using PATTERN-BASED intelligent extraction")
        reader = PyPDF2.PdfReader(pdf_file)
        
        # Extrair todo o texto
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        
        # Dividir em linhas para an√°lise contextual
        lines = text.split('\n')
        
        def clean_spaces(text):
            """Remove espa√ßos extras entre caracteres"""
            # Remove espa√ßos entre letras/n√∫meros individuais
            # Ex: "3 0 - X Q - 9 7" ‚Üí "30-XQ-97"
            cleaned = re.sub(r'(\w)\s+(?=\w)', r'\1', text)
            # Remove espa√ßos ao redor de h√≠fens
            cleaned = re.sub(r'\s*-\s*', '-', cleaned)
            return cleaned
        
        def extract_country_code(text):
            """Extrai c√≥digo do pa√≠s (ex: DE, PT, ES)"""
            match = re.search(r'\b([A-Z]{2})\b', text)
            return match.group(1) if match else text.strip()
        
        def split_postal_city(text):
            """Divide c√≥digo postal e cidade da mesma linha"""
            # Padr√µes comuns: "8000-000 FARO", "12345 LISBOA", etc
            match = re.match(r'([\d-]+)\s+(.+)', text)
            if match:
                return match.group(1).strip(), match.group(2).strip()
            return text, ""
        
        def extract_email_phone(text):
            """Extrai email e telefone da mesma linha"""
            email = ""
            phone = ""
            
            email_match = re.search(r'([A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,})', text, re.IGNORECASE)
            if email_match:
                email = email_match.group(1).lower()
            
            # Telefone: 9 d√≠gitos
            phone_match = re.search(r'(\d{9})', text)
            if phone_match:
                phone = phone_match.group(1)
            
            return email, phone
        
        def extract_brand_model(text):
            """Extrai marca e modelo da mesma linha"""
            # Ex: "PEUGEOT 308" ou "VOLKSWAGEN GOLF"
            parts = text.split()
            if len(parts) >= 2:
                return parts[0], ' '.join(parts[1:])
            return text, ""
        
        def clean_time(text):
            """Limpa hora removendo n√∫meros extras"""
            # Ex: "12 : 009  8  4  1  3" ‚Üí "12:00"
            match = re.search(r'(\d{1,2})\s*:\s*(\d{2})', text)
            if match:
                return f"{match.group(1)}:{match.group(2)}"
            return text.strip()
        
        def extract_time_location(text):
            """Extrai hora e local da mesma linha (Linha 25)"""
            # Ex: "12:00 AEROPORTO FARO"
            match = re.match(r'([\d:]+)\s+(.+)', text)
            if match:
                time = clean_time(match.group(1))
                location = match.group(2).strip()
                return time, location
            return "", text
        
        fields = {}
        
        # === 1. N√öMERO DO CONTRATO ===
        # Padr√£o: XXXXX-VV (5 d√≠gitos, h√≠fen, vers√£o de 2 d√≠gitos)
        # Exemplo: 00000-09, 12345-01, etc.
        contract_match = re.search(r'\b(\d{5}-\d{2})\b', text)
        if contract_match:
            fields['contractNumber'] = contract_match.group(1)
            logging.info(f"   üìù Contrato: {fields['contractNumber']}")
        
        # === 2. EMAIL ===
        # Padr√£o universal de email
        email_match = re.search(r'\b([A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,})\b', text, re.IGNORECASE)
        if email_match:
            fields['clientEmail'] = email_match.group(1).lower()
            logging.info(f"   üìß Email: {fields['clientEmail']}")
        
        # === 3. TELEFONE ===
        # Padr√£o: Com ou sem indicativo internacional
        # Exemplos: +351 912345678, 912345678, +34 600123456
        phone_patterns = [
            r'\+(\d{1,4})\s*(\d{9})',  # Com indicativo: +351 912345678
            r'(\d{9})\s+[A-Z0-9._%+-]+@',  # Antes do email
            r'(?<!\d)(\d{9})(?!\d)',  # 9 d√≠gitos isolados
        ]
        for i, pattern in enumerate(phone_patterns):
            phone_match = re.search(pattern, text)
            if phone_match:
                if i == 0:  # Com indicativo
                    country_code = phone_match.group(1)
                    phone_number = phone_match.group(2)
                    fields['clientPhone'] = f"+{country_code} {phone_number}"
                    logging.info(f"   üìû Telefone: {fields['clientPhone']}")
                    break
                else:
                    # Sem indicativo
                    phone = phone_match.group(1)
                    # Validar que n√£o √© NIF (geralmente come√ßa com 1,2,3,5,6,8)
                    if not (phone.startswith(('1', '2', '3', '5', '6', '8')) and len(phone) == 9):
                        fields['clientPhone'] = phone
                        logging.info(f"   üìû Telefone: {phone}")
                        break
        
        # === 4. NOME DO CLIENTE ===
        # Estrat√©gia: linha AP√ìS o contrato e ANTES do pa√≠s
        # Geralmente √© uma linha com APENAS LETRAS MAI√öSCULAS
        if contract_match:
            contract_pos = text.find(contract_match.group(1))
            text_after_contract = text[contract_pos + len(contract_match.group(1)):]
            
            # Procurar primeira linha que seja APENAS letras mai√∫sculas e espa√ßos (2-50 chars)
            name_match = re.search(r'\n\s*([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á\s]{3,50}?)\s*\n', text_after_contract)
            if name_match:
                name = name_match.group(1).strip()
                # Validar que n√£o tem n√∫meros
                if not re.search(r'\d', name) and len(name.split()) >= 2:
                    fields['clientName'] = name
                    logging.info(f"   üë§ Nome: {name}")
        
        # === 5. PA√çS ===
        # C√≥digo de 2 letras isolado (DE, PT, ES, FR, etc.)
        # Lista expandida de c√≥digos ISO 3166-1 alpha-2
        country_match = re.search(r'\b([A-Z]{2})\b', text)
        if country_match:
            country_code = country_match.group(1)
            # Validar que √© c√≥digo de pa√≠s conhecido (ISO 3166-1 alpha-2)
            known_countries = [
                # Europa
                'PT', 'ES', 'FR', 'DE', 'IT', 'UK', 'GB', 'NL', 'BE', 'CH', 'AT', 'IE', 'PL',
                'SE', 'NO', 'DK', 'FI', 'GR', 'CZ', 'HU', 'RO', 'BG', 'SK', 'HR', 'SI', 'LT',
                'LV', 'EE', 'LU', 'MT', 'CY', 'IS', 'LI', 'MC', 'AD', 'SM', 'VA',
                # Am√©rica
                'US', 'CA', 'MX', 'BR', 'AR', 'CL', 'CO', 'PE', 'VE', 'EC', 'UY', 'PY', 'BO',
                # √Åsia
                'CN', 'JP', 'IN', 'KR', 'TH', 'MY', 'SG', 'ID', 'PH', 'VN', 'TR', 'IL', 'AE',
                'SA', 'QA', 'KW', 'BH', 'OM', 'JO', 'LB',
                # Oceania
                'AU', 'NZ',
                # √Åfrica
                'ZA', 'EG', 'MA', 'TN', 'DZ', 'NG', 'KE', 'GH',
            ]
            if country_code in known_countries:
                fields['country'] = country_code
                logging.info(f"   üåç Pa√≠s: {country_code}")
        
        # === 6. C√ìDIGO POSTAL E CIDADE ===
        # Detectar por padr√µes de c√≥digo postal MUNDIAL
        postal_patterns = [
            # Portugal: 8000-000, 1000-001
            (r'\b(\d{4}-\d{3})\b', 'PORTUGAL'),
            # Espanha: 28001, 08001
            (r'\b([0-5]\d{4})\b', 'SPAIN'),
            # Reino Unido: SW1A 1AA, M1 1AE, B33 8TH
            (r'\b([A-Z]{1,2}\d{1,2}[A-Z]?\s?\d[A-Z]{2})\b', 'UK'),
            # Estados Unidos: 12345, 12345-6789
            (r'\b(\d{5}(?:-\d{4})?)\b', 'USA'),
            # Fran√ßa: 75001, 13008
            (r'\b([0-9]{5})\b', 'FRANCE'),
            # Alemanha: 10115, 80331
            (r'\b([0-9]{5})\b', 'GERMANY'),
            # It√°lia: 00100, 20100
            (r'\b([0-9]{5})\b', 'ITALY'),
            # Holanda: 1012 AB, 1012AB
            (r'\b(\d{4}\s?[A-Z]{2})\b', 'NETHERLANDS'),
            # B√©lgica: 1000, 2000
            (r'\b([1-9]\d{3})\b', 'BELGIUM'),
            # Su√≠√ßa: 8001, 1200
            (r'\b([1-9]\d{3})\b', 'SWITZERLAND'),
            # Canad√°: K1A 0B1, H2X 1Y7
            (r'\b([A-Z]\d[A-Z]\s?\d[A-Z]\d)\b', 'CANADA'),
            # Irlanda: D02 AF30, A65 F4E2
            (r'\b([A-Z]\d{2}\s?[A-Z0-9]{4})\b', 'IRELAND'),
            # Pol√≥nia: 00-950, 31-002
            (r'\b(\d{2}-\d{3})\b', 'POLAND'),
            # √Åustria: 1010, 5020
            (r'\b([1-9]\d{3})\b', 'AUSTRIA'),
            # Gr√©cia: 104 32, 546 21
            (r'\b(\d{3}\s?\d{2})\b', 'GREECE'),
        ]
        
        for pattern, country_hint in postal_patterns:
            postal_match = re.search(pattern, text)
            if postal_match:
                postal_code = postal_match.group(1)
                fields['postalCode'] = postal_code
                logging.info(f"   üìÆ C√≥digo Postal: {postal_code}")
                
                # Procurar cidade ANTES do c√≥digo postal (mesma linha)
                # MELHORADO: Procura texto em MAI√öSCULAS antes do c√≥digo postal
                # Evita pegar n√∫meros (XXXXX) ou texto aleat√≥rio
                city_pattern = r'([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á√ñ√Ñ√úSS][A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á√ñ√Ñ√ú√ü\s-]{2,40}?)\s+' + re.escape(postal_code)
                city_match = re.search(city_pattern, text)
                if city_match:
                    city = city_match.group(1).strip()
                    # Validar que n√£o √© s√≥ n√∫meros ou X's
                    if not re.match(r'^[X0-9\s/-]+$', city):
                        fields['city'] = city
                        logging.info(f"   üèôÔ∏è  Cidade: {city}")
                
                # Se n√£o encontrou pa√≠s ainda, usar hint
                if not fields.get('country'):
                    fields['country'] = country_hint
                break
        
        # === 7. MORADA ===
        # Procurar por palavras-chave MULTI-IDIOMA
        address_keywords = [
            # Portugu√™s
            'RUA', 'AVENIDA', 'TRAVESSA', 'LARGO', 'PRA√áA', 'URBANIZA√á√ÉO', 'ESTRADA',
            # Alem√£o
            'STRA√üE', 'STRASSE', 'WEG', 'PLATZ', 'ALLEE', 'GASSE',
            # Espanhol
            'CALLE', 'AVENIDA', 'PLAZA', 'CAMINO',
            # Ingl√™s
            'STREET', 'AVENUE', 'ROAD', 'LANE', 'DRIVE', 'BOULEVARD',
            # Franc√™s
            'RUE', 'AVENUE', 'BOULEVARD', 'PLACE', 'CHEMIN',
            # Italiano
            'VIA', 'VIALE', 'CORSO', 'PIAZZA', 'STRADA',
        ]
        for keyword in address_keywords:
            address_match = re.search(rf'\b({keyword}[^\n]{{1,80}})', text, re.IGNORECASE)
            if address_match:
                addr = address_match.group(1).strip()
                # Limitar tamanho e remover excesso
                if len(addr) < 100:
                    fields['address'] = addr
                    logging.info(f"   üè† Morada: {addr}")
                    break
        
        # === 8. MATR√çCULA ===
        # Padr√£o portugu√™s: XX-XX-XX (com ou sem espa√ßos)
        plate_patterns = [
            r'([A-Z]{1,2}\s*-\s*\d{2}\s*-\s*[A-Z]{2})',  # Com espa√ßos
            r'([0-9]{2}\s*-\s*[A-Z]{2}\s*-\s*[0-9]{2})',  # Novo formato
        ]
        for pattern in plate_patterns:
            plate_match = re.search(pattern, text)
            if plate_match:
                plate = clean_spaces(plate_match.group(1))
                fields['vehiclePlate'] = plate
                logging.info(f"   üöó Matr√≠cula: {plate}")
                break
        
        # === 9. MARCA E MODELO DO VE√çCULO ===
        # Procurar marcas conhecidas seguidas de modelo
        known_brands = ['PEUGEOT', 'RENAULT', 'CITROEN', 'VOLKSWAGEN', 'VW', 'FORD', 'OPEL',
                       'SEAT', 'FIAT', 'TOYOTA', 'NISSAN', 'HYUNDAI', 'KIA', 'BMW', 'MERCEDES',
                       'AUDI', 'SKODA', 'MAZDA', 'HONDA', 'SUZUKI', 'DACIA', 'VOLVO']
        
        for brand in known_brands:
            brand_pattern = rf'\b({brand})\s+([A-Z0-9\s.]+?)(?=\s*\n|$)'
            brand_match = re.search(brand_pattern, text)
            if brand_match:
                fields['vehicleBrand'] = brand_match.group(1)
                fields['vehicleModel'] = brand_match.group(2).strip()
                logging.info(f"   üè≠ Marca: {fields['vehicleBrand']}")
                logging.info(f"   üöô Modelo: {fields['vehicleModel']}")
                break
        
        # === 10. DATAS E HORAS POR CONTEXTO ===
        # NOVA ABORDAGEM: Usar palavras-chave para encontrar datas e horas corretas
        from datetime import datetime
        current_year = datetime.now().year
        
        # Palavras-chave para identificar se√ß√µes de Entrega (Pickup) e Recolha (Dropoff)
        pickup_keywords = ['ENTREGA', 'PICKUP', 'LEVANTAMENTO', 'SALIDA', 'DEPARTURE', 'ABHOL']
        dropoff_keywords = ['RECOLHA', 'DROPOFF', 'DEVOLU√á√ÉO', 'DEVOLUCION', 'RETURN', 'R√úCKGABE']
        
        # Procurar por contexto de ENTREGA/LEVANTAMENTO
        pickup_context = None
        for keyword in pickup_keywords:
            match = re.search(rf'{keyword}', text, re.IGNORECASE)
            if match:
                pickup_context = match.start()
                logging.info(f"   ‚úÖ Contexto Levantamento encontrado: {keyword}")
                break
        
        # Procurar por contexto de RECOLHA/DEVOLU√á√ÉO
        dropoff_context = None
        for keyword in dropoff_keywords:
            match = re.search(rf'{keyword}', text, re.IGNORECASE)
            if match:
                dropoff_context = match.start()
                logging.info(f"   ‚úÖ Contexto Devolu√ß√£o encontrado: {keyword}")
                break
        
        # Padr√µes de data
        date_pattern = r'(\d{2}\s*[/-]\s*\d{2}\s*[/-]\s*\d{4})'
        time_pattern = r'(\d{1,2}\s*:\s*\d{2})'
        
        # === LEVANTAMENTO (PICKUP) ===
        if pickup_context is not None:
            # Procurar data AP√ìS a palavra-chave de pickup (pr√≥ximos 200 caracteres)
            pickup_section = text[pickup_context:pickup_context + 300]
            
            # Data
            date_match = re.search(date_pattern, pickup_section)
            if date_match:
                date_clean = re.sub(r'\s+', '', date_match.group(1)).replace('/', '-')
                try:
                    day, month, year = map(int, date_clean.split('-'))
                    if current_year - 1 <= year <= current_year + 2:
                        fields['pickupDate'] = date_clean
                        logging.info(f"   üìÖ Data Levantamento: {date_clean}")
                except:
                    pass
            
            # Hora (logo ap√≥s a data)
            time_match = re.search(time_pattern, pickup_section)
            if time_match:
                fields['pickupTime'] = clean_time(time_match.group(1))
                logging.info(f"   üïê Hora Levantamento: {fields['pickupTime']}")
            
            # Local (MAI√öSCULAS pr√≥ximo da palavra-chave, antes da data)
            location_match = re.search(r'\b([A-Z√Ñ√ñ√ú][A-Z√Ñ√ñ√ú√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á\s]{5,50}?)\s+\d{2}\s*[/-]', pickup_section)
            if location_match:
                location = location_match.group(1).strip()
                words = location.split()
                has_business_keyword = re.search(r'AEROPORTO|AIRPORT|FLUGHAFEN|AUTO|RENT|STATION|PRUDENTE|CAR|CARS|HIRE', location, re.IGNORECASE)
                
                # VALIDA√á√ïES:
                # 1. Rejeitar se parece c√≥digo/data (ex: "DE 16", "PT 20")
                is_code_like = bool(re.match(r'^[A-Z]{2}\s+\d+$', location))
                # 2. Rejeitar se s√≥ tem sigla + n√∫mero (menos de 6 chars)
                is_too_short = len(location.replace(' ', '')) < 6
                # 3. Rejeitar se for exatamente 2 palavras simples SEM palavra-chave
                is_person_name = (len(words) == 2 and not has_business_keyword)
                
                if not (is_code_like or is_too_short or is_person_name):
                    fields['pickupLocation'] = location
                    logging.info(f"   üìç Local Levantamento: {location}")
                else:
                    reason = "c√≥digo/sigla" if is_code_like else "muito curto" if is_too_short else "prov√°vel nome"
                    logging.info(f"   ‚ö†Ô∏è  Local rejeitado ({reason}): {location}")
        
        # === DEVOLU√á√ÉO (DROPOFF) ===
        if dropoff_context is not None:
            # Procurar data AP√ìS a palavra-chave de dropoff (pr√≥ximos 200 caracteres)
            dropoff_section = text[dropoff_context:dropoff_context + 300]
            
            # Data
            date_match = re.search(date_pattern, dropoff_section)
            if date_match:
                date_clean = re.sub(r'\s+', '', date_match.group(1)).replace('/', '-')
                try:
                    day, month, year = map(int, date_clean.split('-'))
                    if current_year - 1 <= year <= current_year + 2:
                        fields['dropoffDate'] = date_clean
                        logging.info(f"   üìÖ Data Devolu√ß√£o: {date_clean}")
                except:
                    pass
            
            # Hora (logo ap√≥s a data)
            time_match = re.search(time_pattern, dropoff_section)
            if time_match:
                fields['dropoffTime'] = clean_time(time_match.group(1))
                logging.info(f"   üïê Hora Devolu√ß√£o: {fields['dropoffTime']}")
            
            # Local (MAI√öSCULAS pr√≥ximo da palavra-chave, antes da data)
            location_match = re.search(r'\b([A-Z√Ñ√ñ√ú][A-Z√Ñ√ñ√ú√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á\s]{5,50}?)\s+\d{2}\s*[/-]', dropoff_section)
            if location_match:
                location = location_match.group(1).strip()
                words = location.split()
                has_business_keyword = re.search(r'AEROPORTO|AIRPORT|FLUGHAFEN|AUTO|RENT|STATION|PRUDENTE|CAR|CARS|HIRE', location, re.IGNORECASE)
                
                # VALIDA√á√ïES:
                # 1. Rejeitar se parece c√≥digo/data (ex: "DE 16", "PT 20")
                is_code_like = bool(re.match(r'^[A-Z]{2}\s+\d+$', location))
                # 2. Rejeitar se s√≥ tem sigla + n√∫mero (menos de 6 chars)
                is_too_short = len(location.replace(' ', '')) < 6
                # 3. Rejeitar se for exatamente 2 palavras simples SEM palavra-chave
                is_person_name = (len(words) == 2 and not has_business_keyword)
                
                if not (is_code_like or is_too_short or is_person_name):
                    fields['dropoffLocation'] = location
                    logging.info(f"   üìç Local Devolu√ß√£o: {location}")
                else:
                    reason = "c√≥digo/sigla" if is_code_like else "muito curto" if is_too_short else "prov√°vel nome"
                    logging.info(f"   ‚ö†Ô∏è  Local rejeitado ({reason}): {location}")
        
        # === 12. FALLBACK PARA LOCAIS ===
        # Se n√£o foram encontrados locais por contexto, tentar m√©todo antigo
        if not fields.get('pickupLocation') or not fields.get('dropoffLocation'):
            locations_found = []
            for i, line in enumerate(lines):
                # Se linha tem data, a linha anterior pode ser local
                if re.search(r'\d{2}\s*[/-]\s*\d{2}\s*[/-]\s*\d{4}', line):
                    if i > 0:
                        prev_line = lines[i-1].strip()
                        # Remover horas do in√≠cio
                        prev_line = re.sub(r'^\d{1,2}\s*:\s*\d{2}\s*', '', prev_line)
                        # Se √© mai√∫sculas e tem tamanho razo√°vel e n√£o √© nome de pessoa
                        if prev_line.isupper() and 5 <= len(prev_line) <= 50:
                            words = prev_line.split()
                            has_business_keyword = re.search(r'AEROPORTO|AIRPORT|AUTO|RENT|STATION|PRUDENTE|CAR|CARS|HIRE', prev_line, re.IGNORECASE)
                            
                            # VALIDA√á√ïES:
                            is_code_like = bool(re.match(r'^[A-Z]{2}\s+\d+', prev_line))
                            is_too_short = len(prev_line.replace(' ', '')) < 6
                            is_person_name = (len(words) == 2 and not has_business_keyword)
                            
                            # ACEITA se N√ÉO for c√≥digo, muito curto ou nome de pessoa
                            if not (is_code_like or is_too_short or is_person_name):
                                locations_found.append(prev_line)
            
            if len(locations_found) >= 2:
                if not fields.get('pickupLocation'):
                    fields['pickupLocation'] = locations_found[0]
                    logging.info(f"   üìç Local Levantamento (fallback): {locations_found[0]}")
                if not fields.get('dropoffLocation'):
                    fields['dropoffLocation'] = locations_found[1]
                    logging.info(f"   üìç Local Devolu√ß√£o (fallback): {locations_found[1]}")
            elif len(locations_found) == 1:
                # Mesmo local
                if not fields.get('pickupLocation'):
                    fields['pickupLocation'] = locations_found[0]
                if not fields.get('dropoffLocation'):
                    fields['dropoffLocation'] = locations_found[0]
                logging.info(f"   üìç Local (ambos, fallback): {locations_found[0]}")
        
        # COMBINAR CAMPOS para Damage Report
        # C√≥digo Postal / Cidade
        if fields.get('postalCode') or fields.get('city'):
            postal_code_city = ' / '.join(filter(None, [fields.get('postalCode'), fields.get('city')]))
            if postal_code_city:
                fields['postalCodeCity'] = postal_code_city
        
        # Marca / Modelo
        if fields.get('vehicleBrand') or fields.get('vehicleModel'):
            brand_model = ' / '.join(filter(None, [fields.get('vehicleBrand'), fields.get('vehicleModel')]))
            if brand_model:
                fields['vehicleBrandModel'] = brand_model
        
        # ‚úÖ COPIAR CONTRACT NUMBER PARA RA NUMBER (s√£o o mesmo campo!)
        # Contract Number = Rental Agreement Number = ex: 06424-09
        if 'contractNumber' in fields and fields['contractNumber']:
            fields['raNumber'] = fields['contractNumber']
            logging.info(f"   ‚úÖ RA Number = Contract Number: {fields['raNumber']}")
        
        # Log para debug
        logging.info(f"‚úÖ === CAMPOS EXTRA√çDOS (PATTERN-BASED) ===")
        for key, value in fields.items():
            logging.info(f"   {key}: {value}")
        
        return {"ok": True, "fields": fields, "method": "pattern_based"}
        
    except Exception as e:
        logging.error(f"Error extracting RA fields: {e}")
        import traceback
        return {"ok": False, "error": str(e), "traceback": traceback.format_exc()}

@app.post("/api/rental-agreements/debug-lines")
async def debug_rental_agreement_lines(request: Request, file: UploadFile = File(...)):
    """DEBUG: Extrai TODAS as linhas do PDF numeradas para mapear campos"""
    require_auth(request)
    
    try:
        import PyPDF2
        from io import BytesIO
        
        contents = await file.read()
        pdf_file = BytesIO(contents)
        reader = PyPDF2.PdfReader(pdf_file)
        
        # Extrair TODO o texto
        full_text = ""
        for page_num, page in enumerate(reader.pages, 1):
            page_text = page.extract_text()
            full_text += f"\n=== P√ÅGINA {page_num} ===\n{page_text}\n"
        
        # Dividir em linhas
        lines = full_text.split('\n')
        
        # Numerar linhas
        numbered_lines = []
        for i, line in enumerate(lines, 1):
            numbered_lines.append({
                "line_number": i,
                "text": line.strip(),
                "length": len(line.strip())
            })
        
        logging.info(f"üìÑ DEBUG: Extra√≠das {len(numbered_lines)} linhas do PDF")
        
        return {
            "ok": True,
            "total_lines": len(numbered_lines),
            "lines": numbered_lines,
            "full_text": full_text
        }
        
    except Exception as e:
        logging.error(f"Error debugging RA lines: {e}")
        import traceback
        return {"ok": False, "error": str(e), "traceback": traceback.format_exc()}

@app.post("/api/damage-reports/create")
async def create_damage_report(request: Request):
    """Cria um novo Damage Report"""
    print("üî•üî•üî• ENDPOINT HIT - ANTES DE TUDO")
    logging.error("üíæüíæüíæ CREATE DAMAGE REPORT - IN√çCIO")
    print("üî•üî•üî• ANTES REQUIRE_AUTH")
    require_auth(request)
    print("üî•üî•üî• DEPOIS REQUIRE_AUTH")
    
    try:
        logging.error("üíæ Recebendo JSON...")
        data = await request.json()
        logging.error(f"üíæ JSON recebido: {len(data)} campos")
        
        logging.error("üíæ Entrando no _db_lock...")
        with _db_lock:
            logging.error("üíæ Lock adquirido, conectando BD...")
            conn = _db_connect()
            try:
                # Detectar tipo de BD para CREATE TABLE correto (usar type.__name__ para evitar problemas de isinstance)
                is_postgres = type(conn).__name__ == 'PostgreSQLConnectionWrapper'
                logging.error(f"üíæ BD detectado: {'PostgreSQL' if is_postgres else 'SQLite'} (wrapper={type(conn).__name__}, is_postgres={is_postgres})")
                
                if is_postgres:
                    # PostgreSQL - Usar SERIAL, BYTEA, TIMESTAMP
                    logging.error("üíæ Executando CREATE TABLE (PostgreSQL)...")
                    cur = conn.cursor()
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_reports (
                            id SERIAL PRIMARY KEY,
                                dr_number TEXT UNIQUE,
                                ra_number TEXT,
                                contract_number TEXT,
                                date DATE,
                                client_name TEXT,
                                client_email TEXT,
                                client_phone TEXT,
                                client_address TEXT,
                                client_city TEXT,
                                client_postal_code TEXT,
                                client_country TEXT,
                                vehicle_plate TEXT,
                                vehicle_model TEXT,
                                vehicle_brand TEXT,
                                pickup_date TIMESTAMP,
                                pickup_time TEXT,
                                pickup_location TEXT,
                                return_date TIMESTAMP,
                                return_time TEXT,
                                return_location TEXT,
                                issued_by TEXT,
                                inspection_type TEXT,
                                inspector_name TEXT,
                                mileage INTEGER,
                                fuel_level TEXT,
                                damage_description TEXT,
                                observations TEXT,
                                damage_diagram_data TEXT,
                                repair_items TEXT,
                                damage_images TEXT,
                                total_amount REAL,
                                status TEXT DEFAULT 'draft',
                                pdf_data BYTEA,
                                pdf_filename TEXT,
                                is_protected INTEGER DEFAULT 0,
                                is_deleted INTEGER DEFAULT 0,
                                deleted_at TIMESTAMP,
                                deleted_by TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                created_by TEXT,
                                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            )
                        """)
                    conn.commit()
                    cur.close()
                    logging.error("üíæ CREATE TABLE completado!")
                else:
                    # SQLite - Usar AUTOINCREMENT, BLOB, DATETIME
                    logging.error("üíæ Executando CREATE TABLE (SQLite)...")
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS damage_reports (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            dr_number TEXT UNIQUE,
                            ra_number TEXT,
                            contract_number TEXT,
                            date DATE,
                            client_name TEXT,
                            client_email TEXT,
                            client_phone TEXT,
                            client_address TEXT,
                            client_city TEXT,
                            client_postal_code TEXT,
                            client_country TEXT,
                            vehicle_plate TEXT,
                            vehicle_model TEXT,
                            vehicle_brand TEXT,
                            pickup_date DATETIME,
                            pickup_time TEXT,
                            pickup_location TEXT,
                            return_date DATETIME,
                            return_time TEXT,
                            return_location TEXT,
                            issued_by TEXT,
                            inspection_type TEXT,
                            inspector_name TEXT,
                            mileage INTEGER,
                            fuel_level TEXT,
                            damage_description TEXT,
                            observations TEXT,
                            damage_diagram_data TEXT,
                            repair_items TEXT,
                            damage_images TEXT,
                            total_amount REAL,
                            status TEXT DEFAULT 'draft',
                            pdf_data BLOB,
                            pdf_filename TEXT,
                            is_protected INTEGER DEFAULT 0,
                            is_deleted INTEGER DEFAULT 0,
                            deleted_at TIMESTAMP,
                            deleted_by TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            created_by TEXT,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    logging.error("üíæ CREATE TABLE completado (SQLite)!")
                
                # Adicionar colunas pdf_data e pdf_filename se n√£o existirem (para tabelas antigas)
                # PostgreSQL: Verificar se colunas existem ANTES de adicionar (evita abort da transa√ß√£o)
                # (is_postgres j√° foi detectado acima)
                
                if is_postgres:
                    # PostgreSQL: Verificar colunas primeiro
                    cur = conn.cursor()
                    cur.execute("""
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = 'damage_reports'
                    """)
                    existing_columns = [row[0] for row in cur.fetchall()]
                    
                    if 'pdf_data' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN pdf_data BYTEA")
                        logging.info("‚úÖ Coluna pdf_data adicionada (PostgreSQL)")
                    
                    if 'pdf_filename' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN pdf_filename TEXT")
                        logging.info("‚úÖ Coluna pdf_filename adicionada (PostgreSQL)")
                    
                    if 'is_protected' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN is_protected INTEGER DEFAULT 0")
                        logging.info("‚úÖ Coluna is_protected adicionada (PostgreSQL)")
                    
                    if 'is_deleted' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN is_deleted INTEGER DEFAULT 0")
                        logging.info("‚úÖ Coluna is_deleted adicionada (PostgreSQL)")
                    
                    if 'deleted_at' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN deleted_at TIMESTAMP")
                        logging.info("‚úÖ Coluna deleted_at adicionada (PostgreSQL)")
                    
                    if 'deleted_by' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN deleted_by TEXT")
                        logging.info("‚úÖ Coluna deleted_by adicionada (PostgreSQL)")
                    
                    if 'client_country' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN client_country TEXT")
                        logging.info("‚úÖ Coluna client_country adicionada (PostgreSQL)")
                    
                    if 'pickup_time' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN pickup_time TEXT")
                        logging.info("‚úÖ Coluna pickup_time adicionada (PostgreSQL)")
                    
                    if 'return_time' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN return_time TEXT")
                        logging.info("‚úÖ Coluna return_time adicionada (PostgreSQL)")
                    
                    if 'issued_by' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN issued_by TEXT")
                        logging.info("‚úÖ Coluna issued_by adicionada (PostgreSQL)")
                    
                    if 'damage_images' not in existing_columns:
                        cur.execute("ALTER TABLE damage_reports ADD COLUMN damage_images TEXT")
                        logging.info("‚úÖ Coluna damage_images adicionada (PostgreSQL)")
                    
                    conn.commit()
                    cur.close()
                else:
                    # SQLite: Usar try/except (mais simples)
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN pdf_data BLOB")
                        logging.info("‚úÖ Coluna pdf_data adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN pdf_filename TEXT")
                        logging.info("‚úÖ Coluna pdf_filename adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN is_protected INTEGER DEFAULT 0")
                        logging.info("‚úÖ Coluna is_protected adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN is_deleted INTEGER DEFAULT 0")
                        logging.info("‚úÖ Coluna is_deleted adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN deleted_at TIMESTAMP")
                        logging.info("‚úÖ Coluna deleted_at adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN deleted_by TEXT")
                        logging.info("‚úÖ Coluna deleted_by adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN client_country TEXT")
                        logging.info("‚úÖ Coluna client_country adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN pickup_time TEXT")
                        logging.info("‚úÖ Coluna pickup_time adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN return_time TEXT")
                        logging.info("‚úÖ Coluna return_time adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN issued_by TEXT")
                        logging.info("‚úÖ Coluna issued_by adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN damage_images TEXT")
                        logging.info("‚úÖ Coluna damage_images adicionada (SQLite)")
                    except Exception:
                        pass  # Coluna j√° existe
                
                # Dividir campos combinados
                # C√≥digo Postal / Cidade
                postal_code_city = data.get('postalCodeCity', '')
                if postal_code_city and ' / ' in postal_code_city:
                    parts = postal_code_city.split(' / ', 1)
                    data['postalCode'] = parts[0].strip()
                    data['city'] = parts[1].strip()
                else:
                    data['postalCode'] = data.get('postalCode', postal_code_city)
                    data['city'] = data.get('city', '')
                
                # Marca / Modelo
                brand_model = data.get('vehicleBrandModel', '')
                if brand_model and ' / ' in brand_model:
                    parts = brand_model.split(' / ', 1)
                    data['vehicleBrand'] = parts[0].strip()
                    data['vehicleModel'] = parts[1].strip()
                else:
                    data['vehicleBrand'] = data.get('vehicleBrand', '')
                    data['vehicleModel'] = data.get('vehicleModel', brand_model)
                
                # Verificar se √© UPDATE (dr_number fornecido) ou CREATE (novo)
                existing_dr_number = data.get('existingDRNumber')
                
                if existing_dr_number:
                    # UPDATE - Atualizar DR existente
                    logging.info(f"üîÑ Atualizando DR {existing_dr_number}")
                    
                    # Processar imagem do ve√≠culo com danos (diagrama com pins)
                    vehicle_damage_image_blob = None
                    # Frontend envia 'vehicleDiagram', n√£o 'vehicleDamageImage'
                    if data.get('vehicleDiagram') or data.get('vehicleDamageImage'):
                        import base64
                        image_data = data.get('vehicleDiagram') or data.get('vehicleDamageImage')
                        if image_data.startswith('data:image'):
                            image_data = image_data.split(',')[1]
                        vehicle_damage_image_blob = base64.b64decode(image_data)
                    
                    # Helper: converter string vazia para None (NULL no SQL)
                    def empty_to_none(value):
                        return None if value == '' else value
                    
                    update_values = (
                        empty_to_none(data.get('raNumber')),  # ‚úÖ Frontend envia raNumber (camelCase)
                        empty_to_none(data.get('contractNumber')),
                        empty_to_none(data.get('date')),
                        empty_to_none(data.get('clientName')),
                        empty_to_none(data.get('clientEmail')),
                        empty_to_none(data.get('clientPhone')),
                        empty_to_none(data.get('address')),
                        empty_to_none(data.get('city')),
                        empty_to_none(data.get('postalCode')),
                        empty_to_none(data.get('country')),
                        empty_to_none(data.get('vehiclePlate')),
                        empty_to_none(data.get('vehicleModel')),
                        empty_to_none(data.get('vehicleBrand')),
                        empty_to_none(data.get('vehicleColor')),
                        empty_to_none(data.get('vehicleKm')),
                        empty_to_none(data.get('pickupDate')),  # ‚úÖ NULL se vazio
                        empty_to_none(data.get('pickupTime')),  # ‚úÖ NULL se vazio
                        empty_to_none(data.get('pickupLocation')),
                        empty_to_none(data.get('returnDate')),  # ‚úÖ NULL se vazio
                        empty_to_none(data.get('returnTime')),  # ‚úÖ NULL se vazio
                        empty_to_none(data.get('returnLocation')),
                        empty_to_none(data.get('issuedBy')),
                        empty_to_none(data.get('inspectionType')),
                        empty_to_none(data.get('inspectorName')),
                        empty_to_none(data.get('mileage')),
                        empty_to_none(data.get('fuelPickup')),
                        empty_to_none(data.get('fuelReturn')),
                        empty_to_none(data.get('totalCost')),
                        empty_to_none(data.get('inspectionDate')),  # ‚úÖ NULL se vazio
                        empty_to_none(data.get('damageDescription')),
                        empty_to_none(data.get('observations')),
                        empty_to_none(data.get('damageDiagramData')),
                        empty_to_none(data.get('repairItems')),
                        empty_to_none(data.get('damageImages')),
                        vehicle_damage_image_blob,
                        datetime.now().isoformat(),
                        existing_dr_number
                    )
                    
                    if conn.__class__.__module__ == 'psycopg2.extensions':
                        # PostgreSQL
                        with conn.cursor() as cur:
                            cur.execute("""
                                UPDATE damage_reports SET
                                    ra_number = %s, contract_number = %s, date = %s,
                                    client_name = %s, client_email = %s, client_phone = %s,
                                    client_address = %s, client_city = %s, client_postal_code = %s, client_country = %s,
                                    vehicle_plate = %s, vehicle_model = %s, vehicle_brand = %s,
                                    vehicle_color = %s, vehicle_km = %s,
                                    pickup_date = %s, pickup_time = %s, pickup_location = %s,
                                    return_date = %s, return_time = %s, return_location = %s,
                                    issued_by = %s,
                                    inspection_type = %s, inspector_name = %s, mileage = %s,
                                    fuel_pickup = %s, fuel_return = %s, total_cost = %s, inspection_date = %s,
                                    damage_description = %s, observations = %s, damage_diagram_data = %s,
                                    repair_items = %s, damage_images = %s,
                                    vehicle_damage_image = %s,
                                    updated_at = %s
                                WHERE dr_number = %s
                            """, update_values)
                        conn.commit()
                    else:
                        # SQLite
                        conn.execute("""
                            UPDATE damage_reports SET
                                ra_number = ?, contract_number = ?, date = ?,
                                client_name = ?, client_email = ?, client_phone = ?,
                                client_address = ?, client_city = ?, client_postal_code = ?, client_country = ?,
                                vehicle_plate = ?, vehicle_model = ?, vehicle_brand = ?,
                                vehicle_color = ?, vehicle_km = ?,
                                pickup_date = ?, pickup_time = ?, pickup_location = ?,
                                return_date = ?, return_time = ?, return_location = ?,
                                issued_by = ?,
                                inspection_type = ?, inspector_name = ?, mileage = ?,
                                fuel_pickup = ?, fuel_return = ?, total_cost = ?, inspection_date = ?,
                                damage_description = ?, observations = ?, damage_diagram_data = ?,
                                repair_items = ?, damage_images = ?,
                                vehicle_damage_image = ?,
                                updated_at = ?
                            WHERE dr_number = ?
                        """, update_values)
                        conn.commit()
                    
                    return {"ok": True, "dr_number": existing_dr_number, "message": "Damage Report updated successfully"}
                else:
                    # CREATE - Criar novo DR ou reciclar n√∫mero eliminado
                    # ‚ùå N√ÉO FECHAR conn aqui - manter conex√£o aberta e usar diretamente
                    # conn.close() causava deadlock com _get_next_dr_number() que tamb√©m usa _db_lock
                    
                    # Gerar DR number usando a conex√£o j√° aberta
                    dr_number = _get_next_dr_number(existing_conn=conn)  # Usar vers√£o que recebe conn
                    
                    # Verificar se √© um n√∫mero reciclado (j√° existe na tabela com is_deleted = 1)
                    is_postgres = (
                        isinstance(conn, PostgreSQLConnectionWrapper) or 
                        conn.__class__.__module__ == 'psycopg2.extensions' or
                        type(conn).__name__ == 'PostgreSQLConnectionWrapper'
                    )
                    is_recycled = False
                    logging.error(f"üíæ Verificando reciclagem do DR {dr_number}... is_postgres={is_postgres}")
                    
                    if is_postgres:
                        cur = conn.cursor()
                        cur.execute("SELECT id FROM damage_reports WHERE dr_number = %s AND is_deleted = 1", (dr_number,))
                        is_recycled = cur.fetchone() is not None
                        cur.close()
                    else:
                        cursor = conn.execute("SELECT id FROM damage_reports WHERE dr_number = ? AND is_deleted = 1", (dr_number,))
                        is_recycled = cursor.fetchone() is not None
                    
                    if is_recycled:
                        logging.info(f"‚ôªÔ∏è  Reciclando n√∫mero DR eliminado: {dr_number}")
                    else:
                        logging.info(f"‚ú® Criando novo DR: {dr_number}")
                    
                    # Processar imagem do ve√≠culo com danos (diagrama com pins)
                    vehicle_damage_image_blob = None
                    # Frontend envia 'vehicleDiagram', n√£o 'vehicleDamageImage'
                    if data.get('vehicleDiagram') or data.get('vehicleDamageImage'):
                        import base64
                        image_data = data.get('vehicleDiagram') or data.get('vehicleDamageImage')
                        if image_data.startswith('data:image'):
                            image_data = image_data.split(',')[1]
                        vehicle_damage_image_blob = base64.b64decode(image_data)
                    
                    # Helper: converter string vazia para None (NULL no SQL)
                    def empty_to_none(value):
                        return None if value == '' else value
                    
                    if is_recycled:
                        # UPDATE - Reutilizar registo eliminado e resetar flags de elimina√ß√£o
                        update_values = (
                            empty_to_none(data.get('raNumber')),  # ‚úÖ Frontend envia raNumber (camelCase)
                            empty_to_none(data.get('contractNumber')),
                            empty_to_none(data.get('date')),
                            empty_to_none(data.get('clientName')),
                            empty_to_none(data.get('clientEmail')),
                            empty_to_none(data.get('clientPhone')),
                            empty_to_none(data.get('address')),
                            empty_to_none(data.get('city')),
                            empty_to_none(data.get('postalCode')),
                            empty_to_none(data.get('country')),
                            empty_to_none(data.get('vehiclePlate')),
                            empty_to_none(data.get('vehicleModel')),
                            empty_to_none(data.get('vehicleBrand')),
                            empty_to_none(data.get('pickupDate')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('pickupTime')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('pickupLocation')),
                            empty_to_none(data.get('returnDate')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('returnTime')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('returnLocation')),
                            empty_to_none(data.get('issuedBy')),
                            empty_to_none(data.get('inspectionType')),
                            empty_to_none(data.get('inspectorName')),
                            empty_to_none(data.get('mileage')),
                            empty_to_none(data.get('fuelLevel')),
                            empty_to_none(data.get('damageDescription')),
                            empty_to_none(data.get('observations')),
                            empty_to_none(data.get('damageDiagramData')),
                            empty_to_none(data.get('repairItems')),
                            empty_to_none(data.get('damageImages')),
                            vehicle_damage_image_blob,
                            request.session.get('username', 'unknown'),
                            datetime.now().isoformat(),
                            dr_number
                        )
                        
                        if is_postgres:
                            with conn.cursor() as cur:
                                cur.execute("""
                                    UPDATE damage_reports SET
                                        ra_number = %s, contract_number = %s, date = %s,
                                        client_name = %s, client_email = %s, client_phone = %s,
                                        client_address = %s, client_city = %s, client_postal_code = %s, client_country = %s,
                                        vehicle_plate = %s, vehicle_model = %s, vehicle_brand = %s,
                                        pickup_date = %s, pickup_time = %s, pickup_location = %s,
                                        return_date = %s, return_time = %s, return_location = %s,
                                        issued_by = %s,
                                        inspection_type = %s, inspector_name = %s, mileage = %s, fuel_level = %s,
                                        damage_description = %s, observations = %s, damage_diagram_data = %s,
                                        repair_items = %s, damage_images = %s,
                                        vehicle_damage_image = %s,
                                        created_by = %s,
                                        updated_at = %s,
                                        is_deleted = 0,
                                        deleted_at = NULL,
                                        deleted_by = NULL,
                                        status = 'draft',
                                        pdf_data = NULL,
                                        pdf_filename = NULL
                                    WHERE dr_number = %s
                                """, update_values)
                            conn.commit()
                        else:
                            conn.execute("""
                                UPDATE damage_reports SET
                                    ra_number = ?, contract_number = ?, date = ?,
                                    client_name = ?, client_email = ?, client_phone = ?,
                                    client_address = ?, client_city = ?, client_postal_code = ?, client_country = ?,
                                    vehicle_plate = ?, vehicle_model = ?, vehicle_brand = ?,
                                    pickup_date = ?, pickup_time = ?, pickup_location = ?,
                                    return_date = ?, return_time = ?, return_location = ?,
                                    issued_by = ?,
                                    inspection_type = ?, inspector_name = ?, mileage = ?, fuel_level = ?,
                                    damage_description = ?, observations = ?, damage_diagram_data = ?,
                                    repair_items = ?, damage_images = ?,
                                    vehicle_damage_image = ?,
                                    created_by = ?,
                                    updated_at = ?,
                                    is_deleted = 0,
                                    deleted_at = NULL,
                                    deleted_by = NULL,
                                    status = 'draft',
                                    pdf_data = NULL,
                                    pdf_filename = NULL
                                WHERE dr_number = ?
                            """, update_values)
                            conn.commit()
                        
                        logging.info(f"‚úÖ DR {dr_number} reciclado com sucesso - n√∫mero reutilizado")
                        return {"ok": True, "dr_number": dr_number, "message": "Damage Report created successfully (recycled number)"}
                    else:
                        # INSERT - Criar novo registo
                        logging.error(f"üíæ Preparando INSERT para DR: {dr_number}")
                        
                        # Helper: converter string vazia para None (NULL no SQL)
                        def empty_to_none(value):
                            return None if value == '' else value
                        
                        insert_values = (
                            dr_number,
                            empty_to_none(data.get('raNumber')),  # ‚úÖ Frontend envia raNumber (camelCase)
                            empty_to_none(data.get('contractNumber')),
                            empty_to_none(data.get('date')),
                            empty_to_none(data.get('clientName')),
                            empty_to_none(data.get('clientEmail')),
                            empty_to_none(data.get('clientPhone')),
                            empty_to_none(data.get('address')),
                            empty_to_none(data.get('city')),
                            empty_to_none(data.get('postalCode')),
                            empty_to_none(data.get('country')),
                            empty_to_none(data.get('vehiclePlate')),
                            empty_to_none(data.get('vehicleModel')),
                            empty_to_none(data.get('vehicleBrand')),
                            empty_to_none(data.get('vehicleColor')),
                            empty_to_none(data.get('vehicleKm')),
                            empty_to_none(data.get('pickupDate')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('pickupTime')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('pickupLocation')),
                            empty_to_none(data.get('returnDate')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('returnTime')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('returnLocation')),
                            empty_to_none(data.get('issuedBy')),
                            empty_to_none(data.get('inspectionType')),
                            empty_to_none(data.get('inspectorName')),
                            empty_to_none(data.get('mileage')),
                            empty_to_none(data.get('fuelPickup')),
                            empty_to_none(data.get('fuelReturn')),
                            empty_to_none(data.get('totalCost')),
                            empty_to_none(data.get('inspectionDate')),  # ‚úÖ NULL se vazio
                            empty_to_none(data.get('damageDescription')),
                            empty_to_none(data.get('observations')),
                            empty_to_none(data.get('damageDiagramData')),
                            empty_to_none(data.get('repairItems')),
                            empty_to_none(data.get('damageImages')),
                            vehicle_damage_image_blob,
                            request.session.get('username', 'unknown')
                        )
                        
                        if is_postgres:
                            # PostgreSQL
                            with conn.cursor() as cur:
                                cur.execute("""
                                    INSERT INTO damage_reports (
                                        dr_number, ra_number, contract_number, date,
                                        client_name, client_email, client_phone,
                                        client_address, client_city, client_postal_code, client_country,
                                        vehicle_plate, vehicle_model, vehicle_brand, vehicle_color, vehicle_km,
                                        pickup_date, pickup_time, pickup_location,
                                        return_date, return_time, return_location,
                                        issued_by,
                                        inspection_type, inspector_name, mileage,
                                        fuel_pickup, fuel_return, total_cost, inspection_date,
                                        damage_description, observations, damage_diagram_data,
                                        repair_items, damage_images,
                                        vehicle_damage_image,
                                        created_by
                                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                                """, insert_values)
                                logging.error("üíæ INSERT executado, fazendo COMMIT...")
                            conn.commit()
                            logging.error("üíæ COMMIT completado!")
                        else:
                            # SQLite
                            conn.execute("""
                                INSERT INTO damage_reports (
                                    dr_number, ra_number, contract_number, date,
                                    client_name, client_email, client_phone,
                                    client_address, client_city, client_postal_code, client_country,
                                    vehicle_plate, vehicle_model, vehicle_brand, vehicle_color, vehicle_km,
                                    pickup_date, pickup_time, pickup_location,
                                    return_date, return_time, return_location,
                                    issued_by,
                                    inspection_type, inspector_name, mileage,
                                    fuel_pickup, fuel_return, total_cost, inspection_date,
                                    damage_description, observations, damage_diagram_data,
                                    repair_items, damage_images,
                                    vehicle_damage_image,
                                    created_by
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, insert_values)
                            conn.commit()
                        
                        logging.error(f"üíæ DR {dr_number} criado! Retornando sucesso...")
                        logging.info(f"‚úÖ DR {dr_number} criado com sucesso - novo n√∫mero")
                        return {"ok": True, "dr_number": dr_number, "message": "Damage Report created successfully"}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error creating damage report: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/list")
async def list_damage_reports(request: Request):
    """Lista todos os Damage Reports"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                logging.info("üìã Listando Damage Reports...")
                
                # Verificar se √© PostgreSQL ou SQLite (m√©todo robusto)
                is_postgres = False
                try:
                    # Tentar detectar PostgreSQL pelo tipo de conex√£o
                    if hasattr(conn, 'server_version') or conn.__class__.__module__ in ['psycopg2.extensions', 'psycopg2._psycopg']:
                        is_postgres = True
                    elif hasattr(conn, 'cursor'):
                        # Se tem cursor(), √© psycopg2 (PostgreSQL)
                        try:
                            with conn.cursor() as test_cur:
                                is_postgres = True
                        except:
                            pass
                except:
                    pass
                
                logging.info(f"DB Type: {'PostgreSQL' if is_postgres else 'SQLite'}")
                
                # Verificar se a coluna is_protected existe
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = 'damage_reports'
                        """)
                        columns = [row[0] for row in cur.fetchall()]
                else:
                    cursor = conn.execute("PRAGMA table_info(damage_reports)")
                    columns = [row[1] for row in cursor.fetchall()]
                
                has_is_protected = 'is_protected' in columns
                
                # Construir query baseado no tipo de DB
                if is_postgres:
                    # PostgreSQL - usar POSITION e SUBSTRING
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT 
                                id, dr_number, ra_number, contract_number, date,
                                client_name, client_email, client_country, vehicle_plate, vehicle_model,
                                status, created_at, created_by, pdf_filename
                            FROM damage_reports
                            WHERE (is_deleted = 0 OR is_deleted IS NULL)
                            ORDER BY id DESC
                        """)
                        rows = cur.fetchall()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT 
                            id, dr_number, ra_number, contract_number, date,
                            client_name, client_email, client_country, vehicle_plate, vehicle_model,
                            status, created_at, created_by, pdf_filename
                        FROM damage_reports
                        WHERE (is_deleted = 0 OR is_deleted IS NULL)
                        ORDER BY id DESC
                    """)
                    rows = cursor.fetchall()
                
                logging.info(f"‚úÖ Encontrados {len(rows)} DRs na base de dados")
                
                reports = []
                for row in rows:
                    dr_number = row[1]
                    
                    # FOR√áAR prote√ß√£o para DRs 01-39/2025 e 01-03/2024
                    is_protected = False
                    if dr_number:
                        # Verificar se √© um dos DRs que fizeste upload (sem espa√ßo: DR01/2025)
                        for i in range(1, 40):
                            if dr_number == f"DR{i:02d}/2025":
                                is_protected = True
                                break
                        for i in range(1, 4):
                            if dr_number == f"DR{i:02d}/2024":
                                is_protected = True
                                break
                    
                    report = {
                        'id': row[0],
                        'dr_number': dr_number,
                        'ra_number': row[2],
                        'contract_number': row[3],
                        'date': row[4],
                        'client_name': row[5],
                        'client_email': row[6],
                        'country': row[7],
                        'vehicle_plate': row[8],
                        'vehicle_model': row[9],
                        'status': row[10],
                        'created_at': row[11],
                        'created_by': row[12],
                        'has_pdf': row[13] is not None,
                        'pdf_filename': row[13],
                        'is_protected': is_protected
                    }
                    
                    reports.append(report)
                
                return {"ok": True, "reports": reports}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error listing damage reports: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/numbering/get")
async def get_dr_numbering(request: Request):
    """Obter configura√ß√£o de numera√ß√£o atual"""
    require_auth(request)
    
    try:
        from datetime import datetime
        import psycopg2
        
        logging.info(f"üìñ [DR-NUMBERING] GET Request received")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = isinstance(conn, psycopg2.extensions.connection)
                db_type = "PostgreSQL" if is_postgres else "SQLite"
                logging.info(f"üìä [DR-NUMBERING] Database: {db_type}")
                
                row = None
                
                if is_postgres:
                    # PostgreSQL
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT current_year, current_number, prefix, updated_at
                        FROM damage_report_numbering
                        WHERE id = 1
                    """)
                    row = cursor.fetchone()
                    cursor.close()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT current_year, current_number, prefix, updated_at
                        FROM damage_report_numbering
                        WHERE id = 1
                    """)
                    row = cursor.fetchone()
                
                if row:
                    current_year, current_number, prefix, updated_at = row
                    next_number = f"{prefix}{current_number + 1:02d}/{current_year}"
                    logging.info(f"‚úÖ [DR-NUMBERING] GET Success: current={current_number}, prefix={prefix}, next={next_number}")
                    return {
                        "ok": True,
                        "current_year": current_year,
                        "current_number": current_number,
                        "prefix": prefix,
                        "next_number": next_number,
                        "updated_at": updated_at,
                        "database": db_type
                    }
                else:
                    # Criar registro inicial se n√£o existir
                    logging.warning("‚ö†Ô∏è [DR-NUMBERING] Not found in DB, creating initial record...")
                    current_year = datetime.now().year
                    current_number = 0
                    prefix = 'DR'
                    
                    if is_postgres:
                        # PostgreSQL
                        with conn.cursor() as cur:
                            cur.execute("""
                                INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                                SELECT 1, %s, %s, %s
                                WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                            """, (current_year, current_number, prefix))
                            rows_affected = cur.rowcount
                            logging.info(f"üìù [DR-NUMBERING] INSERT: {rows_affected} row(s) inserted")
                    else:
                        # SQLite
                        cursor = conn.execute("""
                            INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                            SELECT 1, ?, ?, ?
                            WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                        """, (current_year, current_number, prefix))
                        rows_affected = cursor.rowcount
                        logging.info(f"üìù [DR-NUMBERING] INSERT: {rows_affected} row(s) inserted")
                    
                    conn.commit()
                    logging.info(f"üíæ [DR-NUMBERING] COMMIT executed")
                    
                    next_number = f"{prefix}{current_number + 1:02d}/{current_year}"
                    logging.info(f"‚úÖ [DR-NUMBERING] Created initial: next={next_number}")
                    
                    return {
                        "ok": True,
                        "current_year": current_year,
                        "current_number": current_number,
                        "prefix": prefix,
                        "next_number": next_number,
                        "updated_at": datetime.now().isoformat(),
                        "database": db_type
                    }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå [DR-NUMBERING] GET Error: {e}", exc_info=True)
        return {"ok": False, "error": str(e)}

# ‚ö†Ô∏è IMPORTANTE: Estes endpoints DEVEM vir ANTES do endpoint gen√©rico /{dr_number:path}
# para que FastAPI fa√ßa match correto de URLs espec√≠ficas

@app.get("/api/damage-reports/email-template/{lang}")
async def get_email_template_by_lang(request: Request, lang: str):
    """Get email template for specific language"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT id, language_code, language_name, subject_template, body_template, updated_at
                            FROM dr_email_templates
                            WHERE language_code = %s
                        """, (lang,))
                        row = cur.fetchone()
                else:
                    cursor = conn.execute("""
                        SELECT id, language_code, language_name, subject_template, body_template, updated_at
                        FROM dr_email_templates
                        WHERE language_code = ?
                    """, (lang,))
                    row = cursor.fetchone()
                
                if not row:
                    return {"ok": False, "error": f"Template for language '{lang}' not found"}
                
                template = {
                    'id': row[0],
                    'language_code': row[1],
                    'language_name': row[2],
                    'subject_template': row[3],
                    'body_template': row[4],
                    'updated_at': row[5]
                }
                
                return {"ok": True, "template": template}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting email template for {lang}: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/{dr_number:path}/pdf")
async def download_damage_report_pdf(request: Request, dr_number: str, preview: bool = False):
    """Download ou Preview do Damage Report em PDF - Usa template mapeado com coordenadas"""
    logging.error(f"üî•üî•üî• ENDPOINT HIT! DR='{dr_number}', preview={preview} - ANTES REQUIRE_AUTH")
    require_auth(request)
    logging.error(f"üî•üî•üî• DEPOIS REQUIRE_AUTH - Executando c√≥digo...")
    
    try:
        from starlette.responses import Response
        import json
        
        logging.error(f"üìÑ ==================== PDF GENERATION START ====================")
        logging.error(f"üìÑ Generating PDF for DR: '{dr_number}'")
        logging.error(f"üìÑ DR number length: {len(dr_number)}")
        logging.error(f"üìÑ DR number bytes: {dr_number.encode('utf-8')}")
        
        # Buscar dados da base de dados
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar PostgreSQL vs SQLite
                is_postgres = False
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    is_postgres = True
                
                logging.info(f"DB Type: {'PostgreSQL' if is_postgres else 'SQLite'}")
                logging.info(f"Searching for DR: '{dr_number}' (length: {len(dr_number)})")
                
                if is_postgres:
                    cursor = conn.cursor()
                    cursor.execute("SELECT * FROM damage_reports WHERE dr_number = %s AND (is_deleted = 0 OR is_deleted IS NULL)", (dr_number,))
                    row = cursor.fetchone()
                    columns = [desc[0] for desc in cursor.description]
                    cursor.close()
                else:
                    cursor = conn.execute("SELECT * FROM damage_reports WHERE dr_number = ? AND (is_deleted = 0 OR is_deleted IS NULL)", (dr_number,))
                    row = cursor.fetchone()
                    columns = [desc[0] for desc in cursor.description]
                
                if not row:
                    # Debug: Listar DRs dispon√≠veis
                    if is_postgres:
                        cur_debug = conn.cursor()
                        cur_debug.execute("SELECT dr_number FROM damage_reports WHERE (is_deleted = 0 OR is_deleted IS NULL) LIMIT 5")
                        available = cur_debug.fetchall()
                        cur_debug.close()
                    else:
                        cur_debug = conn.execute("SELECT dr_number FROM damage_reports WHERE (is_deleted = 0 OR is_deleted IS NULL) LIMIT 5")
                        available = cur_debug.fetchall()
                    
                    logging.error(f"‚ùå DR '{dr_number}' not found in database")
                    logging.error(f"Available DRs: {[r[0] for r in available]}")
                    raise HTTPException(status_code=404, detail="Damage Report not found")
                
                logging.error(f"‚úÖ DR found in database!")
                logging.error(f"‚úÖ Row columns: {columns[:10]}...")  # Primeiras 10 colunas
                report = dict(zip(columns, row))
                logging.error(f"‚úÖ Report dict created with {len(report)} fields")
            finally:
                conn.close()
        
        # Formatar datas para dd-mm-yyyy (remover hora se existir)
        def format_date(date_str):
            if not date_str:
                return ''
            try:
                from datetime import datetime
                # Se j√° √© objeto datetime
                if isinstance(date_str, datetime):
                    return date_str.strftime('%d-%m-%Y')
                # Se √© string
                date_str = str(date_str)
                # Remove hora se existir (ex: "2025-11-06 00:00:00" ‚Üí "2025-11-06")
                # Ou "2025-11-06T00:00:00" ‚Üí "2025-11-06"
                date_only = date_str.split(' ')[0].split('T')[0]
                # Converte de yyyy-mm-dd para dd-mm-yyyy
                dt = datetime.strptime(date_only, '%Y-%m-%d')
                return dt.strftime('%d-%m-%Y')
            except:
                return str(date_str)  # Retorna original se falhar
        
        # Mapear campos da BD para IDs do mapeador (camelCase)
        report_data = {
            'dr_number': report.get('dr_number', ''),
            'ra_number': report.get('ra_number', ''),
            'contractNumber': report.get('contract_number', ''),
            'date': format_date(report.get('date', '')),
            'created_at': format_date(report.get('created_at', '')),
            'inspection_date': format_date(report.get('date', '')),
            'clientName': report.get('client_name', ''),
            'clientEmail': report.get('client_email', ''),
            'clientPhone': report.get('client_phone', ''),
            'address': report.get('client_address', ''),
            'city': report.get('client_city', ''),
            'postalCode': report.get('client_postal_code', ''),
            'country': report.get('client_country', ''),
            'customer_postal': ' / '.join(filter(None, [report.get('client_postal_code', ''), report.get('client_city', '')])),
            'customer_city': report.get('client_city', ''),
            'vehiclePlate': report.get('vehicle_plate', ''),
            'vehicleBrand': report.get('vehicle_brand', ''),
            'vehicleModel': report.get('vehicle_model', ''),
            'vehicleBrandModel': ' / '.join(filter(None, [report.get('vehicle_brand', ''), report.get('vehicle_model', '')])),
            'vehicleColor': report.get('vehicle_color', ''),
            'vehicleKm': report.get('vehicle_km', ''),
            'pickupDate': format_date(report.get('pickup_date', '')),
            'pickup_date': format_date(report.get('pickup_date', '')),
            'pickupTime': report.get('pickup_time', ''),
            'pickupLocation': report.get('pickup_location', ''),
            'returnDate': format_date(report.get('return_date', '')),
            'return_date': format_date(report.get('return_date', '')),
            'returnTime': report.get('return_time', ''),
            'returnLocation': report.get('return_location', ''),
            'fuel_level_pickup': report.get('fuel_pickup', ''),
            'fuel_level_return': report.get('fuel_return', ''),
            'total_repair_cost': report.get('total_amount', ''),
            'totalRepairCost': report.get('total_amount', ''),
            'total_amount': report.get('total_amount', ''),
            'inspector_name': report.get('issued_by', ''),
            'issued_by': report.get('issued_by', ''),
            'inspection_date': format_date(report.get('date', '')),
            'damage_diagram_data': report.get('damage_diagram_data', ''),
            'damageDiagramData': report.get('damage_diagram_data', '')
        }
        
        # ‚úÖ EXTRAIR DESCRI√á√ïES INDIVIDUAIS DOS DANOS (damage_1, damage_2, ...)
        damages_json = report.get('damage_diagram_data', '')
        if damages_json:
            try:
                damages = json.loads(damages_json)
                for damage in damages:
                    num = damage.get('number')
                    desc = damage.get('description', '')
                    if num:
                        report_data[f'damage_{num}'] = desc
                logging.info(f"‚úÖ Extra√≠das {len(damages)} descri√ß√µes de danos")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear damage_diagram_data")
        
        # ‚úÖ EXTRAIR FOTOS INDIVIDUAIS (damagePhoto1, damagePhoto2, ...)
        images_json = report.get('damage_images', '')
        if images_json:
            try:
                images = json.loads(images_json)
                for idx, image in enumerate(images):
                    photo_data = image.get('data', '')
                    report_data[f'damagePhoto{idx + 1}'] = photo_data
                    report_data[f'damage_photo_{idx + 1}'] = photo_data
                logging.info(f"‚úÖ Extra√≠das {len(images)} fotos")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear damage_images")
        
        # ‚úÖ EXTRAIR ITENS DE REPARA√á√ÉO (repair_line_1, repair_line_2, ...)
        repair_json = report.get('repair_items', '')
        total_calculated = 0.0
        if repair_json:
            try:
                repair_items = json.loads(repair_json)
                for idx, item in enumerate(repair_items):
                    line_num = idx + 1
                    report_data[f'repair_line_{line_num}'] = item.get('description', '')
                    report_data[f'repair_line_{line_num}_qty'] = str(item.get('quantity', ''))
                    hours_val = item.get('hours', '')
                    report_data[f'repair_line_{line_num}_hours'] = '-' if hours_val == 0 or hours_val == '0' else str(hours_val)
                    report_data[f'repair_line_{line_num}_price'] = str(item.get('price', ''))
                    report_data[f'repair_line_{line_num}_subtotal'] = str(item.get('total', ''))
                    # Somar ao total
                    total_calculated += float(item.get('total', 0))
                logging.info(f"‚úÖ Extra√≠dos {len(repair_items)} itens de repara√ß√£o")
                logging.info(f"üí∞ Total calculado: {total_calculated:.2f} ‚Ç¨")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear repair_items")
        
        # ‚úÖ SOBRESCREVER total_repair_cost com valor CALCULADO (igual ao preview)
        if total_calculated > 0:
            report_data['total_repair_cost'] = f"{total_calculated:.2f}"
            report_data['totalRepairCost'] = f"{total_calculated:.2f}"
            report_data['total_amount'] = f"{total_calculated:.2f}"
            logging.info(f"‚úÖ Total repair cost SOBRESCRITO: {total_calculated:.2f} ‚Ç¨")
        
        # ‚úÖ ADICIONAR CROQUI COM PINS
        vehicle_diagram_blob = report.get('vehicle_damage_image')
        if vehicle_diagram_blob:
            import base64
            try:
                diagram_base64 = base64.b64encode(vehicle_diagram_blob).decode('utf-8')
                report_data['vehicle_diagram'] = f'data:image/png;base64,{diagram_base64}'
                logging.info("‚úÖ Croqui com pins adicionado")
            except:
                logging.warning("‚ö†Ô∏è Erro ao converter vehicle_damage_image")
        
        # Usar fun√ß√£o de overlay para preencher template
        logging.error(f"üîß Calling _fill_template_pdf_with_data...")
        logging.error(f"üîß Report data keys: {list(report_data.keys())[:15]}...")
        
        pdf_data = _fill_template_pdf_with_data(report_data)
        
        logging.error(f"‚úÖ PDF generated! Size: {len(pdf_data)} bytes")
        filename = f"{dr_number.replace('/', '_').replace(':', '_')}.pdf"
        logging.error(f"üìÑ ==================== PDF GENERATION END ====================")
        logging.error(f"üìÑ Filename: {filename}")
        
        # Preview: inline (abre no browser) | Download: attachment (faz download)
        disposition = "inline" if preview else "attachment"
        
        return Response(
            content=pdf_data,
            media_type="application/pdf",
            headers={"Content-Disposition": f'{disposition}; filename="{filename}"'}
        )
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå‚ùå‚ùå CRITICAL ERROR in PDF generation: {e}")
        import traceback
        logging.error(f"‚ùå‚ùå‚ùå Full traceback:")
        logging.error(traceback.format_exc())
        logging.error(f"‚ùå‚ùå‚ùå Exception type: {type(e).__name__}")
        # Retornar erro em JSON com detalhe
        return JSONResponse({"ok": False, "error": f"Error generating PDF: {str(e)}"}, status_code=500)

@app.get("/api/damage-reports/{dr_number:path}")
async def get_damage_report(request: Request, dr_number: str):
    """Obt√©m um Damage Report espec√≠fico"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("SELECT * FROM damage_reports WHERE dr_number = %s", (dr_number,))
                        row = cur.fetchone()
                        columns = [desc[0] for desc in cur.description]
                else:
                    # SQLite
                    cursor = conn.execute("SELECT * FROM damage_reports WHERE dr_number = ?", (dr_number,))
                    row = cursor.fetchone()
                    columns = [desc[0] for desc in cursor.description]
                
                if not row:
                    return {"ok": False, "error": "Damage Report not found"}
                
                report = dict(zip(columns, row))
                
                # Converter tipos n√£o-serializ√°veis para JSON
                for key, value in report.items():
                    # Bytes/memoryview/buffer -> None
                    if isinstance(value, (bytes, memoryview, bytearray)):
                        report[key] = None
                    # Decimal -> float
                    elif hasattr(value, '__class__') and 'Decimal' in value.__class__.__name__:
                        report[key] = float(value)
                    # Date/datetime -> string ISO
                    elif hasattr(value, 'isoformat'):
                        report[key] = value.isoformat()
                
                return {"ok": True, "report": report}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting damage report: {e}")
        return {"ok": False, "error": str(e)}

def _get_next_dr_number(existing_conn=None):
    """Gerar pr√≥ximo n√∫mero de DR com reset anual autom√°tico e reciclagem de n√∫meros eliminados
    
    Args:
        existing_conn: Conex√£o existente (se None, cria nova com lock)
    """
    from datetime import datetime
    
    # Se j√° temos conex√£o, usar diretamente SEM lock (lock j√° adquirido pelo caller)
    if existing_conn:
        conn = existing_conn
        should_close = False
    else:
        # Criar nova conex√£o COM lock
        _db_lock.acquire()
        conn = _db_connect()
        should_close = True
    
    try:
        current_year = datetime.now().year
        # ‚úÖ DETEC√á√ÉO CORRETA: PostgreSQLConnectionWrapper ou psycopg2 (nome da classe tamb√©m)
        is_postgres = (
            isinstance(conn, PostgreSQLConnectionWrapper) or 
            conn.__class__.__module__ == 'psycopg2.extensions' or
            type(conn).__name__ == 'PostgreSQLConnectionWrapper'
        )
        
        logging.error(f"üî¢ _get_next_dr_number: is_postgres={is_postgres}, conn type={type(conn).__name__}")
        
        # 1. PRIORIDADE: Verificar se h√° n√∫meros eliminados dispon√≠veis para reutilizar
        logging.info("üîÑ Verificando n√∫meros DR eliminados dispon√≠veis para reciclagem...")
        
        if is_postgres:
            cur = conn.cursor()
            cur.execute("""
                SELECT dr_number 
                FROM damage_reports 
                WHERE is_deleted = 1 
                  AND EXTRACT(YEAR FROM deleted_at) = %s
                ORDER BY dr_number 
                LIMIT 1
            """, (current_year,))
            deleted_dr = cur.fetchone()
            cur.close()
        else:
            cursor = conn.execute("""
                SELECT dr_number 
                FROM damage_reports 
                WHERE is_deleted = 1 
                  AND strftime('%Y', deleted_at) = ?
                ORDER BY dr_number 
                LIMIT 1
            """, (str(current_year),))
            deleted_dr = cursor.fetchone()
        
        if deleted_dr:
            recycled_number = deleted_dr[0]
            logging.info(f"‚ôªÔ∏è Reciclando n√∫mero eliminado: {recycled_number}")
            return recycled_number
        
        logging.info("‚ú® Nenhum n√∫mero eliminado dispon√≠vel - Gerando novo n√∫mero sequencial")
        
        # 2. FALLBACK: Gerar novo n√∫mero sequencial
        # Obter configura√ß√£o atual
        if is_postgres:
            cur = conn.cursor()
            cur.execute("""
                SELECT current_year, current_number, prefix 
                FROM damage_report_numbering 
                WHERE id = 1
            """)
            row = cur.fetchone()
            cur.close()
        else:
            cursor = conn.execute("""
                SELECT current_year, current_number, prefix 
                FROM damage_report_numbering 
                WHERE id = 1
            """)
            row = cursor.fetchone()
        
        if not row:
            # Criar configura√ß√£o inicial APENAS se n√£o existir
            if is_postgres:
                cur = conn.cursor()
                cur.execute("""
                    INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                    SELECT 1, %s, 1, 'DR'
                    WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                """, (current_year,))
                conn.commit()
                cur.close()
            else:
                conn.execute("""
                    INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                    SELECT 1, ?, 1, 'DR'
                    WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                """, (current_year,))
                conn.commit()
            return f"DR01/{current_year}"
        
        saved_year, current_number, prefix = row
        
        # Verificar se mudou o ano (reset)
        if current_year > saved_year:
            # Novo ano - reset para 01
            new_number = 1
            if is_postgres:
                cur = conn.cursor()
                cur.execute("""
                    UPDATE damage_report_numbering 
                    SET current_year = %s, current_number = %s, updated_at = %s
                    WHERE id = 1
                """, (current_year, new_number, datetime.now().isoformat()))
                conn.commit()
                cur.close()
            else:
                conn.execute("""
                    UPDATE damage_report_numbering 
                    SET current_year = ?, current_number = ?, updated_at = ?
                    WHERE id = 1
                """, (current_year, new_number, datetime.now().isoformat()))
                conn.commit()
            return f"{prefix}{new_number:02d}/{current_year}"
        else:
            # Mesmo ano - incrementar
            new_number = current_number + 1
            if is_postgres:
                cur = conn.cursor()
                cur.execute("""
                    UPDATE damage_report_numbering 
                    SET current_number = %s, updated_at = %s
                    WHERE id = 1
                """, (new_number, datetime.now().isoformat()))
                conn.commit()
                cur.close()
            else:
                conn.execute("""
                    UPDATE damage_report_numbering 
                    SET current_number = ?, updated_at = ?
                    WHERE id = 1
                """, (new_number, datetime.now().isoformat()))
                conn.commit()
            return f"{prefix}{new_number:02d}/{current_year}"
    finally:
        # Fechar e liberar lock apenas se criamos a conex√£o aqui
        if should_close:
            conn.close()
            _db_lock.release()

def _ensure_damage_report_tables():
    """DEPRECATED - usar _ensure_damage_reports_tables() no startup"""
    # Esta fun√ß√£o n√£o faz nada - as tabelas s√£o criadas no startup
    pass

def _detect_language_from_country(country_code):
    """
    Detect language based on country code (ISO 3166-1 alpha-2)
    Returns: language_code (pt, en, fr, de)
    Fallback: English for all other countries
    """
    if not country_code:
        return 'pt'  # Default para Portugal
    
    country_code = country_code.upper().strip()
    
    # Mapeamento pa√≠s ‚Üí idioma (apenas PT, EN, FR, DE)
    language_map = {
        # Portugu√™s
        'PT': 'pt', 'BR': 'pt', 'AO': 'pt', 'MZ': 'pt',
        # Alem√£o
        'DE': 'de', 'AT': 'de', 'CH': 'de', 'LU': 'de', 'LI': 'de',
        # Franc√™s
        'FR': 'fr', 'BE': 'fr', 'MC': 'fr', 'SN': 'fr',
        # Ingl√™s (resto do mundo)
        'GB': 'en', 'US': 'en', 'IE': 'en', 'AU': 'en', 'NZ': 'en',
        'CA': 'en', 'ZA': 'en', 'IN': 'en', 'SG': 'en',
        # Espanhol ‚Üí Ingl√™s (sem template ES)
        'ES': 'en', 'AR': 'en', 'CL': 'en', 'CO': 'en', 'MX': 'en',
        'PE': 'en', 'VE': 'en', 'EC': 'en', 'GT': 'en', 'CU': 'en',
        'BO': 'en', 'DO': 'en', 'HN': 'en', 'PY': 'en', 'SV': 'en',
        'NI': 'en', 'CR': 'en', 'PA': 'en', 'UY': 'en',
        # Italiano ‚Üí Ingl√™s (sem template IT)
        'IT': 'en', 'SM': 'en', 'VA': 'en',
        # Holand√™s ‚Üí Ingl√™s (sem template NL)
        'NL': 'en',
    }
    
    detected = language_map.get(country_code, 'en')  # Default: Ingl√™s para pa√≠ses n√£o listados
    logging.info(f"üåç Country '{country_code}' ‚Üí Language '{detected}'")
    return detected

@app.get("/api/damage-reports/email-templates")
async def get_email_templates(request: Request):
    """List all email templates (all languages)"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT id, language_code, language_name, subject_template, body_template, updated_at
                            FROM dr_email_templates
                            ORDER BY language_code
                        """)
                        rows = cur.fetchall()
                else:
                    cursor = conn.execute("""
                        SELECT id, language_code, language_name, subject_template, body_template, updated_at
                        FROM dr_email_templates
                        ORDER BY language_code
                    """)
                    rows = cursor.fetchall()
                
                templates = []
                for row in rows:
                    templates.append({
                        'id': row[0],
                        'language_code': row[1],
                        'language_name': row[2],
                        'subject_template': row[3],
                        'body_template': row[4],
                        'updated_at': row[5]
                    })
                
                return {"ok": True, "templates": templates}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting email templates: {e}")
        return {"ok": False, "error": str(e)}

# Endpoint GET duplicado REMOVIDO - agora est√° na linha 16001 (ANTES do endpoint gen√©rico)

@app.post("/api/damage-reports/email-template/{lang}")
async def save_email_template(request: Request, lang: str):
    """Create or update email template for language"""
    require_auth(request)
    
    try:
        data = await request.json()
        language_name = data.get('language_name', '')
        subject_template = data.get('subject_template', '')
        body_template = data.get('body_template', '')
        
        if not language_name or not subject_template or not body_template:
            return {"ok": False, "error": "Missing required fields"}
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        # Upsert (INSERT ON CONFLICT UPDATE)
                        cur.execute("""
                            INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template, updated_at)
                            VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                            ON CONFLICT (language_code) DO UPDATE SET
                                language_name = EXCLUDED.language_name,
                                subject_template = EXCLUDED.subject_template,
                                body_template = EXCLUDED.body_template,
                                updated_at = CURRENT_TIMESTAMP
                        """, (lang, language_name, subject_template, body_template))
                else:
                    # SQLite - usar INSERT OR REPLACE
                    conn.execute("""
                        INSERT OR REPLACE INTO dr_email_templates (language_code, language_name, subject_template, body_template, updated_at)
                        VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                    """, (lang, language_name, subject_template, body_template))
                
                conn.commit()
                logging.info(f"‚úÖ Email template '{lang}' saved successfully")
                return {"ok": True, "message": f"Template for '{language_name}' saved successfully"}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error saving email template for {lang}: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/send-email")
async def send_damage_report_email(request: Request):
    """
    Send Damage Report via email using Gmail OAuth
    Detects language automatically from client country
    Supports attachments (PDF + images, max 20MB total)
    """
    require_auth(request)
    
    try:
        import base64
        from email.mime.multipart import MIMEMultipart
        from email.mime.text import MIMEText
        from email.mime.base import MIMEBase
        from email import encoders
        from googleapiclient.discovery import build
        from google.oauth2.credentials import Credentials
        
        data = await request.json()
        
        # Dados do DR
        dr_number = data.get('drNumber', '')
        ra_number = data.get('raNumber', '')  # ‚úÖ RA Number do frontend
        client_email = data.get('clientEmail', '')
        client_name = data.get('clientName', '')
        first_name = client_name.split()[0] if client_name else ''
        contract_number = data.get('contractNumber', '')
        vehicle_plate = data.get('vehiclePlate', '')
        date = data.get('date', '')
        country = data.get('country', 'PT')  # ISO 3166-1 code
        
        # üîç DEBUG: Verificar dados recebidos
        logging.info(f"üìß Email data received: DR={dr_number}, RA={ra_number}, Contract={contract_number}")
        
        # Anexos
        pdf_data_base64 = data.get('pdfData', '')  # Base64 do PDF
        attachments = data.get('attachments', [])  # Array de {name, data_base64, type}
        
        # Valida√ß√µes
        if not client_email:
            return {"ok": False, "error": "Client email is required"}
        
        if not dr_number:
            return {"ok": False, "error": "DR number is required"}
        
        # 1. Detectar idioma baseado no pa√≠s
        language_code = _detect_language_from_country(country)
        logging.info(f"üìß Sending DR email: {dr_number} to {client_email} (language: {language_code})")
        
        # 2. Obter template de email
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT subject_template, body_template
                            FROM dr_email_templates
                            WHERE language_code = %s
                        """, (language_code,))
                        row = cur.fetchone()
                else:
                    cursor = conn.execute("""
                        SELECT subject_template, body_template
                        FROM dr_email_templates
                        WHERE language_code = ?
                    """, (language_code,))
                    row = cursor.fetchone()
                
                if not row:
                    # Fallback para ingl√™s se idioma n√£o encontrado
                    logging.warning(f"‚ö†Ô∏è Template for '{language_code}' not found, using 'en'")
                    language_code = 'en'
                    if is_postgres:
                        with conn.cursor() as cur:
                            cur.execute("SELECT subject_template, body_template FROM dr_email_templates WHERE language_code = %s", (language_code,))
                            row = cur.fetchone()
                    else:
                        cursor = conn.execute("SELECT subject_template, body_template FROM dr_email_templates WHERE language_code = ?", (language_code,))
                        row = cursor.fetchone()
                
                if not row:
                    return {"ok": False, "error": "No email templates configured"}
                
                subject_template = row[0]
                body_template = row[1]
            finally:
                conn.close()
        
        # 3. Substituir par√¢metros nos templates
        params = {
            'drNumber': dr_number,
            'raNumber': ra_number,  # ‚úÖ RA Number para template de email
            'firstName': first_name,
            'contractNumber': contract_number,
            'vehiclePlate': vehicle_plate,
            'date': date,
            'email': client_email
        }
        
        subject = subject_template
        body = body_template
        for key, value in params.items():
            subject = subject.replace(f'{{{key}}}', value or '')
            body = body.replace(f'{{{key}}}', value or '')
        
        # 4. Carregar token OAuth do Gmail
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT access_token, refresh_token, expires_at
                            FROM oauth_tokens
                            WHERE provider = 'google'
                            ORDER BY id DESC LIMIT 1
                        """)
                        token_row = cur.fetchone()
                else:
                    cursor = conn.execute("""
                        SELECT access_token, refresh_token, expires_at
                        FROM oauth_tokens
                        WHERE provider = 'google'
                        ORDER BY id DESC LIMIT 1
                    """)
                    token_row = cursor.fetchone()
                
                if not token_row:
                    return {"ok": False, "error": "Gmail not connected. Please configure OAuth in Admin Settings."}
                
                access_token = token_row[0]
                refresh_token = token_row[1]
            finally:
                conn.close()
        
        # 5. Criar mensagem MIME
        message = MIMEMultipart()
        message['To'] = client_email
        message['Subject'] = subject
        
        # Corpo do email (HTML template j√° formatado da BD)
        message.attach(MIMEText(body, 'html'))
        
        # 6. Anexar PDF do DR
        if pdf_data_base64:
            try:
                # Remover prefixo data:application/pdf;base64, se houver
                if ',' in pdf_data_base64:
                    pdf_data_base64 = pdf_data_base64.split(',')[1]
                
                pdf_bytes = base64.b64decode(pdf_data_base64)
                
                part = MIMEBase('application', 'pdf')
                part.set_payload(pdf_bytes)
                encoders.encode_base64(part)
                # ‚úÖ Nome do ficheiro: DR number formatado (ex: DR41_2025.pdf)
                pdf_filename = f"{dr_number.replace('/', '_')}.pdf"
                part.add_header('Content-Disposition', f'attachment; filename="{pdf_filename}"')
                message.attach(part)
                
                logging.info(f"üìé PDF attached: {len(pdf_bytes)} bytes")
            except Exception as e:
                logging.error(f"Error attaching PDF: {e}")
        
        # 7. Anexar outros arquivos (imagens, etc)
        for att in attachments:
            try:
                att_name = att.get('name', 'attachment')
                att_data = att.get('data', '')
                att_type = att.get('type', 'application/octet-stream')
                
                if ',' in att_data:
                    att_data = att_data.split(',')[1]
                
                att_bytes = base64.b64decode(att_data)
                
                # Validar tamanho (m√°x 20MB por anexo)
                if len(att_bytes) > 20 * 1024 * 1024:
                    logging.warning(f"‚ö†Ô∏è Attachment '{att_name}' too large ({len(att_bytes)} bytes), skipping")
                    continue
                
                main_type, sub_type = att_type.split('/')
                part = MIMEBase(main_type, sub_type)
                part.set_payload(att_bytes)
                encoders.encode_base64(part)
                part.add_header('Content-Disposition', f'attachment; filename="{att_name}"')
                message.attach(part)
                
                logging.info(f"üìé Attachment added: {att_name} ({len(att_bytes)} bytes)")
            except Exception as e:
                logging.error(f"Error attaching file '{att.get('name')}': {e}")
        
        # 8. Enviar via Gmail API
        try:
            credentials = Credentials(
                token=access_token,
                refresh_token=refresh_token,
                token_uri='https://oauth2.googleapis.com/token',
                client_id=os.getenv('GOOGLE_CLIENT_ID'),
                client_secret=os.getenv('GOOGLE_CLIENT_SECRET')
            )
            
            service = build('gmail', 'v1', credentials=credentials)
            
            raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')
            send_message = {'raw': raw_message}
            
            result = service.users().messages().send(userId='me', body=send_message).execute()
            
            logging.info(f"‚úÖ Email sent successfully to {client_email} (Message ID: {result['id']})")
            return {
                "ok": True,
                "message": f"Email sent to {client_email}",
                "message_id": result['id'],
                "language": language_code
            }
        except Exception as e:
            logging.error(f"‚ùå Error sending email via Gmail API: {e}")
            return {"ok": False, "error": f"Failed to send email: {str(e)}"}
    
    except Exception as e:
        logging.error(f"Error in send_damage_report_email: {e}", exc_info=True)
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/reload-email-templates")
async def reload_damage_report_email_templates(request: Request):
    """
    Reload email templates from files to database
    Updates all language templates (PT, EN, FR, DE) with corrected versions
    """
    require_auth(request)
    
    try:
        import os
        
        # Templates to reload
        templates = [
            {
                'code': 'pt',
                'name': 'Portugu√™s',
                'subject': 'Relat√≥rio de Danos {drNumber} - Auto Prudente',
                'file': 'email_template_pt_complete.html'
            },
            {
                'code': 'en',
                'name': 'English',
                'subject': 'Damage Report {drNumber} - Auto Prudente',
                'file': 'email_template_en_complete.html'
            },
            {
                'code': 'fr',
                'name': 'Fran√ßais',
                'subject': 'Rapport de Dommages {drNumber} - Auto Prudente',
                'file': 'email_template_fr_complete.html'
            },
            {
                'code': 'de',
                'name': 'Deutsch',
                'subject': 'Schadensbericht {drNumber} - Auto Prudente',
                'file': 'email_template_de_complete.html'
            }
        ]
        
        updated = []
        errors = []
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                for template in templates:
                    try:
                        # Read template file
                        if not os.path.exists(template['file']):
                            errors.append(f"{template['name']}: File not found - {template['file']}")
                            continue
                        
                        with open(template['file'], 'r', encoding='utf-8') as f:
                            body = f.read()
                        
                        # Update database
                        if is_postgres:
                            with conn.cursor() as cur:
                                cur.execute("""
                                    UPDATE dr_email_templates
                                    SET body_template = %s,
                                        subject_template = %s,
                                        updated_at = CURRENT_TIMESTAMP
                                    WHERE language_code = %s
                                """, (body, template['subject'], template['code']))
                                
                                if cur.rowcount == 0:
                                    # Insert if not exists
                                    cur.execute("""
                                        INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template, updated_at)
                                        VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                                    """, (template['code'], template['name'], template['subject'], body))
                        else:
                            cursor = conn.execute("""
                                UPDATE dr_email_templates
                                SET body_template = ?,
                                    subject_template = ?,
                                    updated_at = CURRENT_TIMESTAMP
                                WHERE language_code = ?
                            """, (body, template['subject'], template['code']))
                            
                            if cursor.rowcount == 0:
                                conn.execute("""
                                    INSERT INTO dr_email_templates (language_code, language_name, subject_template, body_template, updated_at)
                                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                                """, (template['code'], template['name'], template['subject'], body))
                        
                        updated.append({
                            'code': template['code'],
                            'name': template['name'],
                            'size': len(body)
                        })
                        
                        logging.info(f"‚úÖ Template {template['name']} ({template['code']}) reloaded: {len(body)} chars")
                        
                    except Exception as e:
                        errors.append(f"{template['name']}: {str(e)}")
                        logging.error(f"Error reloading template {template['code']}: {e}")
                
                conn.commit()
                
            finally:
                conn.close()
        
        if errors:
            return {
                "ok": len(updated) > 0,
                "updated": updated,
                "errors": errors,
                "message": f"Reloaded {len(updated)} templates with {len(errors)} errors"
            }
        else:
            return {
                "ok": True,
                "updated": updated,
                "message": f"Successfully reloaded {len(updated)} email templates"
            }
        
    except Exception as e:
        logging.error(f"Error reloading email templates: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/reload-email-templates")
async def reload_email_templates_page(request: Request):
    """
    Serve a p√°gina HTML para recarregar os templates de email
    """
    require_auth(request)
    
    html_content = """
<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reload DR Email Templates</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; background: #f5f5f5; }
        .container { background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #009cb6; margin-top: 0; }
        button { background: #009cb6; color: white; border: none; padding: 15px 30px; font-size: 16px; border-radius: 4px; cursor: pointer; margin-top: 20px; }
        button:hover { background: #007a8f; }
        button:disabled { background: #ccc; cursor: not-allowed; }
        .result { margin-top: 20px; padding: 15px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; }
        .success { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
        .error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
        .loading { background: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
        .instructions { background: #e7f3ff; padding: 15px; border-left: 4px solid #009cb6; margin: 20px 0; }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîÑ Reload Damage Report Email Templates</h1>
        
        <div class="instructions">
            <strong>üìã Instru√ß√µes:</strong>
            <ol>
                <li>Este endpoint atualiza os templates de email na BD</li>
                <li>L√™ os ficheiros HTML corrigidos (PT, EN, FR, DE)</li>
                <li>Atualiza automaticamente a base de dados PostgreSQL</li>
                <li>Clica no bot√£o abaixo para executar</li>
            </ol>
        </div>
        
        <p><strong>Endpoint:</strong> <code>POST /api/damage-reports/reload-email-templates</code></p>
        
        <button id="reloadBtn" onclick="reloadTemplates()">üîÑ Reload Templates Now</button>
        
        <div id="result"></div>
    </div>

    <script>
        async function reloadTemplates() {
            const btn = document.getElementById('reloadBtn');
            const resultDiv = document.getElementById('result');
            
            btn.disabled = true;
            btn.textContent = '‚è≥ Loading...';
            
            resultDiv.className = 'result loading';
            resultDiv.textContent = 'Connecting to API...';
            
            try {
                const response = await fetch('/api/damage-reports/reload-email-templates', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    credentials: 'include'
                });
                
                const data = await response.json();
                
                if (data.ok) {
                    resultDiv.className = 'result success';
                    let html = '‚úÖ SUCCESS!' + String.fromCharCode(10) + String.fromCharCode(10) + data.message + String.fromCharCode(10) + String.fromCharCode(10);
                    html += 'üìß Updated Templates:' + String.fromCharCode(10);
                    data.updated.forEach(t => {
                        html += '   ‚Ä¢ ' + t.name + ' (' + t.code + '): ' + t.size.toLocaleString() + ' characters' + String.fromCharCode(10);
                    });
                    if (data.errors && data.errors.length > 0) {
                        html += String.fromCharCode(10) + '‚ö†Ô∏è Errors:' + String.fromCharCode(10);
                        data.errors.forEach(e => { html += '   ‚Ä¢ ' + e + String.fromCharCode(10); });
                    }
                    resultDiv.textContent = html;
                } else {
                    resultDiv.className = 'result error';
                    resultDiv.textContent = '‚ùå ERROR:' + String.fromCharCode(10) + String.fromCharCode(10) + (data.error || JSON.stringify(data, null, 2));
                }
                
            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.textContent = '‚ùå NETWORK ERROR:' + String.fromCharCode(10) + String.fromCharCode(10) + error.message;
            } finally {
                btn.disabled = false;
                btn.textContent = 'üîÑ Reload Templates Now';
            }
        }
    </script>
</body>
</html>
    """
    
    return HTMLResponse(content=html_content)

@app.post("/api/damage-reports/upload-pdfs-bulk")
async def upload_damage_reports_pdfs_bulk(request: Request):
    """Upload m√∫ltiplos PDFs de uma vez, detectando DR number do nome do ficheiro"""
    require_auth(request)
    
    try:
        import re
        from datetime import datetime
        
        form = await request.form()
        files = []
        
        # Coletar todos os ficheiros
        for key in form.keys():
            if key.startswith('files'):
                file = form.get(key)
                if file and hasattr(file, 'filename'):
                    files.append(file)
        
        if not files:
            return {"ok": False, "error": "Nenhum ficheiro fornecido"}
        
        results = {
            "uploaded": 0,
            "errors": [],
            "details": []
        }
        
        with _db_lock:
            conn = _db_connect()
            try:
                for file in files:
                    try:
                        filename = file.filename
                        pdf_data = await file.read()
                        
                        # Detectar n√∫mero do DR do nome do ficheiro
                        # Padr√µes: DR01/2025, DR_01_2025, DR-01-2025, DR01:2025, DR01/24, etc.
                        patterns = [
                            r'DR[\s_-]*(\d+)[\s_:/-]*(\d{2,4})',  # DR01/2025, DR01/24, DR_01_2025, DR-01-2025, DR01:2025
                            r'(\d+)[\s_:/-]+(\d{2,4})',            # 01/2025, 01/24, 01_2025, 01:2025
                            r'DR[\s_-]*(\d+)',                     # DR01, DR_01
                        ]
                        
                        dr_number = None
                        for pattern in patterns:
                            match = re.search(pattern, filename, re.IGNORECASE)
                            if match:
                                if len(match.groups()) == 2:
                                    num, year = match.groups()
                                    # Converter ano curto (24) para ano completo (2024)
                                    if len(year) == 2:
                                        year_int = int(year)
                                        # Anos 00-50 = 2000-2050, anos 51-99 = 1951-1999
                                        if year_int <= 50:
                                            year = f"20{year}"
                                        else:
                                            year = f"19{year}"
                                    dr_number = f"DR {int(num):02d}/{year}"
                                else:
                                    num = match.group(1)
                                    current_year = datetime.now().year
                                    dr_number = f"DR {int(num):02d}/{current_year}"
                                break
                        
                        if not dr_number:
                            results["errors"].append(f"{filename}: N√£o foi poss√≠vel detectar n√∫mero do DR")
                            continue
                        
                        # Verificar se DR existe, se n√£o criar automaticamente
                        cursor = conn.execute("SELECT id FROM damage_reports WHERE dr_number = ?", (dr_number,))
                        existing = cursor.fetchone()
                        
                        if not existing:
                            # Criar DR automaticamente a partir do PDF - PROTEGIDO (n√£o pode ser eliminado)
                            logging.info(f"üîí Criando DR {dr_number} PROTEGIDO automaticamente a partir do PDF")
                            conn.execute("""
                                INSERT INTO damage_reports (
                                    dr_number, date, client_name, vehicle_plate,
                                    status, pdf_data, pdf_filename, is_protected,
                                    created_at, created_by, updated_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                dr_number,
                                datetime.now().strftime('%Y-%m-%d'),  # date
                                'A preencher',  # client_name
                                'A preencher',  # vehicle_plate
                                'draft',  # status
                                pdf_data,  # pdf_data
                                filename,  # pdf_filename
                                1,  # is_protected - NUNCA pode ser eliminado
                                datetime.now().isoformat(),  # created_at
                                request.session.get('username', 'admin'),  # created_by
                                datetime.now().isoformat()  # updated_at
                            ))
                        else:
                            # Atualizar DR existente com PDF e marcar como protegido
                            logging.info(f"üîí Atualizando DR {dr_number} e marcando como PROTEGIDO")
                            conn.execute("""
                                UPDATE damage_reports 
                                SET pdf_data = ?, pdf_filename = ?, is_protected = 1, updated_at = ?
                                WHERE dr_number = ?
                            """, (pdf_data, filename, datetime.now().isoformat(), dr_number))
                        
                        results["uploaded"] += 1
                        results["details"].append({
                            "filename": filename,
                            "dr_number": dr_number,
                            "size": len(pdf_data)
                        })
                        
                    except Exception as e:
                        results["errors"].append(f"{filename}: {str(e)}")
                
                conn.commit()
                
                # Verificar que foram guardados
                for detail in results['details']:
                    cursor = conn.execute(
                        "SELECT pdf_filename FROM damage_reports WHERE dr_number = ?", 
                        (detail['dr_number'],)
                    )
                    saved = cursor.fetchone()
                    if saved and saved[0]:
                        logging.info(f"‚úÖ CONFIRMADO: PDF {detail['filename']} guardado no PostgreSQL para {detail['dr_number']}")
                    else:
                        logging.error(f"‚ùå ERRO: PDF {detail['filename']} N√ÉO foi guardado para {detail['dr_number']}")
                
                logging.info(f"‚úÖ {results['uploaded']} PDFs anexados em massa e guardados no PostgreSQL")
                
                return {
                    "ok": True,
                    "uploaded": results["uploaded"],
                    "total": len(files),
                    "errors": results["errors"],
                    "details": results["details"]
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error bulk uploading PDFs: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/upload-pdf/{dr_number:path}")
async def upload_damage_report_pdf(request: Request, dr_number: str):
    """Upload PDF para um Damage Report existente"""
    require_auth(request)
    
    try:
        form = await request.form()
        file = form.get('file')
        
        if not file:
            return {"ok": False, "error": "Nenhum ficheiro fornecido"}
        
        # Ler conte√∫do do PDF
        pdf_data = await file.read()
        filename = file.filename
        
        # Atualizar na BD
        with _db_lock:
            conn = _db_connect()
            try:
                # Verificar se DR existe
                cursor = conn.execute("SELECT id FROM damage_reports WHERE dr_number = ?", (dr_number,))
                if not cursor.fetchone():
                    return {"ok": False, "error": f"DR {dr_number} n√£o encontrado"}
                
                # Atualizar com PDF
                conn.execute("""
                    UPDATE damage_reports 
                    SET pdf_data = ?, pdf_filename = ?, updated_at = ?
                    WHERE dr_number = ?
                """, (pdf_data, filename, datetime.now().isoformat(), dr_number))
                
                conn.commit()
                
                logging.info(f"‚úÖ PDF anexado ao DR {dr_number}: {filename} ({len(pdf_data)} bytes)")
                
                return {
                    "ok": True,
                    "message": f"PDF anexado ao {dr_number}",
                    "filename": filename,
                    "size": len(pdf_data)
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error uploading PDF: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/import-bulk")
async def import_damage_reports_bulk(request: Request):
    """Importar m√∫ltiplos Damage Reports de uma vez (migra√ß√£o)"""
    require_auth(request)
    
    try:
        from datetime import datetime
        import json
        
        data = await request.json()
        reports = data.get('reports', [])
        
        if not reports:
            return {"ok": False, "error": "Nenhum relat√≥rio fornecido"}
        
        imported = 0
        errors = []
        
        with _db_lock:
            conn = _db_connect()
            try:
                for report in reports:
                    try:
                        # Extrair dados do relat√≥rio
                        dr_number = report.get('dr_number')
                        
                        # Verificar se j√° existe
                        cursor = conn.execute("SELECT COUNT(*) FROM damage_reports WHERE dr_number = ?", (dr_number,))
                        if cursor.fetchone()[0] > 0:
                            errors.append(f"{dr_number} j√° existe")
                            continue
                        
                        # Inserir na BD
                        conn.execute("""
                            INSERT INTO damage_reports (
                                dr_number, ra_number, contract_number, date,
                                client_name, client_email, client_phone,
                                client_address, client_city, client_postal_code, client_country,
                                vehicle_plate, vehicle_brand, vehicle_model, vehicle_color, vehicle_km,
                                pickup_date, return_date, pickup_location, return_location,
                                fuel_level_pickup, fuel_level_return,
                                damage_description, total_repair_cost,
                                inspector_name, inspection_date,
                                created_at, created_by
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            dr_number,
                            report.get('ra_number'),
                            report.get('contract_number'),
                            report.get('date'),
                            report.get('client_name'),
                            report.get('client_email'),
                            report.get('client_phone'),
                            report.get('client_address'),
                            report.get('client_city'),
                            report.get('client_postal_code'),
                            report.get('client_country'),
                            report.get('vehicle_plate'),
                            report.get('vehicle_brand'),
                            report.get('vehicle_model'),
                            report.get('vehicle_color'),
                            report.get('vehicle_km'),
                            report.get('pickup_date'),
                            report.get('return_date'),
                            report.get('pickup_location'),
                            report.get('return_location'),
                            report.get('fuel_level_pickup'),
                            report.get('fuel_level_return'),
                            report.get('damage_description'),
                            report.get('total_repair_cost'),
                            report.get('inspector_name'),
                            report.get('inspection_date'),
                            datetime.now().isoformat(),
                            request.session.get('username', 'admin')
                        ))
                        
                        imported += 1
                    except Exception as e:
                        errors.append(f"{dr_number}: {str(e)}")
                
                conn.commit()
                
                # Verificar que foram guardados no PostgreSQL
                cursor = conn.execute("SELECT COUNT(*) FROM damage_reports")
                total_in_db = cursor.fetchone()[0]
                
                logging.info(f"‚úÖ {imported} Damage Reports importados e guardados no PostgreSQL")
                logging.info(f"üìä Total de DRs na base de dados PostgreSQL: {total_in_db}")
                
                return {
                    "ok": True,
                    "imported": imported,
                    "total": len(reports),
                    "errors": errors,
                    "total_in_database": total_in_db
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error importing damage reports: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/protect-uploaded")
async def protect_uploaded_drs(request: Request):
    """TEMPOR√ÅRIO: Marcar todos os DRs com PDF como protegidos"""
    require_auth(request)
    
    try:
        protected_count = 0
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Marcar todos os DRs que t√™m PDF como protegidos
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("""
                            UPDATE damage_reports 
                            SET is_protected = 1 
                            WHERE pdf_data IS NOT NULL AND (is_protected IS NULL OR is_protected = 0)
                        """)
                        protected_count = cur.rowcount
                    conn.commit()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        UPDATE damage_reports 
                        SET is_protected = 1 
                        WHERE pdf_data IS NOT NULL AND (is_protected IS NULL OR is_protected = 0)
                    """)
                    protected_count = cursor.rowcount
                    conn.commit()
                
                logging.info(f"‚úÖ {protected_count} DRs marcados como protegidos")
                
                return {
                    "ok": True,
                    "protected_count": protected_count,
                    "message": f"{protected_count} DRs marcados como protegidos"
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error protecting DRs: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/setup-dr-tables")
async def setup_dr_tables():
    """Criar tabelas de Damage Reports - SEM AUTH"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if not is_postgres:
                    return {"ok": False, "error": "Apenas para PostgreSQL"}
                
                with conn.cursor() as cur:
                    # 1. Tabela principal
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_reports (
                            id SERIAL PRIMARY KEY,
                            dr_number TEXT UNIQUE,
                            ra_number TEXT,
                            contract_number TEXT,
                            date DATE,
                            client_name TEXT,
                            client_email TEXT,
                            client_phone TEXT,
                            client_address TEXT,
                            client_city TEXT,
                            client_postal_code TEXT,
                            client_country TEXT,
                            vehicle_plate TEXT,
                            vehicle_model TEXT,
                            vehicle_brand TEXT,
                            pickup_date TIMESTAMP,
                            pickup_time TEXT,
                            pickup_location TEXT,
                            return_date TIMESTAMP,
                            return_time TEXT,
                            return_location TEXT,
                            issued_by TEXT,
                            inspection_type TEXT,
                            inspector_name TEXT,
                            mileage INTEGER,
                            fuel_level TEXT,
                            damage_description TEXT,
                            observations TEXT,
                            damage_diagram_data TEXT,
                            repair_items TEXT,
                            damage_images TEXT,
                            total_amount REAL,
                            status TEXT DEFAULT 'draft',
                            pdf_data BYTEA,
                            pdf_filename TEXT,
                            is_protected INTEGER DEFAULT 0,
                            is_deleted INTEGER DEFAULT 0,
                            deleted_at TIMESTAMP,
                            deleted_by TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            created_by TEXT,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # 2. Coordenadas
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_coordinates (
                            id SERIAL PRIMARY KEY,
                            field_id TEXT NOT NULL,
                            x REAL NOT NULL,
                            y REAL NOT NULL,
                            width REAL NOT NULL,
                            height REAL NOT NULL,
                            page INTEGER DEFAULT 1,
                            field_type TEXT,
                            template_version INTEGER DEFAULT 1,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # 2b. Hist√≥rico de Mapeamento
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_mapping_history (
                            id SERIAL PRIMARY KEY,
                            template_version INTEGER NOT NULL,
                            field_id TEXT NOT NULL,
                            x REAL NOT NULL,
                            y REAL NOT NULL,
                            width REAL NOT NULL,
                            height REAL NOT NULL,
                            page INTEGER DEFAULT 1,
                            field_type TEXT,
                            mapped_by TEXT,
                            mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # 4. Numera√ß√£o
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS damage_report_numbering (
                            id INTEGER PRIMARY KEY,
                            current_year INTEGER NOT NULL,
                            current_number INTEGER NOT NULL,
                            prefix TEXT DEFAULT 'DR',
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # Inserir config inicial APENAS se n√£o existir
                    cur.execute("""
                        INSERT INTO damage_report_numbering (id, current_year, current_number, prefix)
                        SELECT 1, 2025, 1, 'DR'
                        WHERE NOT EXISTS (SELECT 1 FROM damage_report_numbering WHERE id = 1)
                    """)
                    
                    # √çndices
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_dr_number ON damage_reports(dr_number)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_dr_created_at ON damage_reports(created_at DESC)")
                    
                conn.commit()
                return {"ok": True, "message": "Tabelas criadas com sucesso!"}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error creating tables: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/setup-car-groups-table")
async def setup_car_groups_table():
    """Criar tabela car_groups no PostgreSQL - SEM AUTH"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    # PostgreSQL
                    with conn.cursor() as cur:
                        cur.execute("""
                            CREATE TABLE IF NOT EXISTS car_groups (
                                id SERIAL PRIMARY KEY,
                                code TEXT UNIQUE NOT NULL,
                                name TEXT,
                                model TEXT,
                                brand TEXT,
                                category TEXT,
                                doors INTEGER,
                                seats INTEGER,
                                transmission TEXT,
                                luggage INTEGER,
                                photo_url TEXT,
                                enabled BOOLEAN DEFAULT TRUE,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            )
                        """)
                        
                        # Criar √≠ndices
                        cur.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_name ON car_groups(LOWER(name))")
                        cur.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_model ON car_groups(LOWER(model))")
                        cur.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_code ON car_groups(code)")
                    conn.commit()
                else:
                    # SQLite
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS car_groups (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            code TEXT UNIQUE NOT NULL,
                            name TEXT,
                            model TEXT,
                            brand TEXT,
                            category TEXT,
                            doors INTEGER,
                            seats INTEGER,
                            transmission TEXT,
                            luggage INTEGER,
                            photo_url TEXT,
                            enabled INTEGER DEFAULT 1,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    conn.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_name ON car_groups(LOWER(name))")
                    conn.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_model ON car_groups(LOWER(model))")
                    conn.execute("CREATE INDEX IF NOT EXISTS idx_car_groups_code ON car_groups(code)")
                    conn.commit()
                
                return {"ok": True, "message": "Tabela car_groups criada com sucesso!"}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error creating car_groups table: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/cleanup-drs")
async def cleanup_invalid_drs():
    """Eliminar DRs com formato antigo (:) - SEM AUTH"""
    # require_auth(request)
    
    try:
        invalid_drs = ["1:2025", "2:2025", "3:2025"]
        deleted = []
        errors = []
        
        with _db_lock:
            conn = _db_connect()
            try:
                for dr_number in invalid_drs:
                    try:
                        if conn.__class__.__module__ == 'psycopg2.extensions':
                            # PostgreSQL - rollback antes de cada tentativa
                            conn.rollback()
                            with conn.cursor() as cur:
                                cur.execute("DELETE FROM damage_reports WHERE dr_number = %s", (dr_number,))
                                conn.commit()
                                if cur.rowcount > 0:
                                    deleted.append(dr_number)
                                    logging.info(f"‚úÖ DR {dr_number} eliminado")
                                else:
                                    logging.info(f"‚ÑπÔ∏è  DR {dr_number} n√£o existe")
                        else:
                            # SQLite
                            cursor = conn.execute("DELETE FROM damage_reports WHERE dr_number = ?", (dr_number,))
                            conn.commit()
                            if cursor.rowcount > 0:
                                deleted.append(dr_number)
                                logging.info(f"‚úÖ DR {dr_number} eliminado")
                    except Exception as e:
                        errors.append(f"{dr_number}: {str(e)}")
                        logging.error(f"‚ùå Erro ao eliminar {dr_number}: {e}")
                        conn.rollback()
                
                return {
                    "ok": True,
                    "deleted": deleted,
                    "errors": errors,
                    "message": f"{len(deleted)} DRs eliminados com sucesso"
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error cleaning up DRs: {e}")
        return {"ok": False, "error": str(e)}

@app.delete("/api/damage-reports/{dr_number:path}")
async def delete_damage_report(request: Request, dr_number: str):
    """Eliminar DR - APENAS se n√£o for protegido"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Verificar se DR existe e se √© protegido
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        cur.execute("SELECT is_protected FROM damage_reports WHERE dr_number = %s", (dr_number,))
                        row = cur.fetchone()
                else:
                    cursor = conn.execute("SELECT is_protected FROM damage_reports WHERE dr_number = ?", (dr_number,))
                    row = cursor.fetchone()
                
                if not row:
                    raise HTTPException(status_code=404, detail="DR not found")
                
                is_protected = row[0]
                if is_protected:
                    raise HTTPException(status_code=403, detail="Este DR est√° protegido e n√£o pode ser eliminado")
                
                # Eliminar DR (soft delete)
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        # Soft delete - marcar como eliminado ao inv√©s de deletar
                        cur.execute("""
                            UPDATE damage_reports 
                            SET is_deleted = 1, deleted_at = %s, deleted_by = %s, updated_at = %s
                            WHERE dr_number = %s
                        """, (datetime.now().isoformat(), request.session.get('user_email', 'unknown'), datetime.now().isoformat(), dr_number))
                        conn.commit()
                else:
                    # Soft delete - SQLite
                    conn.execute("""
                        UPDATE damage_reports 
                        SET is_deleted = 1, deleted_at = ?, deleted_by = ?, updated_at = ?
                        WHERE dr_number = ?
                    """, (datetime.now().isoformat(), request.session.get('user_email', 'unknown'), datetime.now().isoformat(), dr_number))
                    conn.commit()
                
                logging.info(f"‚úÖ DR {dr_number} marcado como eliminado (soft delete) - n√∫mero dispon√≠vel para reutiliza√ß√£o")
                return {"ok": True, "message": f"DR {dr_number} eliminado"}
                
            finally:
                conn.close()
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error deleting DR: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/damage-reports/debug")
async def debug_damage_reports(request: Request):
    """DEBUG: Ver todos os DRs na BD"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT 
                                dr_number, 
                                pdf_filename, 
                                CASE WHEN pdf_data IS NULL THEN false ELSE true END as has_pdf,
                                is_protected,
                                created_at
                            FROM damage_reports
                            ORDER BY dr_number
                        """)
                        rows = cur.fetchall()
                else:
                    cursor = conn.execute("""
                        SELECT 
                            dr_number, 
                            pdf_filename, 
                            CASE WHEN pdf_data IS NULL THEN 0 ELSE 1 END as has_pdf,
                            is_protected,
                            created_at
                        FROM damage_reports
                        ORDER BY dr_number
                    """)
                    rows = cursor.fetchall()
                
                drs = []
                for row in rows:
                    drs.append({
                        'dr_number': row[0],
                        'pdf_filename': row[1],
                        'has_pdf': bool(row[2]),
                        'is_protected': bool(row[3]) if row[3] is not None else False,
                        'created_at': row[4]
                    })
                
                return {
                    "ok": True,
                    "total": len(drs),
                    "drs": drs
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error in debug endpoint: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/damage-reports/stats")
async def get_damage_reports_stats(request: Request):
    """Obter estat√≠sticas dos Damage Reports"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Total de DRs
                cursor = conn.execute("SELECT COUNT(*) FROM damage_reports")
                total = cursor.fetchone()[0]
                
                # DRs protegidos
                cursor = conn.execute("SELECT COUNT(*) FROM damage_reports WHERE is_protected = 1")
                protected = cursor.fetchone()[0]
                
                # DRs por ano
                cursor = conn.execute("""
                    SELECT 
                        CASE 
                            WHEN dr_number LIKE '%/%' THEN SUBSTR(dr_number, INSTR(dr_number, '/') + 1)
                            ELSE 'Outros'
                        END as year,
                        COUNT(*) as count
                    FROM damage_reports
                    GROUP BY year
                    ORDER BY year DESC
                """)
                by_year = {row[0]: row[1] for row in cursor.fetchall()}
                
                return {
                    "ok": True,
                    "total": total,
                    "protected": protected,
                    "by_year": by_year
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting stats: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/fix-columns")
async def fix_damage_reports_columns(request: Request):
    """Adicionar colunas pdf_data e pdf_filename se n√£o existirem - TEMPOR√ÅRIO"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = conn.__class__.__module__ in ['psycopg2.extensions', 'psycopg2._psycopg']
                
                pdf_data_added = False
                pdf_filename_added = False
                is_protected_added = False
                
                if is_postgres:
                    # PostgreSQL: Verificar colunas ANTES de adicionar
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = 'damage_reports'
                        """)
                        existing_columns = [row[0] for row in cur.fetchall()]
                        
                        if 'pdf_data' not in existing_columns:
                            cur.execute("ALTER TABLE damage_reports ADD COLUMN pdf_data BYTEA")
                            logging.info("‚úÖ Coluna pdf_data adicionada (PostgreSQL)")
                            pdf_data_added = True
                        else:
                            logging.info("‚ÑπÔ∏è Coluna pdf_data j√° existe (PostgreSQL)")
                        
                        if 'pdf_filename' not in existing_columns:
                            cur.execute("ALTER TABLE damage_reports ADD COLUMN pdf_filename TEXT")
                            logging.info("‚úÖ Coluna pdf_filename adicionada (PostgreSQL)")
                            pdf_filename_added = True
                        else:
                            logging.info("‚ÑπÔ∏è Coluna pdf_filename j√° existe (PostgreSQL)")
                        
                        if 'is_protected' not in existing_columns:
                            cur.execute("ALTER TABLE damage_reports ADD COLUMN is_protected INTEGER DEFAULT 0")
                            logging.info("‚úÖ Coluna is_protected adicionada (PostgreSQL)")
                            is_protected_added = True
                        else:
                            logging.info("‚ÑπÔ∏è Coluna is_protected j√° existe (PostgreSQL)")
                        
                        conn.commit()
                else:
                    # SQLite: Usar try/except
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN pdf_data BLOB")
                        conn.commit()
                        logging.info("‚úÖ Coluna pdf_data adicionada (SQLite)")
                        pdf_data_added = True
                    except Exception as e:
                        pdf_data_added = False
                        logging.info(f"‚ÑπÔ∏è Coluna pdf_data j√° existe: {e}")
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN pdf_filename TEXT")
                        conn.commit()
                        logging.info("‚úÖ Coluna pdf_filename adicionada (SQLite)")
                        pdf_filename_added = True
                    except Exception as e:
                        pdf_filename_added = False
                        logging.info(f"‚ÑπÔ∏è Coluna pdf_filename j√° existe: {e}")
                    
                    try:
                        conn.execute("ALTER TABLE damage_reports ADD COLUMN is_protected INTEGER DEFAULT 0")
                        conn.commit()
                        logging.info("‚úÖ Coluna is_protected adicionada (SQLite)")
                        is_protected_added = True
                    except Exception as e:
                        is_protected_added = False
                        logging.info(f"‚ÑπÔ∏è Coluna is_protected j√° existe: {e}")
                
                return {
                    "ok": True,
                    "pdf_data_added": pdf_data_added,
                    "pdf_filename_added": pdf_filename_added,
                    "is_protected_added": is_protected_added,
                    "message": "Colunas verificadas e adicionadas se necess√°rio"
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error fixing columns: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/numbering/update")
async def update_dr_numbering(request: Request):
    """Atualizar configura√ß√£o de numera√ß√£o manualmente"""
    require_auth(request)
    
    try:
        from datetime import datetime
        import psycopg2
        
        data = await request.json()
        current_number = int(data.get('current_number', 1))
        prefix = data.get('prefix', 'DR')
        username = request.session.get('username', 'admin')
        
        logging.info(f"üîÑ [DR-NUMBERING] UPDATE Request: current_number={current_number}, prefix={prefix}, user={username}")
        
        with _db_lock:
            conn = _db_connect()
            try:
                current_year = datetime.now().year
                
                # Detectar tipo de BD
                is_postgres = isinstance(conn, psycopg2.extensions.connection)
                db_type = "PostgreSQL" if is_postgres else "SQLite"
                logging.info(f"üìä [DR-NUMBERING] Database: {db_type}")
                
                # Atualizar numera√ß√£o
                if is_postgres:
                    # PostgreSQL
                    with conn.cursor() as cur:
                        # Primeiro verificar se o registro existe
                        cur.execute("SELECT current_number FROM damage_report_numbering WHERE id = 1")
                        existing = cur.fetchone()
                        logging.info(f"üìã [DR-NUMBERING] Current value in DB: {existing[0] if existing else 'NOT FOUND'}")
                        
                        cur.execute("""
                            UPDATE damage_report_numbering
                            SET current_number = %s, prefix = %s, current_year = %s, updated_at = %s
                            WHERE id = 1
                        """, (current_number, prefix, current_year, datetime.now().isoformat()))
                        rows_affected = cur.rowcount
                        logging.info(f"‚úÖ [DR-NUMBERING] PostgreSQL UPDATE: {rows_affected} row(s) affected")
                        
                        # Verificar se foi atualizado
                        cur.execute("SELECT current_number, prefix FROM damage_report_numbering WHERE id = 1")
                        updated = cur.fetchone()
                        logging.info(f"‚úÖ [DR-NUMBERING] After UPDATE: current_number={updated[0]}, prefix={updated[1]}")
                else:
                    # SQLite
                    cursor = conn.execute("SELECT current_number FROM damage_report_numbering WHERE id = 1")
                    existing = cursor.fetchone()
                    logging.info(f"üìã [DR-NUMBERING] Current value in DB: {existing[0] if existing else 'NOT FOUND'}")
                    
                    cursor = conn.execute("""
                        UPDATE damage_report_numbering
                        SET current_number = ?, prefix = ?, current_year = ?, updated_at = ?
                        WHERE id = 1
                    """, (current_number, prefix, current_year, datetime.now().isoformat()))
                    rows_affected = cursor.rowcount
                    logging.info(f"‚úÖ [DR-NUMBERING] SQLite UPDATE: {rows_affected} row(s) affected")
                    
                    # Verificar se foi atualizado
                    cursor = conn.execute("SELECT current_number, prefix FROM damage_report_numbering WHERE id = 1")
                    updated = cursor.fetchone()
                    logging.info(f"‚úÖ [DR-NUMBERING] After UPDATE: current_number={updated[0]}, prefix={updated[1]}")
                
                conn.commit()
                logging.info(f"üíæ [DR-NUMBERING] COMMIT executed successfully")
                
                # VERIFICA√á√ÉO CR√çTICA: Ler de novo para confirmar persist√™ncia
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("SELECT current_number, prefix FROM damage_report_numbering WHERE id = 1")
                        verification = cur.fetchone()
                else:
                    cursor = conn.execute("SELECT current_number, prefix FROM damage_report_numbering WHERE id = 1")
                    verification = cursor.fetchone()
                
                if verification and verification[0] == current_number:
                    logging.info(f"‚úÖ [DR-NUMBERING] VERIFIED: Value persisted correctly: {verification[0]}")
                else:
                    logging.error(f"‚ùå [DR-NUMBERING] PERSISTENCE FAILED! Expected {current_number}, found {verification[0] if verification else 'NULL'}")
                    return {
                        "ok": False,
                        "error": f"Persist√™ncia falhou! Esperado {current_number}, encontrado {verification[0] if verification else 'NULL'}"
                    }
                
                next_number = f"{prefix}{(current_number + 1):02d}/{current_year}"
                logging.info(f"‚úÖ [DR-NUMBERING] Numera√ß√£o atualizada: current={current_number}, next={next_number}")
                
                return {
                    "ok": True,
                    "message": "Numera√ß√£o atualizada com sucesso",
                    "next_number": next_number,
                    "current_number": current_number,
                    "prefix": prefix,
                    "database": db_type
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå [DR-NUMBERING] Error updating: {e}", exc_info=True)
        return {"ok": False, "error": str(e)}

@app.post("/api/damage-reports/save-coordinates")
async def save_damage_report_coordinates(request: Request):
    """Guardar coordenadas dos campos do PDF"""
    require_auth(request)
    
    try:
        import json
        from datetime import datetime
        
        # Receber coordenadas (suporta objeto OU array)
        data = await request.json()
        
        # Detectar se data √© array direto ou objeto com 'coordinates'
        if isinstance(data, list):
            # Formato: [ {field_id, x, y, ...}, ... ]
            coordinates = data
        elif isinstance(data, dict):
            # Formato: { coordinates: [...] } ou { "field_id": {...}, ... }
            raw_coordinates = data.get('coordinates', data)
            
            if isinstance(raw_coordinates, list):
                # J√° √© array
                coordinates = raw_coordinates
            elif isinstance(raw_coordinates, dict):
                # Formato antigo: { "field_id": {x, y, ...}, ... }
                # Converter para array: [ {field_id, x, y, page, ...}, ... ]
                coordinates = []
                for field_id, coord in raw_coordinates.items():
                    coord_copy = coord.copy() if isinstance(coord, dict) else {}
                    coord_copy['field_id'] = field_id
                    coordinates.append(coord_copy)
            else:
                coordinates = []
        else:
            coordinates = []
        
        # Obter usu√°rio atual
        username = request.session.get('username', 'system')
        
        # Obter vers√£o ativa do template
        template_version = 1
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT version FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                    cursor.close()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT version FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                
                if row:
                    template_version = row[0]
            finally:
                conn.close()
        
        # Guardar em PostgreSQL/SQLite
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    # PostgreSQL - usar um √∫nico cursor para todas as opera√ß√µes
                    with conn.cursor() as cursor:
                        # Limpar coordenadas antigas
                        cursor.execute("DELETE FROM damage_report_coordinates")
                        
                        logging.info(f"üíæ SAVE Coordinates: Salvando {len(coordinates)} campos")
                        
                        # Inserir novas coordenadas (agora array)
                        for coord in coordinates:
                            field_id_raw = coord.get('field_id')
                            if not field_id_raw:
                                continue
                            
                            # Extrair field_id real (remover @pageX se presente)
                            import re
                            match = re.match(r'^(.+)@page\d+$', field_id_raw)
                            field_id = match.group(1) if match else field_id_raw
                            
                            field_type = _detect_field_type(field_id)
                            
                            # Inserir em coordenadas atuais
                            cursor.execute("""
                                INSERT INTO damage_report_coordinates 
                                (field_id, x, y, width, height, page, field_type, template_version, updated_at)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                field_id,
                                coord.get('x'),
                                coord.get('y'),
                                coord.get('width'),
                                coord.get('height'),
                                coord.get('page', 1),
                                field_type,
                                template_version,
                                datetime.now().isoformat()
                            ))
                            
                            # Inserir em hist√≥rico
                            cursor.execute("""
                                INSERT INTO damage_report_mapping_history 
                                (template_version, field_id, x, y, width, height, page, field_type, mapped_by, mapped_at)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                template_version,
                                field_id,
                                coord.get('x'),
                                coord.get('y'),
                                coord.get('width'),
                                coord.get('height'),
                                coord.get('page', 1),
                                field_type,
                                username,
                                datetime.now().isoformat()
                            ))
                else:
                    # SQLite
                    conn.execute("DELETE FROM damage_report_coordinates")
                    
                    logging.info(f"üíæ SAVE Coordinates: Salvando {len(coordinates)} campos")
                    
                    for coord in coordinates:
                        field_id_raw = coord.get('field_id')
                        if not field_id_raw:
                            continue
                        
                        # Extrair field_id real (remover @pageX se presente)
                        import re
                        match = re.match(r'^(.+)@page\d+$', field_id_raw)
                        field_id = match.group(1) if match else field_id_raw
                        
                        field_type = _detect_field_type(field_id)
                        
                        conn.execute("""
                            INSERT INTO damage_report_coordinates 
                            (field_id, x, y, width, height, page, field_type, template_version, updated_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            field_id,
                            coord.get('x'),
                            coord.get('y'),
                            coord.get('width'),
                            coord.get('height'),
                            coord.get('page', 1),
                            field_type,
                            template_version,
                            datetime.now().isoformat()
                        ))
                        
                        conn.execute("""
                            INSERT INTO damage_report_mapping_history 
                            (template_version, field_id, x, y, width, height, page, field_type, mapped_by, mapped_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            template_version,
                            field_id,
                            coord.get('x'),
                            coord.get('y'),
                            coord.get('width'),
                            coord.get('height'),
                            coord.get('page', 1),
                            field_type,
                            username,
                            datetime.now().isoformat()
                        ))
                
                conn.commit()
                logging.info(f"‚úÖ SAVE: {len(coordinates)} coordenadas guardadas na BD (vers√£o {template_version})")
            finally:
                conn.close()
        
        # Guardar tamb√©m em ficheiro JSON (backup) - converter array para objeto para compatibilidade
        backup_coords = {}
        for coord in coordinates:
            field_id = coord.get('field_id')
            if field_id:
                # Se j√° existe field_id, criar chave composta
                page = coord.get('page', 1)
                key = f"{field_id}@page{page}" if field_id in backup_coords else field_id
                backup_coords[key] = coord
        
        with open('damage_report_coordinates.json', 'w') as f:
            json.dump(backup_coords, f, indent=2)
        
        return {"ok": True, "count": len(coordinates), "version": template_version}
    except Exception as e:
        logging.error(f"Error saving coordinates: {e}")
        return {"ok": False, "error": str(e)}

def _validate_image_data(image_data: str, field_id: str = "unknown") -> bool:
    """Valida se dados de imagem s√£o v√°lidos"""
    is_diagram = 'diagram' in field_id.lower() or 'croqui' in field_id.lower()
    log_fn = logging.error if is_diagram else logging.info
    
    try:
        if not image_data:
            logging.error(f"‚ùå [{field_id}] Image validation: empty data")
            return False
        
        if not isinstance(image_data, str):
            logging.error(f"‚ùå [{field_id}] Image validation: not a string, type={type(image_data)}")
            return False
        
        # Remove prefix if present
        has_prefix = ',' in image_data
        img_data = image_data.split(',')[1] if has_prefix else image_data
        
        log_fn(f"üîç [{field_id}] Validating: total={len(image_data)} chars, prefix={'‚úÖ' if has_prefix else '‚ùå'}, base64={len(img_data)} chars")
        
        if len(img_data) < 100:
            logging.error(f"‚ùå [{field_id}] Image validation: too short ({len(img_data)} chars)")
            return False
        
        # Try to decode
        img_bytes = base64.b64decode(img_data)
        log_fn(f"üîç [{field_id}] Base64 decoded: {len(img_bytes)} bytes")
        
        # Try to open with PIL
        from PIL import Image
        from io import BytesIO
        img = Image.open(BytesIO(img_bytes))
        
        # Check minimum size
        if img.width < 10 or img.height < 10:
            logging.error(f"‚ùå [{field_id}] Image validation: too small ({img.width}x{img.height})")
            return False
        
        log_fn(f"‚úÖ [{field_id}] Image valid: {img.width}x{img.height}, {img.format}")
        return True
    except Exception as e:
        logging.error(f"‚ùå [{field_id}] Image validation failed: {str(e)}")
        return False

def _validate_table_data(table_data) -> bool:
    """Valida estrutura da tabela de repara√ß√µes"""
    try:
        if not table_data:
            return False
        
        if isinstance(table_data, str):
            import json
            table_data = json.loads(table_data)
        
        if not isinstance(table_data, list):
            return False
        
        # Check each row has required fields
        for row in table_data:
            if not isinstance(row, dict):
                return False
            # Must have at least 'description'
            if 'description' not in row:
                return False
        
        return True
    except:
        return False

def _format_currency(value) -> str:
    """Formata valor como moeda portuguesa (120,00) - SEM ‚Ç¨ pois j√° est√° no template"""
    try:
        # Convert to float
        if isinstance(value, str):
            value = float(value.replace(',', '.').replace('‚Ç¨', '').strip())
        
        # Format with thousands separator and 2 decimals - SEM s√≠mbolo ‚Ç¨
        return f"{value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
    except:
        return str(value)

def _format_date(value) -> str:
    """Formata data como dd/mm/yyyy"""
    try:
        from datetime import datetime
        
        if not value:
            return ''
        
        # Try to parse various formats
        for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
            try:
                dt = datetime.strptime(str(value), fmt)
                return dt.strftime('%d/%m/%Y')
            except:
                continue
        
        return str(value)
    except:
        return str(value)

def _format_number(value) -> str:
    """Formata n√∫mero com separadores de milhar"""
    try:
        if isinstance(value, str):
            value = float(value.replace(',', '.').strip())
        
        # Format with thousands separator
        return f"{value:,.0f}".replace(',', '.')
    except:
        return str(value)

def _format_hours(value) -> str:
    """Formata horas SEM casas decimais"""
    try:
        if isinstance(value, str):
            value = float(value.replace(',', '.').strip())
        
        # Sem casas decimais para horas
        return f"{int(value)}"
    except:
        return str(value)

def _get_field_style(field_id: str) -> dict:
    """
    Retorna estilo customizado para o campo
    Retorna: {'font': str, 'size': int, 'color': tuple, 'bold': bool, 'italic': bool}
    """
    field_id_lower = field_id.lower()
    
    # Default style - Arial 8pt, sem negrito ou it√°lico
    style = {
        'font': 'Helvetica',  # ReportLab usa Helvetica (similar a Arial)
        'size': 8,
        'color': (0, 0, 0),  # RGB black
        'bold': False,
        'italic': False
    }
    
    # EXCE√á√ïES:
    # DR number e RA number (contract) ficam em negrito
    if any(word in field_id_lower for word in ['dr_number', 'contract_number', 'contractnumber']):
        style['bold'] = True
    
    # Total da repara√ß√£o: 9pt bold
    if any(word in field_id_lower for word in ['total_repair', 'totalrepair', 'total_cost', 'totalcost']):
        style['size'] = 9
        style['bold'] = True
    
    return style

def _detect_field_type(field_id: str) -> str:
    """
    Detecta o tipo de campo baseado no field_id
    Retorna: 'text', 'image', 'signature', 'table', 'images', 'currency', 'date', 'number', 'hours'
    """
    field_id_lower = field_id.lower()
    
    if 'signature' in field_id_lower or 'sign' in field_id_lower:
        return 'signature'
    elif 'photo' in field_id_lower or 'image' in field_id_lower or 'croqui' in field_id_lower or 'diagram' in field_id_lower:
        return 'image'
    elif field_id_lower == 'repair_items' or field_id_lower == 'table_repair':
        # Apenas repair_items (JSON array) √© tabela - repair_line_X s√£o campos individuais
        return 'table'
    elif 'images' in field_id_lower:
        return 'images'
    elif 'hours' in field_id_lower or 'hour' in field_id_lower:
        return 'hours'  # ‚úÖ TIPO SEPARADO para horas (sem decimais)
    elif any(word in field_id_lower for word in ['cost', 'price', 'total', 'subtotal']):
        return 'currency'
    elif 'date' in field_id_lower:
        return 'date'
    elif any(word in field_id_lower for word in ['km', 'quantity', 'qty', 'number']) and not any(exclude in field_id_lower for exclude in ['contract', 'dr_number', 'ra_number', 'phone']):
        return 'number'
    else:
        return 'text'

def _calculate_centered_y(y, height, font_size):
    """
    Calcula a posi√ß√£o Y para centralizar verticalmente o texto na caixa
    y: posi√ß√£o Y da caixa (inferior)
    height: altura da caixa
    font_size: tamanho da fonte
    """
    # ReportLab desenha texto a partir da baseline
    # Para centralizar: y + (height / 2) - (font_size / 3)
    return y + (height / 2) - (font_size / 3)

def _fill_template_pdf_with_data(report_data: dict) -> bytes:
    """
    Preenche o template PDF ativo com dados usando coordenadas mapeadas
    Suporta: texto, imagens, tabelas, assinaturas
    Retorna PDF preenchido como bytes
    """
    try:
        from PyPDF2 import PdfReader, PdfWriter
        from reportlab.pdfgen import canvas as rl_canvas
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.colors import black, grey
        from reportlab.lib.utils import ImageReader
        from io import BytesIO
        from PIL import Image
        import base64
        import json
        
        # LOG: Dados recebidos
        logging.info("="*80)
        logging.info("üîç PDF FILL - Report data received:")
        logging.info(f"   date: {report_data.get('date', 'MISSING')}")
        logging.info(f"   inspection_date: {report_data.get('inspection_date', 'MISSING')}")
        logging.info(f"   dr_number: '{report_data.get('dr_number', 'MISSING')}'")
        logging.info(f"   ra_number: '{report_data.get('ra_number', 'MISSING')}'")
        logging.info(f"   contract_number: '{report_data.get('contract_number', 'MISSING')}'")
        logging.info(f"   contractNumber: '{report_data.get('contractNumber', 'MISSING')}'")
        logging.info(f"   vehicle_diagram: {'PRESENT (' + str(len(report_data.get('vehicle_diagram', ''))) + ' chars)' if report_data.get('vehicle_diagram') else 'MISSING'}")
        
        # Log TODAS as chaves com "diagram" ou "pin"
        diagram_keys = [k for k in report_data.keys() if 'diagram' in k.lower() or 'pin' in k.lower()]
        logging.info(f"   Keys com diagram/pin: {diagram_keys}")
        for key in diagram_keys:
            val = report_data.get(key, '')
            logging.info(f"      {key}: {type(val).__name__}, len={len(str(val)) if val else 0}")
        
        pins_check = report_data.get('damage_pins') or report_data.get('damageDiagramData') or report_data.get('damage_diagram_data')
        logging.info(f"   damage_pins/damageDiagramData: {'FOUND' if pins_check else 'MISSING'}")
        if pins_check:
            logging.info(f"      Pins data: {str(pins_check)[:200]}...")
        logging.info("="*80)
        
        # 1. Carregar template ativo da BD
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = False
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    is_postgres = True
                
                if is_postgres:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT file_data FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                    cursor.close()
                else:
                    cursor = conn.execute("""
                        SELECT file_data FROM damage_report_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                
                if not row or not row[0]:
                    # Fallback: usar arquivo f√≠sico
                    with open('Damage Report.pdf', 'rb') as f:
                        template_data = f.read()
                else:
                    template_data = row[0]
                    # Convert memoryview to bytes if needed (PostgreSQL returns memoryview)
                    if isinstance(template_data, memoryview):
                        template_data = bytes(template_data)
            finally:
                conn.close()
        
        # 2. Carregar coordenadas mapeadas
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT field_id, x, y, width, height, page 
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                    cursor.close()
                else:
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page 
                        FROM damage_report_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                # Suportar mesmo field_id em m√∫ltiplas p√°ginas usando chave composta
                coordinates = {}
                for row in rows:
                    field_id, x, y, width, height, page = row
                    page_num = int(page) if page else 1
                    
                    # Chave composta: field_id@pageN para m√∫ltiplas p√°ginas
                    key = f"{field_id}@page{page_num}"
                    
                    coordinates[key] = {
                        'field_id': field_id,  # Original field_id para buscar valor
                        'x': float(x),
                        'y': float(y),
                        'width': float(width),
                        'height': float(height),
                        'page': page_num
                    }
            finally:
                conn.close()
        
        # 3. Ler template PDF
        template_pdf = PdfReader(BytesIO(template_data))
        output_pdf = PdfWriter()
        
        # 4. Para cada p√°gina do template
        # SUPORTE A M√öLTIPLAS P√ÅGINAS:
        # - Cada campo tem coords['page'] (1-indexed)
        # - Loop processa todas as p√°ginas do template
        # - Campos s√£o renderizados apenas na p√°gina correta
        # - Tabelas grandes podem span m√∫ltiplas p√°ginas (mapeando campo por p√°gina)
        # - Grid de fotos pode ser distribu√≠do em v√°rias p√°ginas
        num_pages = len(template_pdf.pages)
        
        for page_num in range(num_pages):
            page = template_pdf.pages[page_num]
            page_width = float(page.mediabox.width)
            page_height = float(page.mediabox.height)
            
            # Criar overlay com texto para esta p√°gina
            packet = BytesIO()
            can = rl_canvas.Canvas(packet, pagesize=(page_width, page_height))
            can.setFont("Helvetica", 8)
            
            # Preencher campos desta p√°gina
            for composite_key, coords in coordinates.items():
                # MULTI-PAGE FILTER: Skip se campo n√£o pertence a esta p√°gina
                if coords['page'] != (page_num + 1):
                    continue
                
                # Extrair field_id original da coordenada
                field_id = coords['field_id']
                
                # FIELD NAME ALIASES: Mapear snake_case (BD) ‚Üí camelCase (report_data)
                # Necess√°rio porque coordenadas usam snake_case mas report_data tem camelCase
                field_aliases = {
                    'ra_number': 'ra_number',  # ‚úÖ RA Number (mesmo nome)
                    'contract_number': 'contractNumber',
                    'dr_number': 'dr_number',  # ‚úÖ DR Number (mesmo nome)
                    'vehicle_diagram': 'vehicle_diagram',  # ‚úÖ Croqui (mesmo nome)
                    'vehicleDiagram': 'vehicle_diagram',  # ‚úÖ Alias camelCase ‚Üí snake_case
                    'damage_pins': 'damage_pins',  # ‚úÖ Pins do diagrama
                    'customer_name': 'clientName',
                    'customer_email': 'clientEmail',
                    'customer_phone': 'clientPhone',
                    'customer_address': 'address',
                    'customer_postal': 'customer_postal',  # ‚úÖ C√≥digo Postal + Cidade (campo combinado)
                    'postalCodeCity': 'customer_postal',  # ‚úÖ Alias do form
                    'customer_city': 'city',
                    'customer_country': 'country',
                    'vehicle_plate': 'vehiclePlate',
                    'vehicle_brand': 'vehicleBrand',
                    'vehicle_model': 'vehicleModel',
                    'vehicle_brand_model': 'vehicleBrandModel',  # ‚úÖ Campo combinado Marca / Modelo
                    'vehicleBrandModel': 'vehicleBrandModel',  # ‚úÖ Alias direto
                    'vehicle_color': 'vehicleColor',
                    'vehicle_km': 'vehicleKm',
                    'pickup_date': 'pickupDate',  # ‚úÖ Data FORMATADA dd-mm-yyyy
                    'pickup_time': 'pickupTime',
                    'pickup_location': 'pickupLocation',
                    'return_date': 'returnDate',  # ‚úÖ Data FORMATADA dd-mm-yyyy
                    'return_time': 'returnTime',
                    'return_location': 'returnLocation',
                    'total_amount': 'total_amount',  # ‚úÖ Total da repara√ß√£o
                    'total_repair_cost': 'total_amount',  # ‚úÖ Alias
                    'totalRepairCost': 'total_amount',  # ‚úÖ Alias camelCase
                    'inspector_name': 'inspector_name',  # ‚úÖ Mantido para compatibilidade
                    'issued_by': 'issued_by',  # ‚úÖ Colaborador (alias)
                    'inspection_date': 'inspection_date',
                }
                
                # NOTA: vehicle_diagram e damage_photo_X j√° batem (ambos com underscore)
                # NOTA: damage_description_line_X precisa mapear para damage_X
                for i in range(1, 16):
                    field_aliases[f'damage_description_line_{i}'] = f'damage_{i}'
                
                # Obter valor do campo (tentar field_id direto primeiro, depois alias)
                value = report_data.get(field_id, '')
                alias_used = None
                if not value and field_id in field_aliases:
                    alias_key = field_aliases[field_id]
                    value = report_data.get(alias_key, '')
                    if value:
                        alias_used = alias_key
                
                # LOG ESPECIAL para vehicle_diagram/vehicleDiagram
                if 'diagram' in field_id.lower() and 'vehicle' in field_id.lower():
                    logging.info(f"üîç Processing {field_id}:")
                    logging.info(f"   Direct lookup (field_id='{field_id}'): {'FOUND' if report_data.get(field_id) else 'NOT FOUND'}")
                    if field_id in field_aliases:
                        alias_key = field_aliases[field_id]
                        logging.info(f"   Alias lookup (alias='{alias_key}'): {'FOUND' if report_data.get(alias_key) else 'NOT FOUND'}")
                    logging.info(f"   Final value: {'FOUND (' + str(len(str(value))) + ' chars)' if value else 'NOT FOUND'}")
                
                # Log detalhado para campos importantes
                is_important = any(keyword in field_id.lower() for keyword in ['diagram', 'photo', 'signature', 'dr_number', 'ra_number', 'date', 'quantity', 'pin'])
                if is_important:
                    logging.info(f"üîç Campo: {field_id} (page {coords['page']}) | Type: {field_type}")
                    logging.info(f"   Alias usado: {alias_used if alias_used else 'N/A'}")
                    logging.info(f"   Tem valor? {bool(value)} | Tamanho: {len(str(value)) if value else 0}")
                    if value:
                        preview = str(value)[:80] if not isinstance(value, list) else f"[{len(value)} items]"
                        logging.info(f"   Preview: {preview}")
                    else:
                        # Verificar se existe em report_data com outro nome
                        if 'diagram' in field_id.lower() or 'pin' in field_id.lower():
                            possible_keys = [k for k in report_data.keys() if 'diagram' in k.lower() or 'pin' in k.lower()]
                            logging.info(f"   ‚ö†Ô∏è VAZIO! Chaves dispon√≠veis: {possible_keys}")
                        if 'date' in field_id.lower():
                            date_keys = [k for k in report_data.keys() if 'date' in k.lower()]
                            logging.info(f"   ‚ö†Ô∏è VAZIO! Chaves com 'date': {date_keys}")
                
                if not value:
                    if is_important:
                        logging.warning(f"‚ö†Ô∏è Skipping {field_id} - sem valor")
                    continue
                
                # Detectar tipo de campo (ANTES de converter coordenadas para usar nos logs)
                field_type = _detect_field_type(field_id)
                
                # ‚úÖ EXTRAIR COORDENADAS DO DICION√ÅRIO
                x = coords['x']
                y = page_height - coords['y'] - coords['height']  # Transform Y coordinate
                width = coords['width']
                height = coords['height']
                
                # ‚úÖ N√ÉO EXPANDIR DIAGRAMA - usar tamanho EXATO do mapeamento (preview)
                # Os pins j√° est√£o na imagem capturada pelo html2canvas
                is_diagram = 'diagram' in field_id.lower() or 'croqui' in field_id.lower()
                # EXPANS√ÉO DESATIVADA - causar desalinhamento dos pins
                # if is_diagram and field_type == 'image':
                #     expansion_factor = 1.20
                #     ...
                # Usar coordenadas EXATAS do mapeamento
                
                # LOG PR√â-TRANSFORMA√á√ÉO (para diagrama)
                is_diagram_check = 'diagram' in field_id.lower() or 'croqui' in field_id.lower()
                if is_diagram_check:
                    logging.error(f"üñºÔ∏èüñºÔ∏èüñºÔ∏è TENTANDO DESENHAR CROQUI: {field_id}")
                    logging.error(f"   Valor tem {len(str(value))} chars")
                    logging.error(f"   Coords ORIGINAIS (DB): x={coords['x']}, y={coords['y']}, w={coords['width']}, h={coords['height']}")
                    logging.error(f"   page_height={page_height}")
                    logging.error(f"   Coords TRANSFORMADAS: x={x}, y={y}, w={width}, h={height}")
                
                # IMAGEM ou ASSINATURA - processar TODOS os campos do tipo 'image'
                if field_type == 'image' or field_type == 'signature':
                    # VALIDA√á√ÉO
                    if is_diagram_check:
                        logging.error("üñºÔ∏è Chamando valida√ß√£o...")
                    
                    is_valid = _validate_image_data(value, field_id)
                    
                    if is_diagram_check:
                        logging.error(f"üñºÔ∏è Valida√ß√£o retornou: {is_valid}")
                    
                    if not is_valid:
                        if is_diagram_check:
                            logging.error("üñºÔ∏è VALIDA√á√ÉO FALHOU! Desenhando placeholder...")
                        logging.warning(f"Invalid image data for {field_id}, drawing placeholder")
                        can.setStrokeColor(grey)
                        can.setFillColor(grey)
                        can.rect(x, y, width, height, stroke=1, fill=0)
                        can.setFont("Helvetica", 6)
                        can.drawString(x + 2, y + height/2, "[Invalid Image]")
                        continue
                    
                    try:
                        if is_diagram_check:
                            logging.error("üñºÔ∏è VALIDA√á√ÉO OK! Come√ßando processamento...")
                        
                        # Decode base64 image
                        if isinstance(value, str) and ('data:image' in value or value.startswith('/9j')):
                            if is_diagram_check:
                                logging.error(f"üñºÔ∏è Decodificando base64 ({len(value)} chars)...")
                            
                            # Remove data:image prefix if present
                            img_data = value.split(',')[1] if ',' in value else value
                            img_bytes = base64.b64decode(img_data)
                            
                            if is_diagram_check:
                                logging.error(f"üñºÔ∏è Decodificado: {len(img_bytes)} bytes. Abrindo com PIL...")
                            
                            # Load image with PIL
                            img = Image.open(BytesIO(img_bytes))
                            
                            # Convert to RGB if needed (EXCETO para diagrama - manter transpar√™ncia)
                            is_diagram = 'diagram' in field_id.lower() or 'croqui' in field_id.lower()
                            
                            if is_diagram_check:
                                logging.error(f"üñºÔ∏è Imagem aberta: {img.mode}, {img.size}")
                            
                            logging.info(f"üîç {field_id}: is_diagram={is_diagram}, mode={img.mode}, size={img.size}")
                            
                            if not is_diagram and img.mode in ('RGBA', 'LA', 'P'):
                                # Fotos normais: converter para RGB com fundo branco
                                background = Image.new('RGB', img.size, (255, 255, 255))
                                if img.mode == 'P':
                                    img = img.convert('RGBA')
                                background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                                img = background
                            elif is_diagram:
                                if is_diagram_check:
                                    logging.error(f"üñºÔ∏è √â diagrama! Removendo fundo branco...")
                                
                                # Diagrama: REMOVER fundo branco e garantir transpar√™ncia (SEM NUMPY)
                                if img.mode != 'RGBA':
                                    img = img.convert('RGBA')
                                
                                # Usar PIL para remover fundo branco (sem numpy)
                                pixels = img.load()
                                width_img, height_img = img.size
                                
                                # Threshold para considerar "branco" (RGB > 240)
                                threshold = 240
                                
                                # ‚úÖ FIX: Usar nomes diferentes (px_x, px_y) para n√£o sobrescrever as coordenadas do campo
                                for px_y in range(height_img):
                                    for px_x in range(width_img):
                                        r, g, b, a = pixels[px_x, px_y]
                                        # Se pixel √© branco/claro, tornar transparente
                                        if r > threshold and g > threshold and b > threshold:
                                            pixels[px_x, px_y] = (r, g, b, 0)  # Alpha = 0
                                
                                if is_diagram_check:
                                    logging.error(f"üñºÔ∏è Fundo branco removido!")
                                
                                logging.info(f"‚úÖ Diagrama: fundo branco removido, mant√©m transpar√™ncia (RGBA) - PIL only")
                            
                            # Save to BytesIO for ReportLab
                            img_buffer = BytesIO()
                            if is_diagram and img.mode == 'RGBA':
                                # Diagrama com transpar√™ncia: usar PNG
                                img.save(img_buffer, format='PNG')
                            else:
                                # Fotos normais: usar JPEG
                                img.save(img_buffer, format='JPEG', quality=85)
                            img_buffer.seek(0)
                            
                            # DIAGRAMA: usar CONTAIN (sem crop) | FOTOS: usar COVER (com crop)
                            img_width, img_height = img.size
                            
                            # Vari√°veis para posi√ß√£o final do diagrama (para pins)
                            diagram_final_x, diagram_final_y = x, y
                            diagram_final_width, diagram_final_height = width, height
                            
                            # ‚úÖ is_diagram j√° foi definido acima (linha 18391)
                            if is_diagram:
                                # DIAGRAMA: USAR TAMANHO EXATO DA BOX MAPEADA
                                # Box: X=25, Y=254, W=256, H=201 (p√°gina 1)
                                # Limite direito: X=318 (in√≠cio das descri√ß√µes)
                                # SOLU√á√ÉO: Desenhar EXATAMENTE no tamanho mapeado
                                
                                if is_diagram_check:
                                    logging.error(f"üñºÔ∏è MODO DIAGRAMA - TAMANHO EXATO DA BOX")
                                    logging.error(f"üñºÔ∏è Box mapeada: x={x}, y={y}, w={width}, h={height}")
                                    logging.error(f"üñºÔ∏è Imagem capturada: {img_width}x{img_height}")
                                
                                # ‚úÖ ESCALAR IMAGEM CAPTURADA PARA CABER NA BOX
                                # Frontend captura em ~500px (boa qualidade)
                                # Backend escala para 256√ó201px mantendo aspect ratio
                                
                                # Calcular escala para caber na box mantendo propor√ß√£o
                                img_ratio = img_width / img_height
                                box_ratio = width / height
                                
                                if is_diagram_check:
                                    logging.error(f"üñºÔ∏è Imagem capturada: {img_width}√ó{img_height} (ratio {img_ratio:.3f})")
                                    logging.error(f"üñºÔ∏è Box PDF: {int(width)}√ó{int(height)} (ratio {box_ratio:.3f})")
                                
                                # Escalar mantendo aspect ratio (fit dentro da box)
                                if img_ratio > box_ratio:
                                    # Imagem mais larga - limitar por largura
                                    draw_width = width
                                    draw_height = width / img_ratio
                                    draw_x = x
                                    draw_y = y + (height - draw_height) / 2  # Centralizar verticalmente
                                else:
                                    # Imagem mais alta - limitar por altura
                                    draw_height = height
                                    draw_width = height * img_ratio
                                    draw_x = x + (width - draw_width) / 2  # Centralizar horizontalmente
                                    draw_y = y
                                
                                if is_diagram_check:
                                    logging.error(f"üñºÔ∏è Desenhar escalado: ({int(draw_x)}, {int(draw_y)}) {int(draw_width)}√ó{int(draw_height)}")
                                
                                # Desenhar diagrama escalado mantendo propor√ß√£o
                                logging.error(f"üñºÔ∏è Preparando buffer PNG para {field_id}...")
                                img_buffer = BytesIO()
                                img.save(img_buffer, format='PNG')
                                img_buffer.seek(0)
                                
                                logging.error(f"üñºÔ∏è Chamando can.drawImage em ({int(draw_x)}, {int(draw_y)}, {int(draw_width)}√ó{int(draw_height)})...")
                                can.drawImage(
                                    ImageReader(img_buffer),
                                    draw_x, draw_y,
                                    width=draw_width,
                                    height=draw_height,
                                    preserveAspectRatio=True,
                                    mask='auto'
                                )
                                logging.error(f"üñºÔ∏è‚úÖ DIAGRAMA DESENHADO COM SUCESSO! {field_id}")
                                logging.info(f"‚úÖ Drew diagram {field_id} ({int(img_width)}√ó{int(img_height)} ‚Üí {int(draw_width)}√ó{int(draw_height)})")
                                
                                # üéØ N√ÉO DESENHAR PINS - A imagem vehicleDiagram do frontend J√Å TEM os pins desenhados!
                                # O frontend captura o croqui com html2canvas com os pins j√° vis√≠veis
                                # Se desenharmos novamente, ficam DUPLICADOS (como o user reportou!)
                                # A posi√ß√£o dos n√∫meros NOS PINS est√° correta no CSS (padding-top: 12px)
                                pins_data_disabled = None  # Desativado para evitar duplica√ß√£o
                                if pins_data_disabled:  # Never executes - pins v√™m na imagem!
                                    try:
                                        # Parse JSON se necess√°rio
                                        if isinstance(pins_data, str):
                                            pins_array = json.loads(pins_data) if pins_data.startswith('[') else []
                                        else:
                                            pins_array = pins_data if isinstance(pins_data, list) else []
                                        
                                        if pins_array:
                                            # ‚úÖ FILTRAR pins desta vista (ou todos se view='all')
                                            # Verificar se h√° pins com view espec√≠fica (top/side) ou gen√©rica (all)
                                            has_view_filter = any(p.get('view') in ['top', 'side'] for p in pins_array)
                                            
                                            if has_view_filter:
                                                # Sistema com m√∫ltiplas vistas - filtrar por vista
                                                current_view = 'top' if field_id == 'vehicleDiagram' else 'side'
                                                pins_for_this_view = [p for p in pins_array if p.get('view') == current_view]
                                            else:
                                                # Sistema com vista √∫nica 'all' - usar todos os pins
                                                current_view = 'all'
                                                pins_for_this_view = pins_array
                                            
                                            # ‚úÖ CALCULAR ESCALA: Frontend ‚Üí PDF
                                            # Frontend: Container com imagem 986x700
                                            # PDF: Desenho em draw_width x draw_height
                                            scale_x = draw_width / img_width
                                            scale_y = draw_height / img_height
                                            
                                            logging.error(f"üéØ DESENHANDO PINS PARA VISTA '{current_view}' (field_id={field_id})...")
                                            logging.error(f"   üìä TOTAL PINS RECEBIDOS: {len(pins_array)}")
                                            logging.error(f"   ‚úÖ PINS DESTA VISTA ({current_view}): {len(pins_for_this_view)}")
                                            for idx, p in enumerate(pins_for_this_view):
                                                logging.error(f"      Pin {idx}: number={p.get('number')}, x={p.get('x')}, y={p.get('y')}, view={p.get('view')}, size={p.get('size')}")
                                            logging.error(f"   üìê Diagrama PDF: x={draw_x:.1f}, y={draw_y:.1f}, w={draw_width:.1f}, h={draw_height:.1f}")
                                            logging.error(f"   üìê Imagem original: {img_width}x{img_height}")
                                            logging.error(f"   üìê Escala: scale_x={scale_x:.4f}, scale_y={scale_y:.4f}")
                                            logging.error(f"   üìê Container frontend: {img_width}x{img_height} (onde os pins foram clicados)")
                                            
                                            # Carregar imagem do pin (40x56px no frontend)
                                            pin_img_path = 'static/pin.png'
                                            pin_img = Image.open(pin_img_path)
                                            
                                            # ‚úÖ USAR APENAS PINS DESTA VISTA
                                            for pin in pins_for_this_view:
                                                pin_x_original = pin.get('x', 0)
                                                pin_y_original = pin.get('y', 0)
                                                pin_number = pin.get('number', 1)
                                                pin_size = pin.get('size', 1)
                                                
                                                # ‚úÖ TAMANHO DO PIN: Escalar com diagrama MAS garantir m√≠nimo vis√≠vel
                                                # Frontend: 40x56px base √ó slider do utilizador
                                                # PDF: Escalar pela mesma propor√ß√£o do diagrama + m√≠nimo de 20pt
                                                pin_width_original = 40 * pin_size
                                                pin_height_original = 56 * pin_size
                                                
                                                # ‚úÖ Escalar com diagrama mas com M√çNIMO de 20pt largura (visibilidade)
                                                pin_width_pdf = max(20, pin_width_original * scale_x)
                                                pin_height_pdf = max(28, pin_height_original * scale_y)  # Manter aspect ratio 40:56
                                                
                                                # ‚úÖ COORDENADAS DO CANVAS (986x700):
                                                # Canvas JavaScript: Y=0 no TOPO, Y cresce para BAIXO
                                                # PDF: Y=0 no FUNDO, Y cresce para CIMA
                                                # As coordenadas do canvas j√° s√£o convertidas no frontend!
                                                
                                                pin_x_scaled = draw_x + (pin_x_original * scale_x)
                                                # ‚úÖ INVERTER Y! 
                                                # Canvas: Y=0 topo, cresce para baixo
                                                # PDF: Y=0 fundo, cresce para cima
                                                # draw_y = fundo do diagrama no PDF
                                                # draw_y + draw_height = topo do diagrama no PDF
                                                # F√≥rmula: bottom - offset_from_top
                                                pin_y_scaled = draw_y + draw_height - (pin_y_original * scale_y)
                                                
                                                # üîç DEBUG: Verificar se pin est√° dentro do diagrama
                                                logging.error(f"   üîç Pin #{pin_number} DEBUG:")
                                                logging.error(f"      Canvas: ({pin_x_original:.1f}, {pin_y_original:.1f})")
                                                logging.error(f"      Escalado: ({pin_x_original * scale_x:.1f}, {pin_y_original * scale_y:.1f})")
                                                logging.error(f"      PDF scaled: ({pin_x_scaled:.1f}, {pin_y_scaled:.1f})")
                                                logging.error(f"      Diagrama range: X[{draw_x:.1f}, {draw_x + draw_width:.1f}] Y[{draw_y:.1f}, {draw_y + draw_height:.1f}]")
                                                
                                                # ‚úÖ AJUSTE PARA MANTER PONTA DO PIN NA COORDENADA
                                                # Imagem original: 165x219px
                                                # Ponta visual est√° aproximadamente a 2% da altura a partir do fundo
                                                # (pequena margem/transpar√™ncia inferior)
                                                pin_x_final = pin_x_scaled - (pin_width_pdf / 2)  # Centralizar horizontalmente
                                                
                                                # ‚úÖ Subtrair margem inferior proporcional ao tamanho do pin
                                                # Isto garante que a PONTA VISUAL fica sempre na mesma coordenada
                                                # independentemente do tamanho do pin
                                                pin_tip_offset = pin_height_pdf * 0.02  # 2% margem inferior
                                                pin_y_final = pin_y_scaled - pin_tip_offset
                                                
                                                logging.error(f"      Final pos: ({pin_x_final:.1f}, {pin_y_final:.1f}) ‚Üê Ponta do pin (offset={pin_tip_offset:.1f})")
                                                
                                                # Desenhar imagem do pin
                                                pin_buffer = BytesIO()
                                                pin_img.save(pin_buffer, format='PNG')
                                                pin_buffer.seek(0)
                                                
                                                can.drawImage(
                                                    ImageReader(pin_buffer),
                                                    pin_x_final, pin_y_final,
                                                    width=pin_width_pdf,
                                                    height=pin_height_pdf,
                                                    mask='auto'
                                                )
                                                
                                                # ‚úÖ DESENHAR N√öMERO PRETO CENTRALIZADO NA BOLA DO PIN
                                                can.setFillColorRGB(0, 0, 0)  # PRETO
                                                
                                                # Fonte proporcional ao pin (58% da largura - MAIOR, m√≠nimo 10pt)
                                                font_size = max(10, int(pin_width_pdf * 0.58))
                                                can.setFont("Helvetica-Bold", font_size)  # BOLD
                                                
                                                # ‚úÖ POSICIONAR N√öMERO IGUAL AO HTML (padding-top: 12px)
                                                text_x = pin_x_scaled
                                                
                                                # CSS do HTML: padding-top: 12px significa que o n√∫mero
                                                # come√ßa a 12px do TOPO do pin (height: 56px)
                                                #
                                                # No PDF, Y cresce de BAIXO para CIMA (invertido!)
                                                # 12px do topo = (56px - 12px) = 44px do fundo
                                                # 
                                                # Mas padding-top √© onde o TEXTO COME√áA, n√£o o centro!
                                                # Com line-height: 24px, o centro fica a 12 + 12 = 24px do topo
                                                # 24px do topo = 56 - 24 = 32px do fundo
                                                # 
                                                # Propor√ß√£o: 32/56 = 0.5714 EST√Å MUITO ALTO!
                                                # Na pr√°tica visual, o centro da bola est√° mais baixo (~40%)
                                                
                                                # Testar 40% da altura (entre topo e meio)
                                                text_y_base = pin_y_final + (pin_height_pdf * 0.40)
                                                
                                                # drawCentredString usa BASELINE, ajustar para cima
                                                text_y = text_y_base + (font_size * 0.25)
                                                
                                                can.drawCentredString(text_x, text_y, str(pin_number))
                                                
                                                logging.error(f"   üìç Pin #{pin_number}:")
                                                logging.error(f"      Frontend: ({pin_x_original:.0f}, {pin_y_original:.0f})  [clique do rato]")
                                                logging.error(f"      Escalado: ({pin_x_original * scale_x:.1f}, {pin_y_original * scale_y:.1f})")
                                                logging.error(f"      PDF pos:  ({pin_x_scaled:.1f}, {pin_y_scaled:.1f})  [ap√≥s offset + invers√£o Y]")
                                                logging.error(f"      Final:    ({pin_x_final:.1f}, {pin_y_final:.1f})  [ponta do pin]")
                                                logging.error(f"      Size:     {pin_width_pdf:.1f}x{pin_height_pdf:.1f}  Font: {font_size}pt")
                                            
                                            logging.error(f"üéØ‚úÖ {len(pins_for_this_view)} PINS DESENHADOS PARA VISTA '{current_view}'!")
                                        else:
                                            logging.error("‚ö†Ô∏è Pins array est√° vazio")
                                    except Exception as e:
                                        logging.error(f"‚ùå Erro ao desenhar pins: {e}", exc_info=True)
                                else:
                                    logging.error("‚ö†Ô∏è Nenhum pin data encontrado")
                            else:
                                # FOTOS: COVER mode (preencher com crop)
                                img_aspect = img_width / img_height
                                box_aspect = width / height
                                
                                if img_aspect > box_aspect:
                                    # Imagem mais larga: ajustar pela ALTURA
                                    new_height = height
                                    new_width = height * img_aspect
                                else:
                                    # Imagem mais alta: ajustar pela LARGURA
                                    new_width = width
                                    new_height = width / img_aspect
                                
                                # Redimensionar
                                img_resized = img.resize((int(new_width), int(new_height)), Image.Resampling.LANCZOS)
                                
                                # Crop para caixa exata (center crop)
                                if new_width > width:
                                    left = (new_width - width) / 2
                                    img_cropped = img_resized.crop((int(left), 0, int(left + width), int(new_height)))
                                else:
                                    top = (new_height - height) / 2
                                    img_cropped = img_resized.crop((0, int(top), int(new_width), int(top + height)))
                                
                                # Save
                                img_buffer = BytesIO()
                                if img_cropped.mode == 'RGBA':
                                    background = Image.new('RGB', img_cropped.size, (255, 255, 255))
                                    background.paste(img_cropped, mask=img_cropped.split()[-1])
                                    img_cropped = background
                                img_cropped.save(img_buffer, format='JPEG', quality=90)
                                img_buffer.seek(0)
                                
                                # Desenhar foto
                                can.drawImage(
                                    ImageReader(img_buffer),
                                    x, y,
                                    width=width,
                                    height=height
                                )
                                logging.info(f"‚úÖ Drew photo {field_id} (COVER mode: filled {int(width)}x{int(height)} box with crop)")
                    except Exception as e:
                        logging.error(f"Error drawing image {field_id}: {e}")
                        # Fallback: draw placeholder
                        can.setStrokeColor(grey)
                        can.setFillColor(grey)
                        can.rect(x, y, width, height, stroke=1, fill=0)
                        can.setFont("Helvetica", 6)
                        can.drawString(x + 2, y + height/2, "[Image]")
                
                elif field_type == 'table':
                    # TABELA DE REPARA√á√ïES (s√≥ repair_items √© tabela!)
                    # VALIDA√á√ÉO
                    if not _validate_table_data(value):
                        logging.warning(f"Invalid table data for {field_id}, drawing placeholder")
                        can.setFont("Helvetica", 7)
                        can.setFillColor(black)
                        can.drawString(x + 2, y + 4, "[Invalid Table Data]")
                        continue
                    
                    try:
                        # Parse table data (JSON array)
                        if isinstance(value, str):
                            table_data = json.loads(value) if value.startswith('[') else []
                        else:
                            table_data = value if isinstance(value, list) else []
                        
                        if table_data:
                            # Draw table grid
                            can.setStrokeColor(black)
                            can.setLineWidth(0.5)
                            
                            # Calculate row height
                            num_rows = min(len(table_data), 10)  # Max 10 rows
                            row_height = height / (num_rows + 1)  # +1 for header
                            
                            # Draw horizontal lines
                            for i in range(num_rows + 2):
                                line_y = y + (i * row_height)
                                can.line(x, line_y, x + width, line_y)
                            
                            # Draw vertical lines (5 columns: desc, qty, hours, price, total)
                            col_widths = [width * 0.35, width * 0.12, width * 0.13, width * 0.20, width * 0.20]
                            col_x = x
                            for col_width in col_widths:
                                can.line(col_x, y, col_x, y + height)
                                col_x += col_width
                            can.line(col_x, y, col_x, y + height)  # Last line
                            
                            # Draw header (CENTRALIZADO VERTICALMENTE)
                            can.setFont("Helvetica-Bold", 7)
                            header_font_size = 7
                            header_y_base = y + height - row_height
                            header_y = _calculate_centered_y(header_y_base, row_height, header_font_size)
                            can.drawString(x + 2, header_y, "Descri√ß√£o")
                            can.drawString(x + col_widths[0] + 2, header_y, "Qtd")
                            can.drawString(x + col_widths[0] + col_widths[1] + 2, header_y, "Horas")
                            can.drawString(x + col_widths[0] + col_widths[1] + col_widths[2] + 2, header_y, "Pre√ßo")
                            can.drawString(x + col_widths[0] + col_widths[1] + col_widths[2] + col_widths[3] + 2, header_y, "Total")
                            
                            # Draw rows with FORMATTING (CENTRALIZADO VERTICALMENTE)
                            can.setFont("Helvetica", 6)
                            row_font_size = 6
                            for i, row in enumerate(table_data[:10]):
                                row_y_base = y + height - ((i + 2) * row_height)
                                row_y = _calculate_centered_y(row_y_base, row_height, row_font_size)
                                
                                # Description (truncated, left-aligned)
                                can.drawString(x + 2, row_y, str(row.get('description', ''))[:30])
                                
                                # Quantity (number, center-aligned HORIZONTALMENTE)
                                qty_value = row.get('quantity', row.get('qty', ''))
                                qty_formatted = _format_number(qty_value) if qty_value else ''
                                # Centro da coluna quantity
                                qty_x = x + col_widths[0] + (col_widths[1] / 2)
                                can.drawCentredString(qty_x, row_y, qty_formatted)
                                logging.debug(f"   Qty: {qty_formatted} centered at x={qty_x}")
                                
                                # Hours (sem decimais, center-aligned)
                                hours_value = row.get('hours', '')
                                hours_formatted = _format_hours(hours_value) if hours_value else ''
                                hours_x = x + col_widths[0] + col_widths[1] + (col_widths[2] / 2)
                                can.drawCentredString(hours_x, row_y, hours_formatted)
                                
                                # Price (currency, right-aligned with ‚Ç¨ symbol)
                                price_formatted = _format_currency(row.get('price', '')) if row.get('price') else ''
                                price_x = x + col_widths[0] + col_widths[1] + col_widths[2] + col_widths[3] - 5
                                can.drawRightString(price_x, row_y, price_formatted)
                                
                                # Total (currency, right-aligned with ‚Ç¨ symbol)
                                total_formatted = _format_currency(row.get('total', '')) if row.get('total') else ''
                                total_x = x + col_widths[0] + col_widths[1] + col_widths[2] + col_widths[3] + col_widths[4] - 5
                                can.drawRightString(total_x, row_y, total_formatted)
                            
                            logging.info(f"‚úÖ Drew table for {field_id} with {len(table_data)} rows")
                    except Exception as e:
                        logging.error(f"Error drawing table {field_id}: {e}")
                        # Fallback: draw text
                        can.setFont("Helvetica", 8)
                        can.setFillColor(black)
                        text_y = _calculate_centered_y(y, height, 8)
                        can.drawString(x + 2, text_y, str(value)[:50])
                
                elif field_type == 'currency':
                    # MOEDA FORMATADA (alinhada √† direita para ‚Ç¨ symbol do template)
                    formatted_value = _format_currency(value)
                    style = _get_field_style(field_id)
                    
                    # Apply style
                    font_name = f"{style['font']}-Bold" if style['bold'] else style['font']
                    font_name = f"{font_name}-Oblique" if style['italic'] and not style['bold'] else font_name
                    can.setFont(font_name, style['size'])
                    can.setFillColorRGB(*style['color'])
                    
                    text_y = _calculate_centered_y(y, height, style['size'])
                    # Right-align para alinhar com s√≠mbolo ‚Ç¨ no template
                    can.drawRightString(x + width - 5, text_y, formatted_value)
                
                elif field_type == 'date':
                    # DATA FORMATADA
                    logging.info(f"üìÖ DATE FIELD DETECTED: {field_id}")
                    logging.info(f"   Raw value: '{value}' (type: {type(value).__name__})")
                    logging.info(f"   Position: x={x}, y={y}, w={width}, h={height}")
                    
                    if not value or value == '':
                        logging.warning(f"   ‚ö†Ô∏è DATE VALUE IS EMPTY! Skipping.")
                        # Desenhar placeholder para debug
                        can.setFont("Helvetica", 8)
                        can.setFillColorRGB(1, 0, 0)  # Vermelho
                        text_y = _calculate_centered_y(y, height, 8)
                        can.drawString(x + 2, text_y, "[NO DATE]")
                        continue
                    
                    formatted_value = _format_date(value)
                    style = _get_field_style(field_id)
                    
                    logging.info(f"   Formatted: '{formatted_value}'")
                    logging.info(f"   Style: font={style['font']}, size={style['size']}, color={style['color']}")
                    
                    # Apply style
                    font_name = f"{style['font']}-Bold" if style['bold'] else style['font']
                    font_name = f"{font_name}-Oblique" if style['italic'] and not style['bold'] else font_name
                    can.setFont(font_name, style['size'])
                    can.setFillColorRGB(*style['color'])
                    
                    text_y = _calculate_centered_y(y, height, style['size'])
                    can.drawString(x + 2, text_y, formatted_value)
                    logging.info(f"‚úÖ DATE DRAWN at x={x + 2}, y={text_y} ‚Üí '{formatted_value}'")
                
                elif field_type == 'number':
                    # N√öMERO FORMATADO
                    formatted_value = _format_number(value)
                    style = _get_field_style(field_id)
                    
                    # Apply style
                    font_name = f"{style['font']}-Bold" if style['bold'] else style['font']
                    font_name = f"{font_name}-Oblique" if style['italic'] and not style['bold'] else font_name
                    can.setFont(font_name, style['size'])
                    can.setFillColorRGB(*style['color'])
                    
                    text_y = _calculate_centered_y(y, height, style['size'])
                    
                    # QUANTIDADES: CENTER-ALIGNED (repair_line_X_qty)
                    if '_qty' in field_id.lower():
                        can.drawCentredString(x + width / 2, text_y, formatted_value)
                        logging.info(f"‚úÖ CENTERED QTY: {field_id} = '{formatted_value}' at center x={x + width / 2}")
                    else:
                        can.drawString(x + 2, text_y, formatted_value)
                
                elif field_type == 'hours':
                    # HORAS SEM DECIMAIS (center-aligned)
                    formatted_value = _format_hours(value)
                    style = _get_field_style(field_id)
                    
                    # Apply style
                    font_name = f"{style['font']}-Bold" if style['bold'] else style['font']
                    font_name = f"{font_name}-Oblique" if style['italic'] and not style['bold'] else font_name
                    can.setFont(font_name, style['size'])
                    can.setFillColorRGB(*style['color'])
                    
                    text_y = _calculate_centered_y(y, height, style['size'])
                    # Center-align para horas (como na tabela)
                    can.drawCentredString(x + width / 2, text_y, formatted_value)
                
                else:
                    # TEXTO SIMPLES COM ESTILO
                    style = _get_field_style(field_id)
                    
                    # Apply style
                    font_name = f"{style['font']}-Bold" if style['bold'] else style['font']
                    font_name = f"{font_name}-Oblique" if style['italic'] and not style['bold'] else font_name
                    can.setFont(font_name, style['size'])
                    can.setFillColorRGB(*style['color'])
                    
                    # Convert to uppercase (except for notes/observations fields)
                    text_value = str(value)
                    if 'notes' not in field_id and 'observation' not in field_id:
                        text_value = text_value.upper()
                    
                    # DR NUMBER, RA NUMBER, CONTRACT NUMBER: RIGHT-ALIGNED
                    field_id_lower = field_id.lower()
                    if 'dr_number' in field_id_lower or 'ra_number' in field_id_lower or 'contract' in field_id_lower:
                        text_y = _calculate_centered_y(y, height, style['size'])
                        can.drawRightString(x + width - 2, text_y, text_value[:50])
                        logging.info(f"‚úÖ‚úÖ‚úÖ RIGHT-ALIGNED: {field_id} = '{text_value[:50]}' at x={x + width - 2}, y={text_y}, width={width}")
                    # Handle multiline text for textarea fields
                    elif 'description' in field_id or 'notes' in field_id or 'damage' in field_id:
                        # Multiline text - CADA LINHA CENTRALIZADA VERTICALMENTE
                        lines = text_value.split('\n')
                        
                        # Se for apenas 1 linha, centralizar verticalmente
                        if len(lines) == 1:
                            text_y = _calculate_centered_y(y, height, style['size'])
                            can.drawString(x + 2, text_y, lines[0][:int(width / 5)])
                        else:
                            # M√∫ltiplas linhas: distribuir uniformemente
                            line_height = style['size'] + 2
                            max_lines = int(height / line_height)
                            lines_to_draw = lines[:max_lines]
                            
                            # Calcular espa√ßamento vertical total
                            total_text_height = len(lines_to_draw) * line_height
                            start_y = y + (height - total_text_height) / 2 + total_text_height - line_height
                            
                            for i, line in enumerate(lines_to_draw):
                                if start_y - (i * line_height) > y:
                                    # Centralizar cada linha verticalmente na sua pr√≥pria √°rea
                                    line_y = start_y - (i * line_height) + (line_height - style['size']) / 2
                                    can.drawString(x + 2, line_y, line[:int(width / 5)])
                    else:
                        # Single line text - centralizado verticalmente
                        text_y = _calculate_centered_y(y, height, style['size'])
                        can.drawString(x + 2, text_y, text_value[:int(width / 5)])
            
            can.save()
            
            # Merge overlay com p√°gina original
            packet.seek(0)
            overlay_pdf = PdfReader(packet)
            page.merge_page(overlay_pdf.pages[0])
            output_pdf.add_page(page)
        
        # 5. Retornar PDF preenchido
        output_buffer = BytesIO()
        output_pdf.write(output_buffer)
        output_buffer.seek(0)
        
        logging.info(f"‚úÖ PDF filled successfully:")
        logging.info(f"   - {len(coordinates)} total mapped fields")
        logging.info(f"   - {num_pages} page(s) in template")
        logging.info(f"   - Multi-page support: ACTIVE")
        return output_buffer.read()
        
    except Exception as e:
        logging.error(f"Error filling template PDF: {e}")
        # Fallback: retornar template vazio
        try:
            with open('Damage Report.pdf', 'rb') as f:
                return f.read()
        except:
            raise

@app.post("/api/damage-reports/validate")
async def validate_damage_report_data(request: Request):
    """Valida dados do Damage Report antes de gerar PDF"""
    require_auth(request)
    
    try:
        body = await request.json()
        
        validation_results = {
            'ok': True,
            'errors': [],
            'warnings': [],
            'field_count': 0
        }
        
        # Check required fields
        required_fields = ['drNumber', 'clientName', 'vehiclePlate']
        for field in required_fields:
            if not body.get(field):
                validation_results['errors'].append(f"Campo obrigat√≥rio vazio: {field}")
                validation_results['ok'] = False
        
        # Validate images
        image_fields = ['vehicleDiagram', 'damagePhoto1', 'damagePhoto2', 'damagePhoto3',
                       'damagePhoto4', 'damagePhoto5', 'damagePhoto6', 'damagePhoto7',
                       'damagePhoto8', 'damagePhoto9', 'signatureInspector', 'signatureClient']
        
        for field in image_fields:
            if body.get(field):
                validation_results['field_count'] += 1
                if not _validate_image_data(body[field], field):
                    validation_results['warnings'].append(f"Imagem inv√°lida: {field}")
        
        # Validate repair items table
        if body.get('repairItems'):
            validation_results['field_count'] += 1
            if not _validate_table_data(body['repairItems']):
                validation_results['warnings'].append("Tabela de repara√ß√µes com formato inv√°lido")
        
        # Validate dates
        date_fields = ['pickupDate', 'returnDate', 'inspectionDate']
        for field in date_fields:
            if body.get(field):
                formatted = _format_date(body[field])
                if formatted == str(body[field]):
                    validation_results['warnings'].append(f"Data com formato n√£o reconhecido: {field}")
        
        # Count non-empty fields
        for key, value in body.items():
            if value and key not in image_fields:
                validation_results['field_count'] += 1
        
        return validation_results
        
    except Exception as e:
        logging.error(f"Error validating damage report: {e}")
        return {
            'ok': False,
            'errors': [str(e)],
            'warnings': [],
            'field_count': 0
        }

@app.post("/api/damage-reports/preview-pdf")
async def preview_damage_report_pdf(request: Request):
    """Preview do Damage Report em PDF - Usa template mapeado com coordenadas"""
    require_auth(request)
    
    try:
        from starlette.responses import Response
        
        # Receber dados do formul√°rio
        body = await request.json()
        
        # LOG: Ver o que o frontend envia
        logging.info("=" * 80)
        logging.info("üîç PREVIEW PDF - Dados recebidos do frontend:")
        
        # LOG: Diagrama do ve√≠culo
        vehicle_diagram = body.get('vehicleDiagram', '')
        logging.info("üñºÔ∏è " + "="*80)
        logging.info("üñºÔ∏è VERIFICANDO VEHICLE DIAGRAM...")
        logging.info(f"   body.get('vehicleDiagram'): {type(vehicle_diagram).__name__}")
        if vehicle_diagram:
            has_prefix = ',' in vehicle_diagram
            data_part = vehicle_diagram.split(',')[1] if has_prefix else vehicle_diagram
            logging.info(f"   ‚úÖ vehicleDiagram: RECEBIDO! {len(vehicle_diagram)} chars total")
            logging.info(f"      Prefix: {'‚úÖ Sim (data:image/...)' if has_prefix else '‚ùå N√£o'}")
            logging.info(f"      Base64 data: {len(data_part)} chars")
            logging.info(f"      Preview: {vehicle_diagram[:70]}...")
            logging.info(f"   ‚û°Ô∏è Ser√° adicionado a report_data['vehicle_diagram']")
        else:
            logging.error(f"   ‚ùå‚ùå‚ùå vehicleDiagram: VAZIO OU N√ÉO RECEBIDO!")
            logging.error(f"   body keys: {list(body.keys())}")
            logging.error(f"   Procurando varia√ß√µes...")
            for key in body.keys():
                if 'diagram' in key.lower() or 'vehicle' in key.lower():
                    logging.error(f"      Encontrado: {key} = {len(str(body[key]))} chars")
        logging.info("üñºÔ∏è " + "="*80)
        
        # LOG: Fotos de danos
        photos_count = 0
        for i in range(1, 10):
            photo = body.get(f'damagePhoto{i}', '')
            if photo:
                photos_count += 1
                has_prefix = ',' in photo
                data_part = photo.split(',')[1] if has_prefix else photo
                logging.info(f"   damagePhoto{i}: {len(photo)} chars")
                logging.info(f"      Prefix: {'‚úÖ' if has_prefix else '‚ùå'} | Base64: {len(data_part)} chars | Preview: {photo[:50]}...")
        logging.info(f"   üì∏ Total fotos: {photos_count}")
        logging.info(f"   repairItems: {len(body.get('repairItems', []))} items")
        logging.info(f"   totalRepairCost: {body.get('totalRepairCost', 'N/A')} ‚Ç¨")
        damage_keys = [k for k in body.keys() if k.startswith('damage_')]
        logging.info(f"   Damages (damage_X): {damage_keys}")
        for dk in sorted(damage_keys):
            val = body.get(dk, '')
            logging.info(f"      {dk}: '{val[:50]}...' ({len(val)} chars)" if len(val) > 50 else f"      {dk}: '{val}'")
        logging.info("=" * 80)
        
        # Dividir campos combinados
        # C√≥digo Postal / Cidade
        postal_code_city = body.get('postalCodeCity', '')
        if postal_code_city and ' / ' in postal_code_city:
            parts = postal_code_city.split(' / ', 1)
            postal_code = parts[0].strip()
            city = parts[1].strip()
        else:
            postal_code = body.get('postalCode', postal_code_city)
            city = body.get('city', '')
        
        # Marca / Modelo
        brand_model = body.get('vehicleBrandModel', '')
        if brand_model and ' / ' in brand_model:
            parts = brand_model.split(' / ', 1)
            vehicle_brand = parts[0].strip()
            vehicle_model = parts[1].strip()
        else:
            vehicle_brand = body.get('vehicleBrand', '')
            vehicle_model = body.get('vehicleModel', brand_model)
        
        # Mapear campos do frontend para IDs do mapeador (usando os mesmos nomes camelCase)
        report_data = {
            'dr_number': body.get('drNumber', ''),
            'ra_number': body.get('raNumber', ''),  # ‚úÖ RA Number
            'contractNumber': body.get('contractNumber', ''),
            'date': body.get('date', ''),
            'inspection_date': body.get('date', ''),  # ‚úÖ Alias para compatibilidade de mapeamento
            'clientName': body.get('clientName', ''),
            'clientEmail': body.get('clientEmail', ''),
            'clientPhone': body.get('clientPhone', ''),
            'address': body.get('address', ''),
            'postalCodeCity': postal_code_city,  # Campo combinado
            'city': city,  # Para compatibilidade com mapeamentos antigos
            'postalCode': postal_code,  # Para compatibilidade com mapeamentos antigos
            'country': body.get('country', ''),
            'vehiclePlate': body.get('vehiclePlate', ''),
            'vehicleBrandModel': brand_model,  # Campo combinado
            'vehicleBrand': vehicle_brand,  # Para compatibilidade com mapeamentos antigos
            'vehicleModel': vehicle_model,  # Para compatibilidade com mapeamentos antigos
            'vehicleColor': body.get('vehicleColor', ''),
            'vehicleKm': body.get('vehicleKm', ''),
            'pickupDate': body.get('pickupDate', ''),
            'pickupTime': body.get('pickupTime', ''),
            'pickupLocation': body.get('pickupLocation', ''),
            'returnDate': body.get('returnDate', ''),
            'returnTime': body.get('returnTime', ''),
            'returnLocation': body.get('returnLocation', ''),
            'fuel_level_pickup': body.get('fuelPickup', ''),
            'fuel_level_return': body.get('fuelReturn', ''),
            'total_repair_cost': body.get('totalCost', ''),
            'inspector_name': body.get('issuedBy', ''),  # ‚úÖ Colaborador (frontend: issuedBy)
            'issued_by': body.get('issuedBy', ''),  # ‚úÖ Alias alternativo
            # Campos avan√ßados: imagens, tabelas, assinaturas
            'damage_description': body.get('damageDescription', ''),
            'vehicle_diagram': body.get('vehicleDiagram', ''),  # Croqui base64
            'damage_diagram_data': body.get('damageDiagramData', ''),  # ‚úÖ Pins dos danos (JSON array)
            'damageDiagramData': body.get('damageDiagramData', ''),  # ‚úÖ Alias para compatibilidade
            'damage_pins': body.get('damageDiagramData', ''),  # ‚úÖ Alias adicional para os pins
            'damage_photo_1': body.get('damagePhoto1', ''),
            'damage_photo_2': body.get('damagePhoto2', ''),
            'damage_photo_3': body.get('damagePhoto3', ''),
            'damage_photo_4': body.get('damagePhoto4', ''),
            'damage_photo_5': body.get('damagePhoto5', ''),
            'damage_photo_6': body.get('damagePhoto6', ''),
            'damage_photo_7': body.get('damagePhoto7', ''),
            'damage_photo_8': body.get('damagePhoto8', ''),
            'damage_photo_9': body.get('damagePhoto9', ''),
            'repair_items': body.get('repairItems', []),  # Tabela de repara√ß√µes (JSON array)
            'signature_inspector': body.get('signatureInspector', ''),  # Assinatura base64
            'signature_client': body.get('signatureClient', '')  # Assinatura base64
        }
        
        # Adicionar descri√ß√µes de danos individuais (damage_1, damage_2, ...)
        # IMPORTANTE: Mapper tem "damage_description_line_X", mas recebemos "damage_X"
        # O alias no _fill_template_pdf_with_data faz: damage_description_line_X ‚Üí damage_X
        damage_count = 0
        for i in range(1, 16):
            damage_key = f'damage_{i}'
            if damage_key in body:
                report_data[damage_key] = body.get(damage_key, '')
                damage_count += 1
        logging.info(f"üìù Adicionados {damage_count} damage_X ao report_data")
        
        # Processar repairItems: distribuir pelos campos individuais do PDF
        repair_items = body.get('repairItems', [])
        if repair_items and isinstance(repair_items, list):
            logging.info(f"üîß Processando {len(repair_items)} repairItems:")
            for idx, item in enumerate(repair_items[:10], start=1):  # Max 10 linhas
                # Cada item tem: description, quantity, hours, price, total
                report_data[f'repair_line_{idx}'] = item.get('description', '')
                report_data[f'repair_line_{idx}_qty'] = str(item.get('quantity', ''))
                hours_val = item.get('hours', '')
                report_data[f'repair_line_{idx}_hours'] = '-' if hours_val == 0 or hours_val == '0' else str(hours_val)
                report_data[f'repair_line_{idx}_price'] = str(item.get('price', ''))
                report_data[f'repair_line_{idx}_subtotal'] = str(item.get('total', ''))
                logging.info(f"   L{idx}: {item.get('description', '')[:30]} | Qty:{item.get('quantity','')} | Hours:{item.get('hours','')} | ‚Ç¨{item.get('price','')} | Total:‚Ç¨{item.get('total','')}")
        
        # Adicionar total de repara√ß√£o (soma de todos os subtotais)
        total_repair_cost = body.get('totalRepairCost', '0.00')
        report_data['total_repair_cost'] = total_repair_cost
        logging.info(f"üí∞ Total repair cost: {total_repair_cost} ‚Ç¨")
        
        # Usar fun√ß√£o de overlay para preencher template
        pdf_data = _fill_template_pdf_with_data(report_data)
        
        import time
        timestamp = int(time.time())
        return Response(
            content=pdf_data,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"inline; filename=damage_report_preview_{timestamp}.pdf",
                "Cache-Control": "no-cache, no-store, must-revalidate, max-age=0",
                "Pragma": "no-cache",
                "Expires": "0",
                "X-Timestamp": str(timestamp)
            }
        )
    except Exception as e:
        logging.error(f"Error generating preview PDF: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/damage-reports-pdf")
async def download_original_pdf_query(request: Request, dr_number: str, preview: bool = False):
    """Download ou preview do PDF original (via query parameter)"""
    require_auth(request)
    
    try:
        logging.info(f"üîç [QUERY] Procurando PDF para DR: [{dr_number}]")
        
        with _db_lock:
            conn = _db_connect()
            try:
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        cur.execute("SELECT pdf_data, pdf_filename FROM damage_reports WHERE dr_number = %s", (dr_number,))
                        row = cur.fetchone()
                else:
                    cursor = conn.execute("SELECT pdf_data, pdf_filename FROM damage_reports WHERE dr_number = ?", (dr_number,))
                    row = cursor.fetchone()
                
                if not row or not row[0]:
                    logging.error(f"‚ùå DR {dr_number} n√£o encontrado ou sem PDF")
                    raise HTTPException(status_code=404, detail=f"PDF not found for DR {dr_number}")
                
                pdf_data = row[0]
                if isinstance(pdf_data, memoryview):
                    pdf_data = bytes(pdf_data)
                
                pdf_filename = row[1] or f"DR_{dr_number.replace('/', '_')}.pdf"
                disposition = "inline" if preview else "attachment"
                
                return Response(
                    content=pdf_data,
                    media_type="application/pdf",
                    headers={"Content-Disposition": f'{disposition}; filename="{pdf_filename}"'}
                )
            finally:
                conn.close()
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error downloading PDF: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/damage-reports/{dr_number:path}/pdf-original")
async def download_original_pdf(request: Request, dr_number: str, preview: bool = False):
    """Download ou preview do PDF original que foi feito upload"""
    require_auth(request)
    
    try:
        logging.info(f"üîç Procurando PDF para DR: [{dr_number}] (len={len(dr_number)}, repr={repr(dr_number)})")
        logging.info(f"üìç URL path: {request.url.path}")
        logging.info(f"üìç Raw path: {request.scope.get('path', '')}")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Primeiro listar todos os DRs para debug
                logging.info("üìã Listando todos os DRs na BD:")
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        cur.execute("SELECT dr_number, pdf_filename, CASE WHEN pdf_data IS NULL THEN 'NULL' ELSE 'OK' END as pdf_status FROM damage_reports")
                        all_drs = cur.fetchall()
                        for dr in all_drs:
                            logging.info(f"  - {dr[0]} | PDF: {dr[1]} | Data: {dr[2]}")
                else:
                    cursor = conn.execute("SELECT dr_number, pdf_filename, CASE WHEN pdf_data IS NULL THEN 'NULL' ELSE 'OK' END as pdf_status FROM damage_reports")
                    all_drs = cursor.fetchall()
                    for dr in all_drs:
                        logging.info(f"  - {dr[0]} | PDF: {dr[1]} | Data: {dr[2]}")
                
                # Agora buscar o DR espec√≠fico
                if conn.__class__.__module__ == 'psycopg2.extensions':
                    with conn.cursor() as cur:
                        cur.execute("SELECT pdf_data, pdf_filename FROM damage_reports WHERE dr_number = %s", (dr_number,))
                        row = cur.fetchone()
                        logging.info(f"üîç Resultado PostgreSQL: {row is not None}")
                else:
                    # SQLite
                    cursor = conn.execute("SELECT pdf_data, pdf_filename FROM damage_reports WHERE dr_number = ?", (dr_number,))
                    row = cursor.fetchone()
                    logging.info(f"üîç Resultado SQLite: {row is not None}")
                
                if not row:
                    logging.error(f"‚ùå DR {dr_number} n√£o encontrado na BD")
                    raise HTTPException(status_code=404, detail=f"DR {dr_number} not found")
                
                if not row[0]:
                    logging.error(f"‚ùå DR {dr_number} n√£o tem PDF (pdf_data √© NULL)")
                    raise HTTPException(status_code=404, detail=f"PDF not found for DR {dr_number}")
                
                pdf_data = row[0]
                pdf_filename = row[1] or f"DR_{dr_number.replace('/', '_').replace(':', '_')}.pdf"
                
                logging.info(f"‚úÖ PDF encontrado: {pdf_filename}, size: {len(pdf_data) if pdf_data else 0} bytes")
                
                # Converter bytes se necess√°rio (PostgreSQL retorna memoryview)
                if isinstance(pdf_data, memoryview):
                    pdf_data = bytes(pdf_data)
                
                # Se preview=true, mostrar inline, sen√£o download
                disposition = "inline" if preview else "attachment"
                
                return Response(
                    content=pdf_data,
                    media_type="application/pdf",
                    headers={
                        "Content-Disposition": f'{disposition}; filename="{pdf_filename}"'
                    }
                )
            finally:
                conn.close()
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error downloading original PDF: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

# Endpoint duplicado REMOVIDO - agora est√° na linha 15998 (ANTES do endpoint gen√©rico)

@app.post("/api/damage-reports/{dr_number:path}/generate-and-save-pdf")
async def generate_and_save_damage_report_pdf(request: Request, dr_number: str):
    """
    Gera o PDF do Damage Report dinamicamente e SALVA na BD
    √ötil para DRs criados manualmente que ainda n√£o t√™m PDF
    """
    require_auth(request)
    
    try:
        logging.info(f"üîß GENERATE AND SAVE PDF - DR: {dr_number}")
        
        # 1. Buscar dados do DR
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                if is_postgres:
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT * FROM damage_reports WHERE dr_number = {placeholder} AND (is_deleted = 0 OR is_deleted IS NULL)", (dr_number,))
                    row = cursor.fetchone()
                    columns = [desc[0] for desc in cursor.description]
                    cursor.close()
                else:
                    cursor = conn.execute(f"SELECT * FROM damage_reports WHERE dr_number = {placeholder} AND (is_deleted = 0 OR is_deleted IS NULL)", (dr_number,))
                    row = cursor.fetchone()
                    columns = [desc[0] for desc in cursor.description]
                
                if not row:
                    logging.error(f"‚ùå DR '{dr_number}' not found")
                    return JSONResponse({"ok": False, "error": "Damage Report not found"}, status_code=404)
                
                report = dict(zip(columns, row))
            finally:
                conn.close()
        
        # Formatar datas para dd-mm-yyyy (remover hora se existir)
        def format_date(date_str):
            if not date_str:
                return ''
            try:
                from datetime import datetime
                # Se j√° √© objeto datetime
                if isinstance(date_str, datetime):
                    return date_str.strftime('%d-%m-%Y')
                # Se √© string
                date_str = str(date_str)
                # Remove hora se existir (ex: "2025-11-06 00:00:00" ‚Üí "2025-11-06")
                # Ou "2025-11-06T00:00:00" ‚Üí "2025-11-06"
                date_only = date_str.split(' ')[0].split('T')[0]
                # Converte de yyyy-mm-dd para dd-mm-yyyy
                dt = datetime.strptime(date_only, '%Y-%m-%d')
                return dt.strftime('%d-%m-%Y')
            except:
                return str(date_str)  # Retorna original se falhar
        
        # 2. Mapear dados para gera√ß√£o
        report_data = {
            'dr_number': report.get('dr_number', ''),
            'ra_number': report.get('ra_number', ''),
            'contractNumber': report.get('contract_number', ''),
            'date': format_date(report.get('date', '')),
            'created_at': format_date(report.get('created_at', '')),
            'inspection_date': format_date(report.get('date', '')),
            'clientName': report.get('client_name', ''),
            'clientEmail': report.get('client_email', ''),
            'clientPhone': report.get('client_phone', ''),
            'address': report.get('client_address', ''),
            'city': report.get('client_city', ''),
            'postalCode': report.get('client_postal_code', ''),
            'country': report.get('client_country', ''),
            'customer_postal': ' / '.join(filter(None, [report.get('client_postal_code', ''), report.get('client_city', '')])),
            'customer_city': report.get('client_city', ''),
            'vehiclePlate': report.get('vehicle_plate', ''),
            'vehicleBrand': report.get('vehicle_brand', ''),
            'vehicleModel': report.get('vehicle_model', ''),
            'vehicleBrandModel': ' / '.join(filter(None, [report.get('vehicle_brand', ''), report.get('vehicle_model', '')])),
            'vehicleKm': report.get('mileage', ''),
            'pickupDate': format_date(report.get('pickup_date', '')),
            'pickup_date': format_date(report.get('pickup_date', '')),
            'pickupTime': report.get('pickup_time', ''),
            'pickupLocation': report.get('pickup_location', ''),
            'returnDate': format_date(report.get('return_date', '')),
            'return_date': format_date(report.get('return_date', '')),
            'returnTime': report.get('return_time', ''),
            'returnLocation': report.get('return_location', ''),
            'fuel_level_pickup': report.get('fuel_level', ''),
            'fuel_level_return': report.get('fuel_level', ''),
            'total_repair_cost': report.get('total_amount', ''),
            'totalRepairCost': report.get('total_amount', ''),
            'total_amount': report.get('total_amount', ''),
            'inspector_name': report.get('issued_by', ''),
            'issued_by': report.get('issued_by', ''),
            'inspection_date': format_date(report.get('date', '')),
            'damage_diagram_data': report.get('damage_diagram_data', ''),
            'damageDiagramData': report.get('damage_diagram_data', '')
        }
        
        # ‚úÖ EXTRAIR DESCRI√á√ïES INDIVIDUAIS DOS DANOS (damage_1, damage_2, ...)
        import json
        damages_json = report.get('damage_diagram_data', '')
        if damages_json:
            try:
                damages = json.loads(damages_json)
                for damage in damages:
                    num = damage.get('number')
                    desc = damage.get('description', '')
                    if num:
                        report_data[f'damage_{num}'] = desc
                logging.info(f"‚úÖ Extra√≠das {len(damages)} descri√ß√µes de danos")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear damage_diagram_data")
        
        # ‚úÖ EXTRAIR FOTOS INDIVIDUAIS (damagePhoto1, damagePhoto2, ...)
        images_json = report.get('damage_images', '')
        if images_json:
            try:
                images = json.loads(images_json)
                for idx, image in enumerate(images):
                    photo_data = image.get('data', '')
                    report_data[f'damagePhoto{idx + 1}'] = photo_data
                    report_data[f'damage_photo_{idx + 1}'] = photo_data
                logging.info(f"‚úÖ Extra√≠das {len(images)} fotos")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear damage_images")
        
        # ‚úÖ EXTRAIR ITENS DE REPARA√á√ÉO (repair_line_1, repair_line_2, ...)
        repair_json = report.get('repair_items', '')
        total_calculated = 0.0
        if repair_json:
            try:
                repair_items = json.loads(repair_json)
                for idx, item in enumerate(repair_items):
                    line_num = idx + 1
                    report_data[f'repair_line_{line_num}'] = item.get('description', '')
                    report_data[f'repair_line_{line_num}_qty'] = str(item.get('quantity', ''))
                    hours_val = item.get('hours', '')
                    report_data[f'repair_line_{line_num}_hours'] = '-' if hours_val == 0 or hours_val == '0' else str(hours_val)
                    report_data[f'repair_line_{line_num}_price'] = str(item.get('price', ''))
                    report_data[f'repair_line_{line_num}_subtotal'] = str(item.get('total', ''))
                    # Somar ao total
                    total_calculated += float(item.get('total', 0))
                logging.info(f"‚úÖ Extra√≠dos {len(repair_items)} itens de repara√ß√£o")
                logging.info(f"üí∞ Total calculado: {total_calculated:.2f} ‚Ç¨")
            except:
                logging.warning("‚ö†Ô∏è Erro ao parsear repair_items")
        
        # ‚úÖ SOBRESCREVER total_repair_cost com valor CALCULADO (igual ao preview)
        if total_calculated > 0:
            report_data['total_repair_cost'] = f"{total_calculated:.2f}"
            report_data['totalRepairCost'] = f"{total_calculated:.2f}"
            report_data['total_amount'] = f"{total_calculated:.2f}"
            logging.info(f"‚úÖ Total repair cost SOBRESCRITO: {total_calculated:.2f} ‚Ç¨")
        
        # ‚úÖ ADICIONAR CROQUI COM PINS
        vehicle_diagram_blob = report.get('vehicle_damage_image')
        if vehicle_diagram_blob:
            import base64
            try:
                diagram_base64 = base64.b64encode(vehicle_diagram_blob).decode('utf-8')
                report_data['vehicle_diagram'] = f'data:image/png;base64,{diagram_base64}'
                logging.info("‚úÖ Croqui com pins adicionado")
            except:
                logging.warning("‚ö†Ô∏è Erro ao converter vehicle_damage_image")
        
        # 3. Gerar PDF
        logging.info(f"üîß Calling _fill_template_pdf_with_data...")
        pdf_data = _fill_template_pdf_with_data(report_data)
        logging.info(f"‚úÖ PDF generated! Size: {len(pdf_data)} bytes")
        
        # 4. SALVAR NA BD
        filename = f"{dr_number.replace('/', '_').replace(':', '_')}.pdf"
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                if is_postgres:
                    cursor = conn.cursor()
                    cursor.execute(f"""
                        UPDATE damage_reports 
                        SET pdf_data = {placeholder}, 
                            pdf_filename = {placeholder},
                            updated_at = CURRENT_TIMESTAMP
                        WHERE dr_number = {placeholder}
                    """, (pdf_data, filename, dr_number))
                    conn.commit()
                    cursor.close()
                else:
                    conn.execute(f"""
                        UPDATE damage_reports 
                        SET pdf_data = {placeholder}, 
                            pdf_filename = {placeholder},
                            updated_at = CURRENT_TIMESTAMP
                        WHERE dr_number = {placeholder}
                    """, (pdf_data, filename, dr_number))
                    conn.commit()
                
                logging.info(f"‚úÖ PDF saved to database! Filename: {filename}")
            finally:
                conn.close()
        
        return JSONResponse({
            "ok": True, 
            "message": f"PDF generated and saved successfully",
            "filename": filename,
            "size": len(pdf_data)
        })
        
    except Exception as e:
        logging.error(f"‚ùå Error generating and saving PDF: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# VEHICLE DAMAGE AI DETECTION (FREE - No API costs!)
# ============================================================

@app.post("/api/vehicle/detect-damage")
async def detect_vehicle_damage(request: Request, file: UploadFile = File(...)):
    """
    Detect vehicle damage using FREE AI model (Hugging Face)
    NO API COSTS - Model runs locally
    
    Returns:
        - has_damage: bool
        - damage_type: str (GLASS SHATTER, DENT, LAMP BROKEN, SCRATCH, CRACK)
        - confidence: float (0-1)
        - verdict: str
    """
    require_auth(request)
    
    try:
        # Read image bytes
        image_bytes = await file.read()
        
        # Import AI module
        import vehicle_damage_ai
        
        # Analyze image (FREE!)
        result = vehicle_damage_ai.analyze_vehicle_damage(image_bytes)
        
        return JSONResponse(result)
    
    except Exception as e:
        logging.error(f"Error detecting damage: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

@app.on_event("startup")
async def load_ai_models():
    """Load AI models at startup"""
    try:
        import vehicle_damage_ai
        vehicle_damage_ai.load_damage_detection_model()
        logging.info("‚úÖ AI models loaded at startup")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Could not load AI models: {e}")

# ============================================================
# VEHICLE INSPECTIONS - Check-in/Check-out System
# ============================================================

@app.get("/vehicle-inspection", response_class=HTMLResponse)
async def vehicle_inspection_page(request: Request):
    """Vehicle inspection page with real-time camera"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    return templates.TemplateResponse("vehicle_inspection.html", {
        "request": request
    })

@app.get("/vehicle-inspections", response_class=HTMLResponse)
async def vehicle_inspections_list(request: Request):
    """List all vehicle inspections"""
    try:
        require_auth(request)
    except HTTPException:
        return RedirectResponse(url="/login", status_code=HTTP_303_SEE_OTHER)
    
    # TODO: Create list page
    return templates.TemplateResponse("vehicle_inspections_list.html", {
        "request": request
    })

@app.post("/api/vehicle-inspections/create")
async def create_vehicle_inspection(request: Request):
    """
    Create new vehicle inspection with photos and AI analysis
    Saves to database with all photos and results
    """
    require_auth(request)
    
    try:
        form = await request.form()
        
        # Generate inspection number
        import datetime
        now = datetime.datetime.now()
        inspection_number = f"VI-{now.strftime('%Y%m%d')}-{now.strftime('%H%M%S')}"
        
        # Extract vehicle info
        inspection_type = form.get('inspection_type', 'check_in')
        vehicle_plate = form.get('vehicle_plate', '').strip()
        
        if not vehicle_plate:
            return JSONResponse({"ok": False, "error": "Vehicle plate is required"}, status_code=400)
        
        # Parse AI results
        import json
        ai_results = json.loads(form.get('ai_results', '{}'))
        
        # Count damages
        damage_count = sum(1 for r in ai_results.values() if r.get('ok') and r.get('has_damage'))
        has_damage = damage_count > 0
        
        # Calculate average confidence
        confidences = [r.get('confidence_percent', 0) for r in ai_results.values() if r.get('ok')]
        ai_confidence_avg = sum(confidences) / len(confidences) if confidences else 0
        
        # Collect damage types
        damage_types = [r.get('damage_type') for r in ai_results.values() if r.get('ok') and r.get('has_damage') and r.get('damage_type')]
        ai_damages_detected = json.dumps(damage_types) if damage_types else None
        
        # Determine severity
        max_confidence = max(confidences) if confidences else 0
        if not has_damage:
            damage_severity = 'none'
        elif max_confidence > 80:
            damage_severity = 'severe'
        elif max_confidence > 60:
            damage_severity = 'moderate'
        else:
            damage_severity = 'minor'
        
        # Save to database
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    # PostgreSQL
                    cursor = conn.cursor()
                    
                    # Insert inspection
                    cursor.execute("""
                        INSERT INTO vehicle_inspections 
                        (inspection_number, inspection_type, vehicle_plate, vehicle_brand, vehicle_model,
                         contract_number, customer_name, customer_email, customer_phone,
                         inspector_name, inspector_notes, has_damage, damage_count, damage_severity,
                         ai_analysis_complete, ai_confidence_avg, ai_damages_detected,
                         odometer_reading, fuel_level, status, photo_count)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (
                        inspection_number,
                        inspection_type,
                        vehicle_plate,
                        form.get('vehicle_brand', '').strip() or None,
                        form.get('vehicle_model', '').strip() or None,
                        form.get('contract_number', '').strip() or None,
                        form.get('customer_name', '').strip() or None,
                        form.get('customer_email', '').strip() or None,
                        form.get('customer_phone', '').strip() or None,
                        form.get('inspector_name', '').strip(),
                        form.get('inspector_notes', '').strip() or None,
                        has_damage,
                        damage_count,
                        damage_severity,
                        True,  # ai_analysis_complete
                        round(ai_confidence_avg, 2),
                        ai_damages_detected,
                        int(form.get('odometer_reading', 0)) if form.get('odometer_reading') else None,
                        form.get('fuel_level', '') or None,
                        'completed',
                        len([k for k in form.keys() if k.startswith('photo_')])
                    ))
                    
                    inspection_id = cursor.fetchone()[0]
                    
                    # Save photos
                    photo_types = ['front', 'back', 'left', 'right', 'interior', 'odometer']
                    for idx, photo_type in enumerate(photo_types):
                        photo_file = form.get(f'photo_{photo_type}')
                        if photo_file:
                            photo_bytes = await photo_file.read()
                            
                            # Get AI result for this photo
                            ai_result = ai_results.get(photo_type, {})
                            
                            cursor.execute("""
                                INSERT INTO inspection_photos
                                (inspection_id, photo_type, photo_order, image_data, image_filename,
                                 image_size, image_format, ai_analyzed, ai_has_damage, ai_damage_type,
                                 ai_confidence, ai_result)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                inspection_id,
                                photo_type,
                                idx + 1,
                                photo_bytes,
                                f"{photo_type}.jpg",
                                len(photo_bytes),
                                'jpg',
                                ai_result.get('ok', False),
                                ai_result.get('has_damage', False),
                                ai_result.get('damage_type'),
                                ai_result.get('confidence'),
                                json.dumps(ai_result)
                            ))
                    
                    conn.commit()
                    cursor.close()
                    
                    logging.info(f"‚úÖ Vehicle inspection saved: {inspection_number}")
                    
                    return JSONResponse({
                        "ok": True,
                        "inspection_number": inspection_number,
                        "inspection_id": inspection_id,
                        "has_damage": has_damage,
                        "damage_count": damage_count
                    })
                
                else:
                    # SQLite - create tables if needed
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS vehicle_inspections (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            inspection_number TEXT UNIQUE,
                            inspection_type TEXT,
                            vehicle_plate TEXT,
                            vehicle_brand TEXT,
                            vehicle_model TEXT,
                            contract_number TEXT,
                            customer_name TEXT,
                            customer_email TEXT,
                            customer_phone TEXT,
                            inspector_name TEXT,
                            inspector_notes TEXT,
                            has_damage INTEGER DEFAULT 0,
                            damage_count INTEGER DEFAULT 0,
                            damage_severity TEXT,
                            ai_analysis_complete INTEGER DEFAULT 0,
                            ai_confidence_avg REAL,
                            ai_damages_detected TEXT,
                            odometer_reading INTEGER,
                            fuel_level TEXT,
                            status TEXT DEFAULT 'draft',
                            photo_count INTEGER DEFAULT 0,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS inspection_photos (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            inspection_id INTEGER,
                            photo_type TEXT,
                            photo_order INTEGER,
                            image_data BLOB,
                            image_filename TEXT,
                            image_size INTEGER,
                            image_format TEXT,
                            ai_analyzed INTEGER DEFAULT 0,
                            ai_has_damage INTEGER DEFAULT 0,
                            ai_damage_type TEXT,
                            ai_confidence REAL,
                            ai_result TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY (inspection_id) REFERENCES vehicle_inspections(id)
                        )
                    """)
                    
                    # Insert inspection
                    cursor = conn.execute("""
                        INSERT INTO vehicle_inspections 
                        (inspection_number, inspection_type, vehicle_plate, vehicle_brand, vehicle_model,
                         contract_number, customer_name, customer_email, customer_phone,
                         inspector_name, inspector_notes, has_damage, damage_count, damage_severity,
                         ai_analysis_complete, ai_confidence_avg, ai_damages_detected,
                         odometer_reading, fuel_level, status, photo_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        inspection_number,
                        inspection_type,
                        vehicle_plate,
                        form.get('vehicle_brand', '').strip() or None,
                        form.get('vehicle_model', '').strip() or None,
                        form.get('contract_number', '').strip() or None,
                        form.get('customer_name', '').strip() or None,
                        form.get('customer_email', '').strip() or None,
                        form.get('customer_phone', '').strip() or None,
                        form.get('inspector_name', '').strip(),
                        form.get('inspector_notes', '').strip() or None,
                        1 if has_damage else 0,
                        damage_count,
                        damage_severity,
                        1,  # ai_analysis_complete
                        round(ai_confidence_avg, 2),
                        ai_damages_detected,
                        int(form.get('odometer_reading', 0)) if form.get('odometer_reading') else None,
                        form.get('fuel_level', '') or None,
                        'completed',
                        len([k for k in form.keys() if k.startswith('photo_')])
                    ))
                    
                    inspection_id = cursor.lastrowid
                    
                    # Save photos
                    photo_types = ['front', 'back', 'left', 'right', 'interior', 'odometer']
                    for idx, photo_type in enumerate(photo_types):
                        photo_file = form.get(f'photo_{photo_type}')
                        if photo_file:
                            photo_bytes = await photo_file.read()
                            ai_result = ai_results.get(photo_type, {})
                            
                            conn.execute("""
                                INSERT INTO inspection_photos
                                (inspection_id, photo_type, photo_order, image_data, image_filename,
                                 image_size, image_format, ai_analyzed, ai_has_damage, ai_damage_type,
                                 ai_confidence, ai_result)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                inspection_id,
                                photo_type,
                                idx + 1,
                                photo_bytes,
                                f"{photo_type}.jpg",
                                len(photo_bytes),
                                'jpg',
                                1 if ai_result.get('ok', False) else 0,
                                1 if ai_result.get('has_damage', False) else 0,
                                ai_result.get('damage_type'),
                                ai_result.get('confidence'),
                                json.dumps(ai_result)
                            ))
                    
                    conn.commit()
                    
                    logging.info(f"‚úÖ Vehicle inspection saved (SQLite): {inspection_number}")
                    
                    return JSONResponse({
                        "ok": True,
                        "inspection_number": inspection_number,
                        "inspection_id": inspection_id,
                        "has_damage": has_damage,
                        "damage_count": damage_count
                    })
            
            finally:
                conn.close()
    
    except Exception as e:
        logging.error(f"Error creating inspection: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

# ============================================================
# RENTAL AGREEMENT - Templates and Coordinates
# ============================================================

@app.post("/api/rental-agreements/upload-template")
async def upload_rental_agreement_template(request: Request, file: UploadFile = File(...)):
    """Upload do template de Rental Agreement (PDF)"""
    require_auth(request)
    
    try:
        from PyPDF2 import PdfReader
        from io import BytesIO
        from datetime import datetime
        
        contents = await file.read()
        filename = file.filename
        
        # Verificar se √© PDF v√°lido e contar p√°ginas
        try:
            pdf_reader = PdfReader(BytesIO(contents))
            num_pages = len(pdf_reader.pages)
        except Exception as e:
            return {"ok": False, "error": f"PDF inv√°lido: {str(e)}"}
        
        username = request.session.get('username', 'admin')
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                # Obter pr√≥xima vers√£o
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("SELECT COALESCE(MAX(version), 0) + 1 FROM rental_agreement_templates")
                        version = cur.fetchone()[0]
                else:
                    cursor = conn.execute("SELECT COALESCE(MAX(version), 0) + 1 FROM rental_agreement_templates")
                    version = cursor.fetchone()[0]
                
                # Desativar templates antigos
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("UPDATE rental_agreement_templates SET is_active = 0")
                else:
                    conn.execute("UPDATE rental_agreement_templates SET is_active = 0")
                
                # Inserir novo template
                notes = f"Rental Agreement v{version}"
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO rental_agreement_templates 
                            (version, filename, file_data, num_pages, uploaded_by, uploaded_at, is_active, notes)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        """, (version, filename, contents, num_pages, username, datetime.now(), 1, notes))
                else:
                    conn.execute("""
                        INSERT INTO rental_agreement_templates 
                        (version, filename, file_data, num_pages, uploaded_by, uploaded_at, is_active, notes)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (version, filename, contents, num_pages, username, datetime.now().isoformat(), 1, notes))
                
                conn.commit()
                
                logging.info(f"‚úÖ RA Template uploaded: v{version}, {num_pages} p√°ginas")
                
                return {
                    "ok": True,
                    "message": "Template RA carregado com sucesso",
                    "version": version,
                    "num_pages": num_pages,
                    "filename": filename
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error uploading RA template: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return {"ok": False, "error": str(e)}

@app.get("/api/rental-agreements/get-active-template")
async def get_active_rental_agreement_template(request: Request):
    """Obter o template PDF ativo do RA para o mapeador"""
    require_auth(request)
    
    from starlette.responses import Response
    
    try:
        logging.info("üì• [RA-TEMPLATE] Starting to load active template...")
        
        # Garantir que a tabela existe
        try:
            logging.info("üì• [RA-TEMPLATE] Ensuring tables exist...")
            _ensure_rental_agreement_tables()
            logging.info("‚úÖ [RA-TEMPLATE] Tables ensured")
        except Exception as table_error:
            logging.error(f"‚ùå [RA-TEMPLATE] Failed to ensure RA tables: {table_error}")
            import traceback
            logging.error(traceback.format_exc())
        
        # Buscar template ativo da BD
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                logging.info(f"üì• [RA-TEMPLATE] DB type: {'PostgreSQL' if is_postgres else 'SQLite'}")
                
                if is_postgres:
                    try:
                        cursor = conn.cursor()
                        logging.info("üì• [RA-TEMPLATE] Executing PostgreSQL query...")
                        cursor.execute("""
                            SELECT file_data, filename 
                            FROM rental_agreement_templates 
                            WHERE is_active = 1 
                            ORDER BY version DESC LIMIT 1
                        """)
                        row = cursor.fetchone()
                        cursor.close()
                        logging.info(f"üì• [RA-TEMPLATE] Query result: {row is not None}")
                    except Exception as pg_error:
                        logging.error(f"‚ùå [RA-TEMPLATE] PostgreSQL query error: {pg_error}")
                        import traceback
                        logging.error(traceback.format_exc())
                        raise
                else:
                    try:
                        logging.info("üì• [RA-TEMPLATE] Executing SQLite query...")
                        cursor = conn.execute("""
                            SELECT file_data, filename 
                            FROM rental_agreement_templates 
                            WHERE is_active = 1 
                            ORDER BY version DESC LIMIT 1
                        """)
                        row = cursor.fetchone()
                        logging.info(f"üì• [RA-TEMPLATE] Query result: {row is not None}")
                    except Exception as sqlite_error:
                        logging.error(f"‚ùå [RA-TEMPLATE] SQLite query error: {sqlite_error}")
                        import traceback
                        logging.error(traceback.format_exc())
                        raise
                
                if not row or not row[0]:
                    logging.warning("‚ö†Ô∏è No active RA template found in database")
                    return Response(
                        content=b'{"error": "No active RA template found. Please upload a template first in Admin Settings."}',
                        media_type="application/json",
                        status_code=404
                    )
                
                pdf_data = row[0]
                filename = row[1] if row[1] else 'rental_agreement_template.pdf'
                
                # PostgreSQL retorna BYTEA como memoryview - converter para bytes
                if isinstance(pdf_data, memoryview):
                    pdf_data = bytes(pdf_data)
                
                logging.info(f"üìÑ Serving RA template from DB: {filename} ({len(pdf_data)} bytes)")
                return Response(
                    content=pdf_data,
                    media_type="application/pdf",
                    headers={
                        'Content-Disposition': f'inline; filename="{filename}"',
                        'Cache-Control': 'no-cache'
                    }
                )
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error getting active RA template: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return Response(
            content=f'{{"error": "Server error loading RA template. Check if table exists and template is uploaded."}}'.encode(),
            media_type="application/json",
            status_code=500
        )

@app.post("/api/rental-agreements/force-create-tables")
async def force_create_ra_tables(request: Request):
    """Force: Criar tabelas de RA manualmente"""
    require_auth(request)
    
    try:
        logging.info("üîß Force creating RA tables...")
        _ensure_rental_agreement_tables()
        logging.info("‚úÖ RA tables created successfully")
        return {"ok": True, "message": "RA tables created"}
    except Exception as e:
        logging.error(f"‚ùå Error force creating RA tables: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return {"ok": False, "error": str(e)}

@app.get("/api/rental-agreements/debug-status")
async def debug_rental_agreement_status(request: Request):
    """Debug: Verificar estado das tabelas e templates de RA"""
    require_auth(request)
    
    try:
        status = {
            "tables_exist": False,
            "template_count": 0,
            "active_template": None,
            "error": None,
            "db_type": None
        }
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                status["db_type"] = "PostgreSQL" if is_postgres else "SQLite"
                
                # Verificar se tabela existe
                if is_postgres:
                    cursor = conn.cursor()
                    logging.info(f"[DEBUG] Checking RA tables in PostgreSQL...")
                    cursor.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = 'rental_agreement_templates'
                        )
                    """)
                    status["tables_exist"] = cursor.fetchone()[0]
                    
                    if status["tables_exist"]:
                        # Contar templates
                        cursor.execute("SELECT COUNT(*) FROM rental_agreement_templates")
                        status["template_count"] = cursor.fetchone()[0]
                        
                        # Ver template ativo
                        cursor.execute("""
                            SELECT version, filename, num_pages, uploaded_at, is_active
                            FROM rental_agreement_templates
                            WHERE is_active = 1
                            ORDER BY version DESC LIMIT 1
                        """)
                        row = cursor.fetchone()
                        if row:
                            status["active_template"] = {
                                "version": row[0],
                                "filename": row[1],
                                "num_pages": row[2],
                                "uploaded_at": str(row[3]),
                                "is_active": row[4]
                            }
                    cursor.close()
                else:
                    # SQLite
                    cursor = conn.execute("""
                        SELECT name FROM sqlite_master 
                        WHERE type='table' AND name='rental_agreement_templates'
                    """)
                    status["tables_exist"] = cursor.fetchone() is not None
                    
                    if status["tables_exist"]:
                        cursor = conn.execute("SELECT COUNT(*) FROM rental_agreement_templates")
                        status["template_count"] = cursor.fetchone()[0]
                        
                        cursor = conn.execute("""
                            SELECT version, filename, num_pages, uploaded_at, is_active
                            FROM rental_agreement_templates
                            WHERE is_active = 1
                            ORDER BY version DESC LIMIT 1
                        """)
                        row = cursor.fetchone()
                        if row:
                            status["active_template"] = {
                                "version": row[0],
                                "filename": row[1],
                                "num_pages": row[2],
                                "uploaded_at": str(row[3]),
                                "is_active": row[4]
                            }
            finally:
                conn.close()
        
        return status
    except Exception as e:
        logging.error(f"Debug error: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return {
            "tables_exist": False,
            "template_count": 0,
            "active_template": None,
            "error": str(e)
        }

@app.get("/api/rental-agreements/debug-coords")
async def debug_ra_coordinates(request: Request):
    """DEBUG: Verificar coordenadas no banco"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            is_postgres = hasattr(conn, 'cursor')
            
            if is_postgres:
                with conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) FROM rental_agreement_coordinates")
                    count = cur.fetchone()[0]
                    
                    cur.execute("""
                        SELECT field_id, x, y, width, height, page 
                        FROM rental_agreement_coordinates 
                        ORDER BY field_id
                    """)
                    coords = cur.fetchall()
            else:
                cursor = conn.execute("SELECT COUNT(*) FROM rental_agreement_coordinates")
                count = cursor.fetchone()[0]
                
                cursor = conn.execute("""
                    SELECT field_id, x, y, width, height, page 
                    FROM rental_agreement_coordinates 
                    ORDER BY field_id
                """)
                coords = cursor.fetchall()
            
            result = {
                "ok": True,
                "total": count,
                "coordinates": [
                    {
                        "field_id": row[0],
                        "x": row[1],
                        "y": row[2],
                        "width": row[3],
                        "height": row[4],
                        "page": row[5]
                    }
                    for row in coords
                ]
            }
            
            conn.close()
            return result
    except Exception as e:
        import traceback
        return {
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }

@app.get("/api/rental-agreements/list-fields")
async def list_rental_agreement_fields(request: Request):
    """üîç DEBUG: Listar todos os field_ids mapeados no RA"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT field_id, x, y, width, height, page
                            FROM rental_agreement_coordinates
                            ORDER BY field_id
                        """)
                        rows = cur.fetchall()
                else:
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page
                        FROM rental_agreement_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                fields = []
                for row in rows:
                    fields.append({
                        'field_id': row[0],
                        'x': row[1],
                        'y': row[2],
                        'width': row[3],
                        'height': row[4],
                        'page': row[5] if row[5] else 1
                    })
                
                logging.info(f"üìä LIST RA Fields: {len(fields)} total")
                
                # Verificar se pickupDate e returnDate existem
                has_pickup = any(f['field_id'] == 'pickupDate' for f in fields)
                has_return = any(f['field_id'] == 'returnDate' for f in fields)
                
                return {
                    "ok": True, 
                    "total": len(fields),
                    "fields": fields,
                    "has_pickupDate": has_pickup,
                    "has_returnDate": has_return
                }
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error listing RA fields: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/api/rental-agreements/get-coordinates")
async def get_rental_agreement_coordinates(request: Request):
    """Obter coordenadas dos campos do RA"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT field_id, x, y, width, height, page, field_type, template_version
                            FROM rental_agreement_coordinates
                            ORDER BY field_id
                        """)
                        rows = cur.fetchall()
                else:
                    cursor = conn.execute("""
                        SELECT field_id, x, y, width, height, page, field_type, template_version
                        FROM rental_agreement_coordinates
                        ORDER BY field_id
                    """)
                    rows = cursor.fetchall()
                
                coordinates = {}
                template_version = 1
                for row in rows:
                    field_id = row[0]
                    coordinates[field_id] = {
                        'x': row[1],
                        'y': row[2],
                        'width': row[3],
                        'height': row[4],
                        'page': row[5] if row[5] else 1
                    }
                    template_version = row[7] if row[7] else 1
                
                logging.info(f"üìä GET RA Coordinates: {len(coordinates)} fields")
                return {"ok": True, "coordinates": coordinates, "version": template_version}
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error getting RA coordinates: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/api/rental-agreements/save-coordinates")
async def save_rental_agreement_coordinates(request: Request):
    """Guardar coordenadas dos campos do RA"""
    require_auth(request)
    
    try:
        import json
        from datetime import datetime
        
        data = await request.json()
        coordinates = data.get('coordinates', data)
        
        username = request.session.get('username', 'system')
        
        # Obter vers√£o ativa do template
        template_version = 1
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT version FROM rental_agreement_templates 
                            WHERE is_active = 1 
                            ORDER BY version DESC LIMIT 1
                        """)
                        row = cur.fetchone()
                else:
                    cursor = conn.execute("""
                        SELECT version FROM rental_agreement_templates 
                        WHERE is_active = 1 
                        ORDER BY version DESC LIMIT 1
                    """)
                    row = cursor.fetchone()
                
                if row:
                    template_version = row[0]
            finally:
                conn.close()
        
        # Guardar coordenadas
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cursor:
                        cursor.execute("DELETE FROM rental_agreement_coordinates")
                        
                        logging.info(f"üíæ SAVE RA Coordinates: Salvando {len(coordinates)} campos")
                        
                        for field_id, coord in coordinates.items():
                            field_type = _detect_field_type(field_id)
                            
                            cursor.execute("""
                                INSERT INTO rental_agreement_coordinates 
                                (field_id, x, y, width, height, page, field_type, template_version, updated_at)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                field_id,
                                coord['x'],
                                coord['y'],
                                coord['width'],
                                coord['height'],
                                coord.get('page', 1),
                                field_type,
                                template_version,
                                datetime.now().isoformat()
                            ))
                            
                            cursor.execute("""
                                INSERT INTO rental_agreement_mapping_history 
                                (template_version, field_id, x, y, width, height, page, field_type, mapped_by, mapped_at)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                template_version,
                                field_id,
                                coord['x'],
                                coord['y'],
                                coord['width'],
                                coord['height'],
                                coord.get('page', 1),
                                field_type,
                                username,
                                datetime.now().isoformat()
                            ))
                else:
                    conn.execute("DELETE FROM rental_agreement_coordinates")
                    
                    logging.info(f"üíæ SAVE RA Coordinates: Salvando {len(coordinates)} campos")
                    
                    for field_id, coord in coordinates.items():
                        field_type = _detect_field_type(field_id)
                        
                        conn.execute("""
                            INSERT INTO rental_agreement_coordinates 
                            (field_id, x, y, width, height, page, field_type, template_version, updated_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            field_id,
                            coord['x'],
                            coord['y'],
                            coord['width'],
                            coord['height'],
                            coord.get('page', 1),
                            field_type,
                            template_version,
                            datetime.now().isoformat()
                        ))
                        
                        conn.execute("""
                            INSERT INTO rental_agreement_mapping_history 
                            (template_version, field_id, x, y, width, height, page, field_type, mapped_by, mapped_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            template_version,
                            field_id,
                            coord['x'],
                            coord['y'],
                            coord['width'],
                            coord['height'],
                            coord.get('page', 1),
                            field_type,
                            username,
                            datetime.now().isoformat()
                        ))
                
                conn.commit()
                logging.info(f"‚úÖ SAVE RA: {len(coordinates)} coordenadas guardadas na BD (vers√£o {template_version})")
            finally:
                conn.close()
        
        return {"ok": True, "count": len(coordinates), "version": template_version}
    except Exception as e:
        logging.error(f"Error saving RA coordinates: {e}")
        return {"ok": False, "error": str(e)}

# ============================================================
# EXPORT HISTORY - Way2Rentals, Abbycar, etc.
# ============================================================

@app.post("/api/export-history/save")
async def save_export_history(request: Request):
    """Salva export na database para hist√≥rico"""
    require_auth(request)
    try:
        body = await request.json()
        
        filename = body.get("filename", "")
        broker = body.get("broker", "")
        location = body.get("location", "")
        period_start = body.get("period_start")
        period_end = body.get("period_end")
        month = body.get("month", 0)
        year = body.get("year", 0)
        month_name = body.get("month_name", "")
        file_content = body.get("file_content", "")
        file_size = len(file_content)
        
        username = request.session.get("username", "unknown")
        
        with _db_lock:
            con = _db_connect()
            try:
                query = _convert_query_for_db("""
                    INSERT INTO export_history 
                    (filename, broker, location, period_start, period_end, month, year, month_name, 
                     file_content, file_size, exported_by, export_date)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
                """, con)
                
                if con.__class__.__module__ == 'psycopg2.extensions':
                    with con.cursor() as cur:
                        cur.execute(query, (filename, broker, location, period_start, period_end, month, year, 
                                           month_name, file_content, file_size, username))
                else:
                    con.execute(query, (filename, broker, location, period_start, period_end, month, year, 
                                       month_name, file_content, file_size, username))
                con.commit()
                
                export_id = con.execute("SELECT last_insert_rowid()").fetchone()[0]
                
                # Cleanup: Manter apenas √∫ltimos 12 meses
                cutoff_date = f"{year - 1}-{month:02d}-01"
                con.execute("""
                    DELETE FROM export_history 
                    WHERE export_date < ?
                """, (cutoff_date,))
                con.commit()
                
                return _no_store_json({
                    "ok": True,
                    "export_id": export_id,
                    "message": "Export saved successfully"
                })
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/export-history/list")
async def list_export_history(request: Request):
    """Lista exports salvos (√∫ltimos 12 meses)"""
    require_auth(request)
    try:
        broker = request.query_params.get("broker")
        location = request.query_params.get("location")
        year = request.query_params.get("year")
        month = request.query_params.get("month")
        
        query = """
            SELECT id, filename, broker, location, period_start, period_end, 
                   month, year, month_name, file_size, exported_by, export_date, last_downloaded
            FROM export_history
            WHERE 1=1
        """
        params = []
        
        if broker:
            query += " AND broker = ?"
            params.append(broker)
        if location:
            query += " AND location = ?"
            params.append(location)
        if year:
            query += " AND year = ?"
            params.append(int(year))
        if month:
            query += " AND month = ?"
            params.append(int(month))
        
        query += " ORDER BY year DESC, month DESC, export_date DESC"
        
        with _db_lock:
            con = _db_connect()
            try:
                rows = con.execute(query, params).fetchall()
                
                exports = []
                for row in rows:
                    exports.append({
                        "id": row[0],
                        "filename": row[1],
                        "broker": row[2],
                        "location": row[3],
                        "period_start": row[4],
                        "period_end": row[5],
                        "month": row[6],
                        "year": row[7],
                        "month_name": row[8],
                        "file_size": row[9],
                        "exported_by": row[10],
                        "export_date": row[11],
                        "last_downloaded": row[12]
                    })
                
                return _no_store_json({
                    "ok": True,
                    "exports": exports,
                    "total": len(exports)
                })
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/export-history/download/{export_id}")
async def download_export_history(request: Request, export_id: int):
    """Download de export salvo"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                row = con.execute("""
                    SELECT filename, file_content, broker
                    FROM export_history
                    WHERE id = ?
                """, (export_id,)).fetchone()
                
                if not row:
                    return _no_store_json({"ok": False, "error": "Export not found"}, 404)
                
                filename, file_content, broker = row
                
                # Update last_downloaded
                query = _convert_query_for_db("""
                    UPDATE export_history
                    SET last_downloaded = datetime('now')
                    WHERE id = ?
                """, con)
                
                if con.__class__.__module__ == 'psycopg2.extensions':
                    with con.cursor() as cur:
                        cur.execute(query, (export_id,))
                else:
                    con.execute(query, (export_id,))
                con.commit()
                
                # Return CSV file
                from starlette.responses import Response
                return Response(
                    content=file_content,
                    media_type="text/csv",
                    headers={
                        "Content-Disposition": f'attachment; filename="{filename}"'
                    }
                )
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/export-automated-prices-excel")
async def export_automated_prices_excel(request: Request):
    """Export automated prices to Excel (Abbycar format)"""
    import sys
    print("[BACKEND] ========== EXPORT AUTOMATED PRICES EXCEL REQUEST RECEIVED ==========", flush=True)
    sys.stdout.flush()
    require_auth(request)
    print("[BACKEND] Authentication passed", flush=True)
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
        from datetime import datetime
        import io
        
        print("[BACKEND] Reading request data...", flush=True)
        sys.stdout.flush()
        data = await request.json()
        print(f"[BACKEND] Data received: {len(str(data))} chars", flush=True)
        location = data.get('location', 'Unknown')
        date = data.get('date', datetime.now().strftime('%Y-%m-%d'))
        prices = data.get('prices', {})
        print(f"[BACKEND] Location: {location}, Date: {date}, Prices groups: {len(prices)}", flush=True)
        
        # Car group mapping (SIPP codes to groups)
        car_group_mapping = {
            'MDMV': 'B1',  # Mini 4 Doors Manual
            'MCMV': 'B1',  # Mini Coupe Manual (same group)
            'EDMV': 'B2',  # Economy Manual
            'NDMR': 'B2',  # Economy 4 Doors Manual (same group)
            'MDMR': 'D',   # Mini 4 Doors Manual
            'HDMV': 'D',   # Mini Elite 4 Doors Manual (same group)
            'MDAR': 'E1',  # Mini Auto
            'MDAV': 'E1',  # Mini 4 Doors Auto (same group)
            'EDAV': 'E2',  # Economy Auto
            'EDAR': 'E2',  # Economy Auto (same group)
            'CFMR': 'F',   # Compact Manual
            'DFMR': 'F',   # Compact 4 Doors Manual (same group)
            'MTMR': 'G',   # Mini Elite Manual
            'CFMV': 'J1',  # Compact Manual
            'DFMV': 'J1',  # Compact 4 Doors Manual (same group)
            'IWMR': 'J2',  # Intermediate Wagon Manual
            'IWMV': 'J2',  # Intermediate Wagon Manual (same group)
            'CFAR': 'L1',  # Compact Auto
            'CGAR': 'L1',  # Compact Auto (same group)
            'CFAV': 'L1',  # Compact Auto (same group)
            'SVMR': 'M1',  # Standard Manual
            'SVMD': 'M1',  # Standard Manual (same group)
            'SVMV': 'M1',  # Standard Manual (same group)
            'SVAD': 'M2',  # Standard Auto
            'SVAR': 'M2',  # Standard Auto (same group)
            'LVMD': 'N',   # Large Manual
            'LVMR': 'N'    # Large Manual (same group)
        }
        
        # Load template
        print("[BACKEND] Loading Abbycar template...", flush=True)
        from openpyxl import load_workbook, Workbook
        import csv
        
        # Try to load CSV first (priority), fallback to XLSX
        try:
            print("[BACKEND] Trying Abbycar.csv...", flush=True)
            # Create new workbook from CSV
            wb = Workbook()
            ws = wb.active
            
            with open('Abbycar.csv', 'r', encoding='utf-8') as csvfile:
                csv_reader = csv.reader(csvfile, delimiter=';')
                for row_idx, row in enumerate(csv_reader, start=1):
                    for col_idx, value in enumerate(row, start=1):
                        ws.cell(row_idx, col_idx).value = value
            
            print(f"[BACKEND] CSV template loaded and converted: {ws.max_row} rows, {ws.max_column} cols", flush=True)
        except FileNotFoundError:
            print("[BACKEND] Abbycar.csv not found, trying Abbycar.xlsx...", flush=True)
            try:
                wb = load_workbook('Abbycar.xlsx')
                ws = wb.active
                print(f"[BACKEND] XLSX template loaded: {ws.max_row} rows, {ws.max_column} cols", flush=True)
            except FileNotFoundError:
                print("[BACKEND] ERROR: Neither Abbycar.csv nor Abbycar.xlsx found!", flush=True)
                raise Exception("Abbycar template not found. Please ensure Abbycar.csv or Abbycar.xlsx is in the root directory.")
        
        # Determine station code based on location
        station_code = "FAO" if "faro" in location.lower() else "ABF" if "albufeira" in location.lower() else "UNK"
        print(f"[BACKEND] Station code: {station_code}", flush=True)
        
        # Template already has headers and formatting - just update data rows
        
        # Map internal group codes to primary SIPP code for display
        group_to_sipp = {
            'B1': 'MDMV',
            'B2': 'EDMV',
            'D': 'MDMR',
            'E1': 'MDAR',
            'E2': 'EDAV',
            'F': 'CFMR',
            'G': 'MTMR',
            'J1': 'CFMV',
            'J2': 'IWMR',
            'L1': 'CFAR',
            'L2': 'CGAR',
            'M1': 'SVMR',
            'M2': 'SVAD',
            'N': 'LVMD'
        }
        
        # Model examples from original Abbycar.xlsx
        sipp_to_model = {
            'MDMV': 'Peugeot 108',
            'MDMR': 'Fiat Panda',
            'MCMV': 'Citroen C1',
            'NDMR': 'Toyota Aygo',
            'EDMV': 'Opel Corsa',
            'HDMV': 'Ford Fiesta',
            'MDAR': 'Kia Picanto',
            'EDAV': 'Citroen C3/Opel Corsa',
            'MDAV': 'Toyota Aygo',
            'EDAR': 'Opel Corsa',
            'CFMR': 'Seat Arona',
            'DFMR': 'Kia Stonic',
            'MTMR': 'Fiat 500 Cabrio',
            'CFMV': 'Peugeot 2008',
            'IWMR': 'Peugeot 308 SW',
            'DFMV': 'Citroen C4',
            'IWMV': 'Opel Astra STW',
            'CFAR': 'Seat Arona',
            'CGAR': 'Citroen C3 Aircross',
            'CFAV': 'VW T-Cross',
            'SVMR': 'Dacia Jogger SL Extreme',
            'SVMD': 'Citroen Grand C4',
            'SVAD': 'Citroen Grand C4 Automatic',
            'SVMV': 'Peugeot Rifter',
            'SVAR': 'Peugeot Rifter Automatic',
            'LVMD': 'Fiat Talento',
            'LVMR': 'Opel Vivaro'
        }
        
        # Use SIPP codes in the EXACT order from original Abbycar.xlsx
        sipp_codes_order = [
            'MDMV', 'MDMR', 'MCMV', 'NDMR', 'EDMV', 'HDMV', 'MDAR', 'EDAV', 'MDAV', 'EDAR',
            'CFMR', 'DFMR', 'MTMR', 'CFMV', 'IWMR', 'DFMV', 'IWMV', 'CFAR', 'CGAR', 'CFAV',
            'SVMR', 'SVMD', 'SVAD', 'SVMV', 'SVAR', 'LVMD', 'LVMR'
        ]
        print(f"[BACKEND] Processing {len(sipp_codes_order)} SIPP codes...", flush=True)
        
        # Price calculation logic based on periods
        def calculate_price_for_day(group_prices, day):
            """
            Calculate price based on day period:
            - Days 1-7: NET fixed prices (no division)
            - Days 8+: Daily prices (price / days)
            """
            # Try both string and integer keys (frontend sends integers)
            price = group_prices.get(day) or group_prices.get(str(day), '')
            
            if not price:
                return ''
            
            price = float(price)
            
            # Days 1-7: Fixed NET prices (no division)
            if day <= 7:
                return price
            
            # Days 8+: Daily prices (divide by days)
            # 8-10 daily: use day 9 price / 9
            # 11-12 daily: use day 9 price / 9
            # 13-14 daily: use day 14 price / 14
            # 15-21 daily: use day 22 price / 22
            # 22-28 daily: use day 28 price / 28
            
            if day == 8:  # 8-10 daily
                # Use day 9 price / 9
                price_9 = group_prices.get(9) or group_prices.get('9', '')
                if price_9:
                    return float(price_9) / 9
                return price / 8
            elif day == 9:  # 11-12 daily
                # Use day 9 price / 9
                return price / 9
            elif day == 14:  # 13-14 daily
                return price / 14
            elif day == 22:  # 15-21 daily
                return price / 22
            elif day == 28:  # 22-28 daily
                return price / 28
            
            return price
        
        # Get Abbycar price adjustments
        abbycar_adjustment = _get_abbycar_adjustment()
        abbycar_low_deposit_enabled = _get_abbycar_low_deposit_enabled()
        abbycar_low_deposit_adjustment = _get_abbycar_low_deposit_adjustment()
        
        # Define Low Deposit groups (ALWAYS defined, but only filled if enabled)
        # SIPP codes: MCMV, NDMR, HDMV, MDAV, EDAR, DFMR, DFMV, IWMV, CFAV, SVMV, SVAR, LVMR
        # Map to internal groups:
        low_deposit_sipp_codes = ['MCMV', 'NDMR', 'HDMV', 'MDAV', 'EDAR', 'DFMR', 'DFMV', 'IWMV', 'CFAV', 'SVMV', 'SVAR', 'LVMR']
        low_deposit_groups = list(set([car_group_mapping[sipp] for sipp in low_deposit_sipp_codes if sipp in car_group_mapping]))
        # Result: ['B1', 'B2', 'D', 'E1', 'E2', 'F', 'J1', 'J2', 'L1', 'M1', 'M2', 'N']
        
        # Fill data rows - template already has formatting, just update values
        print(f"[BACKEND] Filling {len(sipp_codes_order)} rows with prices...", flush=True)
        row_num = 2
        for sipp_code in sipp_codes_order:
            # Get internal group from SIPP code
            internal_group = car_group_mapping.get(sipp_code, sipp_code)
            group_prices = prices.get(internal_group, {})
            
            # Check if this is a Low Deposit group
            is_low_deposit_group = sipp_code in low_deposit_sipp_codes
            
            # Column 1: Stations (template already has formatting)
            ws.cell(row_num, 1).value = station_code
            
            # Columns 7-18: Prices (1 day fixed through 22-28 daily)
            # Map frontend days to Excel columns
            # Frontend: 1,2,3,4,5,6,7,8,9,14,22,28
            # Excel cols: 7,8,9,10,11,12,13,14,15,16,17,18
            price_columns = [
                (1, 7),   # 1 day fixed ‚Üí Col 7
                (2, 8),   # 2 days fixed ‚Üí Col 8
                (3, 9),   # 3 days fixed ‚Üí Col 9
                (4, 10),  # 4 days fixed ‚Üí Col 10
                (5, 11),  # 5 days fixed ‚Üí Col 11
                (6, 12),  # 6 days fixed ‚Üí Col 12
                (7, 13),  # 7 days fixed ‚Üí Col 13
                (8, 14),  # 8 days ‚Üí Col 14 (8-10 daily)
                (9, 15),  # 9 days ‚Üí Col 15 (11-12 daily) 
                (14, 16), # 14 days ‚Üí Col 16 (13-14 daily)
                (22, 17), # 22 days ‚Üí Col 17 (15-21 daily)
                (28, 18)  # 28 days ‚Üí Col 18 (22-28 daily)
            ]
            
            for day_key, col_idx in price_columns:
                price = calculate_price_for_day(group_prices, int(day_key))
                
                # Check if this is a Low Deposit group and if it's disabled
                should_skip_price = is_low_deposit_group and not abbycar_low_deposit_enabled
                
                if price and not should_skip_price:
                    # Apply Abbycar adjustment percentage
                    total_adjustment = abbycar_adjustment
                    
                    # Add Low Deposit adjustment if group is in Low Deposit list AND enabled
                    if is_low_deposit_group and abbycar_low_deposit_enabled:
                        total_adjustment += abbycar_low_deposit_adjustment
                    
                    adjusted_price = float(price) * (1 + total_adjustment / 100)
                    ws.cell(row_num, col_idx).value = adjusted_price
                else:
                    # Leave empty if: no price OR (Low Deposit group AND disabled)
                    ws.cell(row_num, col_idx).value = ''
            
            row_num += 1
        
        print(f"[BACKEND] Filled {row_num - 2} rows with prices", flush=True)
        
        # Save workbook to BytesIO
        print("[BACKEND] Saving workbook to BytesIO...", flush=True)
        excel_file = io.BytesIO()
        wb.save(excel_file)
        excel_file.seek(0)
        
        # Verify it's actually an Excel file
        excel_bytes = excel_file.getvalue()
        print(f"[BACKEND] Excel file size: {len(excel_bytes)} bytes", flush=True)
        
        if len(excel_bytes) < 100:
            raise Exception("Generated Excel file is too small - likely corrupted")
        
        # Check Excel signature (PK zip header)
        if not excel_bytes.startswith(b'PK'):
            raise Exception("Generated file is not a valid Excel file (missing PK signature)")
        
        excel_file.seek(0)
        
        # Generate filename: ABBYCAR-FARO-01-31-NOV or ABBYCAR-ALBUFEIRA-01-31-NOV
        # Extract month name from date
        from datetime import datetime
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        month_names = ['JAN','FEV','MAR','ABR','MAI','JUN','JUL','AGO','SET','OUT','NOV','DEZ']
        month_name = month_names[date_obj.month - 1]
        
        # Location name (uppercase)
        location_name = location.upper().replace(' ', '-')
        
        # Filename format: ABBYCAR-LOCATION-01-31-MONTH
        filename = f"ABBYCAR-{location_name}-01-31-{month_name}.xlsx"
        print(f"[BACKEND] Excel file ready: {filename} ({len(excel_bytes)} bytes)", flush=True)
        
        # SALVAR NA BASE DE DADOS (persistente)
        print("[BACKEND] Saving to database...", flush=True)
        try:
            excel_bytes = excel_file.getvalue()
            username = request.session.get("username", "admin")
            save_file_to_db(
                filename=filename,
                filepath=f"/exports/{filename}",
                file_data=excel_bytes,
                content_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                uploaded_by=username
            )
            log_to_db("INFO", f"Excel export saved to DB: {filename}", "main", "export_automated_prices_excel")
            print(f"[BACKEND] ‚úÖ Saved to database: {filename} ({len(excel_bytes)} bytes)", flush=True)
        except Exception as e:
            log_to_db("ERROR", f"Failed to save Excel to DB: {str(e)}", "main", "export_automated_prices_excel")
            print(f"[BACKEND] ‚ö†Ô∏è Database save failed: {str(e)} - download will still work", flush=True)
        
        # Reset para retornar
        excel_file.seek(0)
        
        print("[BACKEND] Sending response to frontend...", flush=True)
        from starlette.responses import Response
        return Response(
            content=excel_file.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Cache-Control": "no-cache"
            }
        )
        
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# API ENDPOINTS - LOCALSTORAGE MIGRATION
# ============================================================

@app.post("/api/save-search-history")
async def save_search_history(request: Request):
    """Save automated prices search to history (PostgreSQL)"""
    require_auth(request)
    try:
        data = await request.json()
        location = data.get('location', 'Unknown')
        date = data.get('date', datetime.now().strftime('%Y-%m-%d'))
        prices = data.get('prices', {})
        price_count = data.get('priceCount', 0)
        username = request.session.get("username", "admin")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Ensure table exists
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS search_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        location TEXT NOT NULL,
                        search_date TEXT NOT NULL,
                        prices_data TEXT NOT NULL,
                        price_count INTEGER DEFAULT 0,
                        searched_by TEXT,
                        searched_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Insert search
                conn.execute("""
                    INSERT INTO search_history (location, search_date, prices_data, price_count, searched_by)
                    VALUES (?, ?, ?, ?, ?)
                """, (location, date, json.dumps(prices), price_count, username))
                
                conn.commit()
                
                return _no_store_json({"ok": True, "message": "Search saved to history"})
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        log_to_db("ERROR", f"Failed to save search history: {str(e)}", "main", "save_search_history")
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/get-search-history")
async def get_search_history(request: Request):
    """Get search history from PostgreSQL"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT id, location, search_date, prices_data, price_count, searched_by, searched_at
                    FROM search_history
                    ORDER BY searched_at DESC
                    LIMIT 50
                """)
                
                rows = cursor.fetchall()
                history = []
                
                for row in rows:
                    history.append({
                        'id': row[0],
                        'location': row[1],
                        'date': row[2],
                        'prices': json.loads(row[3]) if row[3] else {},
                        'priceCount': row[4],
                        'searchedBy': row[5],
                        'searchedAt': row[6]
                    })
                
                return _no_store_json({"ok": True, "history": history})
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

def _ensure_recent_searches_table():
    """Create recent_searches table on startup"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Check if using PostgreSQL or SQLite - MULTIPLE METHODS
                is_postgres = False
                
                # Method 1: Check connection type name
                conn_type = type(conn).__name__
                if 'psycopg' in conn_type.lower() or conn_type == 'connection':
                    is_postgres = True
                
                # Method 2: Check if DATABASE_URL exists (PostgreSQL indicator)
                if not is_postgres and os.getenv('DATABASE_URL'):
                    is_postgres = True
                
                # Method 3: Try isinstance check
                if not is_postgres:
                    try:
                        import psycopg2
                        is_postgres = isinstance(conn, psycopg2.extensions.connection)
                    except:
                        pass
                
                logging.info(f"üîç Database type detected: {'PostgreSQL' if is_postgres else 'SQLite'} (conn type: {conn_type}, DATABASE_URL: {bool(os.getenv('DATABASE_URL'))})")
                
                if is_postgres:
                    # PostgreSQL syntax
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS recent_searches (
                            id SERIAL PRIMARY KEY,
                            location TEXT NOT NULL,
                            start_date TEXT NOT NULL,
                            days INTEGER NOT NULL,
                            results_data TEXT NOT NULL,
                            timestamp TEXT NOT NULL,
                            "user" TEXT,
                            source TEXT DEFAULT 'manual',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # CRITICAL MIGRATION: Add source column if table already exists without it
                    try:
                        conn.execute("ALTER TABLE recent_searches ADD COLUMN source TEXT DEFAULT 'manual'")
                        conn.commit()
                        logging.info("‚úÖ Added 'source' column to recent_searches (PostgreSQL)")
                    except Exception as e:
                        conn.rollback()  # MUST rollback on error
                        error_msg = str(e).lower()
                        if 'already exists' in error_msg or 'duplicate column' in error_msg:
                            logging.info("‚ÑπÔ∏è Column 'source' already exists in recent_searches")
                        else:
                            logging.error(f"‚ö†Ô∏è Failed to add 'source' column: {e}")
                else:
                    # SQLite syntax
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS recent_searches (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            location TEXT NOT NULL,
                            start_date TEXT NOT NULL,
                            days INTEGER NOT NULL,
                            results_data TEXT NOT NULL,
                            timestamp TEXT NOT NULL,
                            user TEXT,
                            source TEXT DEFAULT 'manual',
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    
                    # CRITICAL MIGRATION: Add source column if table already exists without it
                    try:
                        conn.execute("ALTER TABLE recent_searches ADD COLUMN source TEXT DEFAULT 'manual'")
                        conn.commit()
                        logging.info("‚úÖ Added 'source' column to recent_searches (SQLite)")
                    except Exception as e:
                        conn.rollback()  # MUST rollback on error
                        error_msg = str(e).lower()
                        if 'duplicate column' in error_msg or 'already exists' in error_msg:
                            logging.info("‚ÑπÔ∏è Column 'source' already exists in recent_searches")
                        else:
                            logging.error(f"‚ö†Ô∏è Failed to add 'source' column: {e}")
                
                # Create index for faster queries
                try:
                    if is_postgres:
                        conn.execute("""
                            CREATE INDEX IF NOT EXISTS idx_recent_searches_user 
                            ON recent_searches("user", created_at DESC)
                        """)
                    else:
                        conn.execute("""
                            CREATE INDEX IF NOT EXISTS idx_recent_searches_user 
                            ON recent_searches(user, created_at DESC)
                        """)
                except Exception:
                    pass  # Index might already exist
                
                conn.commit()
                logging.info(f"‚úÖ recent_searches table created/verified ({'PostgreSQL' if is_postgres else 'SQLite'})")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error creating recent_searches table: {e}")
        import traceback
        traceback.print_exc()

def _ensure_missing_columns():
    """Add missing columns to existing tables for PostgreSQL compatibility"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                import psycopg2
                is_postgres = isinstance(conn, psycopg2.extensions.connection)
                
                logging.info(f"üîß Checking missing columns ({'PostgreSQL' if is_postgres else 'SQLite'})...")
                
                # Add price_num to price_snapshots if missing
                try:
                    if is_postgres:
                        conn.execute("""
                            ALTER TABLE price_snapshots 
                            ADD COLUMN IF NOT EXISTS price_num NUMERIC
                        """)
                    else:
                        # SQLite doesn't support IF NOT EXISTS in ALTER
                        try:
                            conn.execute("ALTER TABLE price_snapshots ADD COLUMN price_num REAL")
                        except:
                            pass  # Column already exists
                    logging.info("  ‚úÖ price_snapshots.price_num checked")
                except Exception as e:
                    logging.warning(f"  ‚ö†Ô∏è price_snapshots.price_num: {e}")
                
                # Add location to ai_learning_data if missing
                try:
                    if is_postgres:
                        conn.execute("""
                            ALTER TABLE ai_learning_data 
                            ADD COLUMN IF NOT EXISTS location TEXT
                        """)
                    else:
                        try:
                            conn.execute("ALTER TABLE ai_learning_data ADD COLUMN location TEXT")
                        except:
                            pass
                    logging.info("  ‚úÖ ai_learning_data.location checked")
                except Exception as e:
                    logging.warning(f"  ‚ö†Ô∏è ai_learning_data.location: {e}")
                
                # Add config to automated_price_rules if missing
                try:
                    if is_postgres:
                        conn.execute("""
                            ALTER TABLE automated_price_rules 
                            ADD COLUMN IF NOT EXISTS config TEXT
                        """)
                    else:
                        try:
                            conn.execute("ALTER TABLE automated_price_rules ADD COLUMN config TEXT")
                        except:
                            pass
                    logging.info("  ‚úÖ automated_price_rules.config checked")
                except Exception as e:
                    logging.warning(f"  ‚ö†Ô∏è automated_price_rules.config: {e}")
                
                conn.commit()
                logging.info("‚úÖ Missing columns check completed")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error checking missing columns: {e}")

def _ensure_suppliers_table():
    """Create suppliers table if missing"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                import psycopg2
                is_postgres = isinstance(conn, psycopg2.extensions.connection)
                
                if is_postgres:
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS suppliers (
                            id SERIAL PRIMARY KEY,
                            name TEXT NOT NULL,
                            logo_path TEXT,
                            active INTEGER DEFAULT 1,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                else:
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS suppliers (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            name TEXT NOT NULL,
                            logo_path TEXT,
                            active INTEGER DEFAULT 1,
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                
                conn.commit()
                logging.info(f"‚úÖ suppliers table created/verified ({'PostgreSQL' if is_postgres else 'SQLite'})")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error creating suppliers table: {e}")
        import traceback
        traceback.print_exc()

def _adapt_query_for_db(query: str, is_postgres: bool) -> str:
    """Adapt SQL query syntax for PostgreSQL or SQLite"""
    if is_postgres:
        # PostgreSQL: DATE(ts::timestamp) instead of DATE(ts)
        query = query.replace("DATE(ts)", "DATE(ts::timestamp)")
        # PostgreSQL: strftime -> EXTRACT
        query = query.replace("strftime('%m', ts)", "EXTRACT(MONTH FROM ts::timestamp)")
        query = query.replace("strftime('%Y', ts)", "EXTRACT(YEAR FROM ts::timestamp)")
        # PostgreSQL: ? placeholders -> %s
        query = query.replace(" = ?", " = %s")
        query = query.replace(" IN (?", " IN (%s")
    return query

def _ensure_missing_tables():
    """Ensure all missing tables and columns exist (suppliers, ai_learning_data, automated_price_rules, price_snapshots)"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                # Robust PostgreSQL detection
                is_postgres = False
                conn_type = type(conn).__name__
                
                if 'psycopg' in conn_type.lower() or conn_type == 'connection':
                    is_postgres = True
                elif os.getenv('DATABASE_URL'):
                    is_postgres = True
                else:
                    try:
                        import psycopg2
                        is_postgres = isinstance(conn, psycopg2.extensions.connection)
                    except:
                        pass
                
                logging.info(f"üîç Checking missing tables/columns ({'PostgreSQL' if is_postgres else 'SQLite'})")
                
                if is_postgres:
                    # 1. Ensure suppliers table exists
                    try:
                        conn.execute("""
                            CREATE TABLE IF NOT EXISTS suppliers (
                                id SERIAL PRIMARY KEY,
                                name TEXT NOT NULL UNIQUE,
                                logo_path TEXT,
                                active INTEGER DEFAULT 1,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            )
                        """)
                        logging.info("‚úÖ suppliers table created/verified")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è suppliers table: {e}")
                    
                    # 2. Ensure ai_learning_data has location column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='ai_learning_data' AND column_name='location'
                                ) THEN
                                    ALTER TABLE ai_learning_data ADD COLUMN location TEXT;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ ai_learning_data.location column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è ai_learning_data.location: {e}")
                    
                    # 2b. Ensure ai_learning_data has original_price column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='ai_learning_data' AND column_name='original_price'
                                ) THEN
                                    ALTER TABLE ai_learning_data ADD COLUMN original_price REAL;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ ai_learning_data.original_price column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è ai_learning_data.original_price: {e}")
                    
                    # 2c. Ensure ai_learning_data has new_price column (may be named net_price)
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='ai_learning_data' AND column_name='new_price'
                                ) THEN
                                    ALTER TABLE ai_learning_data ADD COLUMN new_price REAL;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ ai_learning_data.new_price column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è ai_learning_data.new_price: {e}")
                    
                    # 2d. Ensure ai_learning_data has timestamp column (may be named ts)
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='ai_learning_data' AND column_name='timestamp'
                                ) THEN
                                    ALTER TABLE ai_learning_data ADD COLUMN timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ ai_learning_data.timestamp column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è ai_learning_data.timestamp: {e}")
                    
                    # 3. Ensure automated_price_rules has config column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='automated_price_rules' AND column_name='config'
                                ) THEN
                                    ALTER TABLE automated_price_rules ADD COLUMN config TEXT;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ automated_price_rules.config column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_price_rules.config: {e}")
                    
                    # 4. Ensure price_snapshots exists with price_num (should already exist from init_db)
                    try:
                        conn.execute("""
                            CREATE TABLE IF NOT EXISTS price_snapshots (
                                id SERIAL PRIMARY KEY,
                                ts TIMESTAMP NOT NULL,
                                location TEXT NOT NULL,
                                pickup_date TEXT,
                                pickup_time TEXT,
                                days INTEGER,
                                supplier TEXT,
                                car TEXT,
                                price_text TEXT,
                                price_num REAL,
                                currency TEXT,
                                link TEXT
                            )
                        """)
                        logging.info("‚úÖ price_snapshots table ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è price_snapshots: {e}")
                    
                    # 5. Ensure pricing_strategies table exists (CRITICAL!)
                    try:
                        conn.execute("""
                            CREATE TABLE IF NOT EXISTS pricing_strategies (
                                id SERIAL PRIMARY KEY,
                                location TEXT NOT NULL,
                                grupo TEXT NOT NULL,
                                month INTEGER,
                                day INTEGER,
                                priority INTEGER NOT NULL DEFAULT 1,
                                strategy_type TEXT NOT NULL,
                                config TEXT NOT NULL,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            )
                        """)
                        logging.info("‚úÖ pricing_strategies table created/verified")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è pricing_strategies table: {e}")
                    
                    # 5b. Ensure pricing_strategies has config column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='pricing_strategies' AND column_name='config'
                                ) THEN
                                    ALTER TABLE pricing_strategies ADD COLUMN config TEXT NOT NULL DEFAULT '{}';
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ pricing_strategies.config column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è pricing_strategies.config: {e}")
                    
                    # 6. Ensure automated_price_rules table exists (CRITICAL!)
                    try:
                        conn.execute("""
                            CREATE TABLE IF NOT EXISTS automated_price_rules (
                                id SERIAL PRIMARY KEY,
                                location TEXT NOT NULL,
                                grupo TEXT NOT NULL,
                                month INTEGER NOT NULL,
                                day INTEGER NOT NULL,
                                config TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                UNIQUE(location, grupo, month, day)
                            )
                        """)
                        logging.info("‚úÖ automated_price_rules table created/verified")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_price_rules table: {e}")
                    
                    # 6b. Remove NOT NULL constraint from rules_json (old column)
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='automated_price_rules' AND column_name='rules_json'
                                ) THEN
                                    ALTER TABLE automated_price_rules ALTER COLUMN rules_json DROP NOT NULL;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ automated_price_rules.rules_json constraint removed")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_price_rules.rules_json: {e}")
                    
                    # 7. Ensure automated_prices_history table exists (CRITICAL FOR DAILY REPORTS!)
                    try:
                        conn.execute("""
                            CREATE TABLE IF NOT EXISTS automated_prices_history (
                                id SERIAL PRIMARY KEY,
                                location TEXT NOT NULL,
                                grupo TEXT NOT NULL,
                                dias INTEGER NOT NULL,
                                pickup_date TEXT NOT NULL,
                                auto_price REAL NOT NULL,
                                real_price REAL NOT NULL,
                                strategy_used TEXT,
                                strategy_details TEXT,
                                min_price_applied REAL,
                                created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                                created_by TEXT,
                                source TEXT DEFAULT 'manual'
                            )
                        """)
                        logging.info("‚úÖ automated_prices_history table created/verified")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_prices_history table: {e}")
                    
                    # 7b. Ensure automated_prices_history has 'pickup_date' column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='automated_prices_history' AND column_name='pickup_date'
                                ) THEN
                                    ALTER TABLE automated_prices_history ADD COLUMN pickup_date TEXT;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ automated_prices_history.pickup_date column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_prices_history.pickup_date: {e}")
                    
                    # 7c. Ensure automated_prices_history has 'created_at' column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='automated_prices_history' AND column_name='created_at'
                                ) THEN
                                    ALTER TABLE automated_prices_history ADD COLUMN created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ automated_prices_history.created_at column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_prices_history.created_at: {e}")
                    
                    # 7d. Ensure automated_prices_history has 'source' column
                    try:
                        conn.execute("""
                            DO $$ 
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM information_schema.columns 
                                    WHERE table_name='automated_prices_history' AND column_name='source'
                                ) THEN
                                    ALTER TABLE automated_prices_history ADD COLUMN source TEXT DEFAULT 'manual';
                                END IF;
                            END $$;
                        """)
                        logging.info("‚úÖ automated_prices_history.source column ensured")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_prices_history.source: {e}")
                    
                    # 7e. Create index for automated_prices_history
                    try:
                        conn.execute("""
                            CREATE INDEX IF NOT EXISTS idx_auto_prices_history 
                            ON automated_prices_history(location, grupo, pickup_date, created_at)
                        """)
                        logging.info("‚úÖ automated_prices_history index created/verified")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è automated_prices_history index: {e}")
                    
                    conn.commit()
                else:
                    # SQLite - just ensure tables exist (columns should be there from init_db)
                    logging.info("‚úÖ SQLite detected - tables managed by init_db()")
                
                logging.info(f"‚úÖ All missing tables/columns verified")
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error ensuring missing tables: {e}")
        import traceback
        traceback.print_exc()

@app.post("/api/recent-searches/save")
async def save_recent_searches(request: Request):
    """Save recent searches with COMPLETE car data to PostgreSQL"""
    require_auth(request)
    try:
        # Aumentar limite de payload para dados grandes (284 carros completos)
        data = await request.json()
        searches = data.get('searches', [])
        username = request.session.get("username", "admin")
        
        logging.info(f"[RECENT-SEARCHES] Saving {len(searches)} searches for user {username}")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Ensure table exists (compatible with both PostgreSQL and SQLite)
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                
                if is_postgres:
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS recent_searches (
                            id SERIAL PRIMARY KEY,
                            location TEXT NOT NULL,
                            start_date TEXT NOT NULL,
                            days INTEGER NOT NULL,
                            results_data TEXT NOT NULL,
                            timestamp TEXT NOT NULL,
                            "user" TEXT,
                            source TEXT DEFAULT 'manual',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    # Add source column if it doesn't exist (migration)
                    try:
                        # Try to add column directly - will fail if already exists
                        conn.execute("ALTER TABLE recent_searches ADD COLUMN source TEXT DEFAULT 'manual'")
                        conn.commit()
                        logging.info("‚úÖ Added 'source' column to recent_searches table")
                    except Exception as e:
                        conn.rollback()  # CRITICAL for PostgreSQL - must rollback on error
                        # Check if error is because column already exists
                        error_msg = str(e).lower()
                        if 'already exists' in error_msg or 'duplicate column' in error_msg:
                            logging.info("‚ÑπÔ∏è Column 'source' already exists in recent_searches")
                        else:
                            logging.error(f"‚ùå Failed to add 'source' column: {e}")
                            logging.error(f"   Error type: {type(e).__name__}")
                        pass  # Continue even if migration fails
                else:
                    conn.execute("""
                        CREATE TABLE IF NOT EXISTS recent_searches (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            location TEXT NOT NULL,
                            start_date TEXT NOT NULL,
                            days INTEGER NOT NULL,
                            results_data TEXT NOT NULL,
                            timestamp TEXT NOT NULL,
                            user TEXT,
                            source TEXT DEFAULT 'manual',
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    # Add source column if it doesn't exist (migration)
                    try:
                        # Try to add column directly - will fail if already exists
                        conn.execute("ALTER TABLE recent_searches ADD COLUMN source TEXT DEFAULT 'manual'")
                        conn.commit()
                        logging.info("‚úÖ Added 'source' column to recent_searches table")
                    except Exception as e:
                        conn.rollback()  # CRITICAL for SQLite - must rollback on error
                        # Check if error is because column already exists
                        error_msg = str(e).lower()
                        if 'duplicate column' in error_msg or 'already exists' in error_msg:
                            logging.info("‚ÑπÔ∏è Column 'source' already exists in recent_searches")
                        else:
                            logging.error(f"‚ùå Failed to add 'source' column: {e}")
                            logging.error(f"   Error type: {type(e).__name__}")
                        pass  # Continue even if migration fails
                
                # Migrate automated reports settings from user_settings to price_automation_settings
                try:
                    # Check if old config exists in user_settings
                    old_cursor = conn.execute(
                        "SELECT setting_value FROM user_settings WHERE setting_key = 'automated_reports' LIMIT 1"
                    )
                    old_row = old_cursor.fetchone()
                    
                    if old_row and old_row[0]:
                        # Check if new config already exists
                        new_cursor = conn.execute(
                            "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                        )
                        new_row = new_cursor.fetchone()
                        
                        if not new_row:
                            # Migrate old config to new location
                            placeholder = "%s" if is_postgres else "?"
                            insert_query = f"""
                                INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                                VALUES ({placeholder}, {placeholder}, 'json', CURRENT_TIMESTAMP)
                            """
                            conn.execute(insert_query, ('automatedReportsSettings', old_row[0]))
                            conn.commit()
                            logging.info("‚úÖ Migrated automated reports settings from user_settings to price_automation_settings")
                except Exception as e:
                    logging.debug(f"Automated reports settings migration: {e}")
                    pass  # Migration not needed or already done
                
                # Clear old searches for this user (keep max 3)
                placeholder = "%s" if is_postgres else "?"
                user_col = '"user"' if is_postgres else 'user'
                conn.execute(f"DELETE FROM recent_searches WHERE {user_col} = {placeholder}", (username,))
                
                # Insert new searches with COMPLETE data
                for idx, search in enumerate(searches[:3]):  # Max 3 searches
                    results = search.get('results', [])
                    results_json = json.dumps(results)
                    source = search.get('source', 'manual')  # 'manual' or 'automated'
                    
                    logging.info(f"[RECENT-SEARCHES] Search {idx+1}: {search.get('location')} - {len(results)} cars - {len(results_json)} bytes - source: {source}")
                    
                    placeholders_str = ', '.join([placeholder] * 7)
                    conn.execute(f"""
                        INSERT INTO recent_searches (location, start_date, days, results_data, timestamp, {user_col}, source)
                        VALUES ({placeholders_str})
                    """, (
                        search.get('location'),
                        search.get('startDate'),
                        search.get('days'),
                        results_json,  # JSON completo com todos os dados
                        search.get('timestamp'),
                        username,
                        source
                    ))
                
                conn.commit()
                logging.info(f"‚úÖ [RECENT-SEARCHES] Saved {len(searches)} searches successfully")
                
                return _no_store_json({"ok": True, "message": f"Recent searches saved ({len(searches)} searches)"})
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        logging.error(f"‚ùå Failed to save recent searches: {str(e)}")
        logging.error(f"   Traceback: {error_detail}")
        
        # Tentar identificar o tipo de erro
        if "payload" in str(e).lower() or "too large" in str(e).lower():
            logging.error("   üí° Error likely due to payload size")
        elif "timeout" in str(e).lower():
            logging.error("   üí° Error likely due to database timeout")
        
        return _no_store_json({"ok": False, "error": str(e), "traceback": error_detail}, 500)

@app.get("/api/recent-searches/load")
async def load_recent_searches(request: Request):
    """Load recent searches from PostgreSQL - includes automated searches"""
    require_auth(request)
    try:
        username = request.session.get("username", "admin")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # PostgreSQL compatibility
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                placeholder = "%s" if is_postgres else "?"
                user_col = '"user"' if is_postgres else 'user'
                
                # MODIFIED: Include both user's searches AND automated searches
                cursor = conn.execute(f"""
                    SELECT location, start_date, days, results_data, timestamp, source, {user_col}
                    FROM recent_searches
                    WHERE {user_col} = {placeholder} OR source = 'automated'
                    ORDER BY created_at DESC
                    LIMIT 50
                """, (username,))
                
                rows = cursor.fetchall()
                searches = []
                
                for row in rows:
                    searches.append({
                        'location': row[0],
                        'startDate': row[1],
                        'days': row[2],
                        'results': json.loads(row[3]) if row[3] else [],
                        'timestamp': row[4],
                        'source': row[5] if len(row) > 5 else 'manual',  # 'automated' or 'manual'
                        'user': row[6] if len(row) > 6 else username  # Show who created it
                    })
                
                logging.info(f"üì• Loaded {len(searches)} recent searches (including automated)")
                return _no_store_json({"ok": True, "searches": searches})
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        logging.error(f"Failed to load recent searches: {str(e)}")
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/save-vans-pricing")
async def save_vans_pricing(request: Request):
    """Save Commercial Vans pricing to PostgreSQL"""
    require_auth(request)
    try:
        data = await request.json()
        username = request.session.get("username", "admin")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Ensure table exists
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS vans_pricing (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        c3_1day REAL,
                        c3_2days REAL,
                        c3_3days REAL,
                        c4_1day REAL,
                        c4_2days REAL,
                        c4_3days REAL,
                        c5_1day REAL,
                        c5_2days REAL,
                        c5_3days REAL,
                        updated_by TEXT,
                        updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Insert new pricing
                conn.execute("""
                    INSERT INTO vans_pricing (
                        c3_1day, c3_2days, c3_3days,
                        c4_1day, c4_2days, c4_3days,
                        c5_1day, c5_2days, c5_3days,
                        updated_by
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    data.get('c3_1day'), data.get('c3_2days'), data.get('c3_3days'),
                    data.get('c4_1day'), data.get('c4_2days'), data.get('c4_3days'),
                    data.get('c5_1day'), data.get('c5_2days'), data.get('c5_3days'),
                    username
                ))
                
                conn.commit()
                return _no_store_json({"ok": True, "message": "Vans pricing saved"})
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/vans-pricing")
async def get_vans_pricing(request: Request):
    """Get Commercial Vans pricing (C3, C4, C5)"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                cursor = con.execute("SELECT * FROM vans_pricing ORDER BY id DESC LIMIT 1")
                row = cursor.fetchone()
                
                if row:
                    return _no_store_json({
                        "ok": True,
                        "pricing": {
                            "c3_1day": row[1], "c3_2days": row[2], "c3_3days": row[3],
                            "c4_1day": row[4], "c4_2days": row[5], "c4_3days": row[6],
                            "c5_1day": row[7], "c5_2days": row[8], "c5_3days": row[9],
                            "updated_at": row[10]
                        }
                    })
                else:
                    # Return defaults
                    return _no_store_json({
                        "ok": True,
                        "pricing": {
                            "c3_1day": 112, "c3_2days": 144, "c3_3days": 180,
                            "c4_1day": 152, "c4_2days": 170, "c4_3days": 210,
                            "c5_1day": 175, "c5_2days": 190, "c5_3days": 240
                        }
                    })
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/vans-pricing")
async def save_vans_pricing(request: Request):
    """Save Commercial Vans pricing"""
    require_auth(request)
    try:
        data = await request.json()
        user = request.session.get('user', 'admin')
        
        with _db_lock:
            con = _db_connect()
            try:
                con.execute("""
                    INSERT INTO vans_pricing (
                        c3_1day, c3_2days, c3_3days,
                        c4_1day, c4_2days, c4_3days,
                        c5_1day, c5_2days, c5_3days,
                        updated_by
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    data.get('c3_1day', 112), data.get('c3_2days', 144), data.get('c3_3days', 180),
                    data.get('c4_1day', 152), data.get('c4_2days', 170), data.get('c4_3days', 210),
                    data.get('c5_1day', 175), data.get('c5_2days', 190), data.get('c5_3days', 240),
                    user
                ))
                con.commit()
                return _no_store_json({"ok": True, "message": "Vans pricing saved successfully"})
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/price-validation/rules")
async def get_price_validation_rules(request: Request):
    """Get Price Validation Rules from database"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                # Get the most recent rules
                row = con.execute("""
                    SELECT rules_json, updated_at 
                    FROM price_validation_rules 
                    ORDER BY id DESC LIMIT 1
                """).fetchone()
                
                if row:
                    import json
                    return _no_store_json({
                        "ok": True,
                        "rules": json.loads(row[0]),
                        "updated_at": row[1]
                    })
                else:
                    # Return empty array if no rules exist
                    return _no_store_json({
                        "ok": True,
                        "rules": [],
                        "updated_at": None
                    })
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/price-validation/rules")
async def save_price_validation_rules(request: Request):
    """Save Price Validation Rules to database"""
    require_auth(request)
    try:
        data = await request.json()
        rules = data.get('rules', [])
        user = request.session.get('user', 'admin')
        
        import json
        rules_json = json.dumps(rules)
        
        with _db_lock:
            con = _db_connect()
            try:
                # Insert new rules (keeping history)
                con.execute("""
                    INSERT INTO price_validation_rules (rules_json, updated_by)
                    VALUES (?, ?)
                """, (rules_json, user))
                con.commit()
                
                return _no_store_json({
                    "ok": True,
                    "message": "Price validation rules saved successfully",
                    "count": len(rules)
                })
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/automation-settings")
async def get_automation_settings(request: Request):
    """Get price automation settings"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                cursor = con.execute("SELECT setting_key, setting_value, setting_type FROM price_automation_settings")
                rows = cursor.fetchall()
                
                settings = {}
                for row in rows:
                    key, value, stype = row
                    if stype == 'json':
                        import json
                        settings[key] = json.loads(value)
                    elif stype == 'number':
                        settings[key] = float(value)
                    else:
                        settings[key] = value
                
                return _no_store_json({"ok": True, "settings": settings})
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/automation-settings")
async def save_automation_settings(request: Request):
    """Save price automation settings"""
    require_auth(request)
    try:
        data = await request.json()
        
        with _db_lock:
            con = _db_connect()
            try:
                for key, value in data.items():
                    import json
                    if isinstance(value, (list, dict)):
                        value_str = json.dumps(value)
                        stype = 'json'
                    elif isinstance(value, (int, float)):
                        value_str = str(value)
                        stype = 'number'
                    else:
                        value_str = str(value)
                        stype = 'string'
                    
                    con.execute("""
                        INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                        VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                        ON CONFLICT (setting_key) DO UPDATE SET
                            setting_value = EXCLUDED.setting_value,
                            setting_type = EXCLUDED.setting_type,
                            updated_at = CURRENT_TIMESTAMP
                    """, (key, value_str, stype))
                
                con.commit()
                return _no_store_json({"ok": True, "message": "Settings saved successfully"})
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.get("/api/custom-days")
async def get_custom_days(request: Request):
    """Get custom days configuration"""
    require_auth(request)
    try:
        with _db_lock:
            con = _db_connect()
            try:
                cursor = con.execute("SELECT days_array FROM custom_days ORDER BY id DESC LIMIT 1")
                row = cursor.fetchone()
                
                if row:
                    import json
                    return _no_store_json({"ok": True, "days": json.loads(row[0])})
                else:
                    # Default days
                    return _no_store_json({"ok": True, "days": [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 22, 28, 31, 60]})
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/custom-days")
async def save_custom_days(request: Request):
    """Save custom days configuration"""
    require_auth(request)
    try:
        data = await request.json()
        days = data.get('days', [])
        
        import json
        with _db_lock:
            con = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                is_postgres = hasattr(con, 'cursor')
                placeholder = "%s" if is_postgres else "?"
                
                query = f"INSERT INTO custom_days (days_array) VALUES ({placeholder})"
                con.execute(query, (json.dumps(days),))
                con.commit()
                return _no_store_json({"ok": True, "message": "Custom days saved successfully"})
            finally:
                con.close()
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

@app.post("/api/fetch-car-photos")
async def fetch_car_photos(request: Request):
    """Fetch car photos from multiple searches (Albufeira + Faro, different dates)"""
    require_auth(request)
    try:
        import asyncio
        from datetime import datetime, timedelta
        from carjet_direct import scrape_carjet_direct
        import sys
        
        # Locations to search
        locations = [
            "Albufeira",
            "Aeroporto de Faro"
        ]
        
        # Generate dates for next 7 days
        today = datetime.now()
        dates = [(today + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(7)]
        
        # Days to search
        days_to_search = [7, 14, 28]
        
        all_photos = {}  # {car_name: photo_url}
        
        print(f"üîç Starting car photo fetch from CarJet...")
        print(f"üìç Locations: {locations}")
        print(f"üìÖ Dates: {dates}")
        print(f"üìÜ Days: {days_to_search}")
        
        # Search each combination - Use mock data for quick testing
        for loc_idx, location in enumerate(locations):
            # ============================================
            # DELAY ENTRE LOCATIONS (2-5s aleat√≥rio)
            # ============================================
            if loc_idx > 0:  # N√£o fazer delay na primeira location
                # random j√° importado globalmente
                delay_between_locations = random.uniform(2.0, 5.0)
                print(f"\n‚è≥ Aguardando {delay_between_locations:.1f}s antes da pr√≥xima location...", file=sys.stderr, flush=True)
                time.sleep(delay_between_locations)
            
            for date in dates:
                for days in days_to_search:
                    try:
                        print(f"\nüîé Searching: {location}, {date}, {days} days", file=sys.stderr, flush=True)
                        
                        # Calculate end date
                        start_dt = datetime.fromisoformat(f"{date}T10:00")
                        end_dt = start_dt + timedelta(days=days)
                        
                        # SCRAPING REAL do CarJet
                        print(f"[PHOTOS] Scraping CarJet for {location}, {days} days...", file=sys.stderr, flush=True)
                        
                        # Use scrape_carjet_direct para obter dados reais
                        items = scrape_carjet_direct(location, start_dt, end_dt, quick=1)
                        
                        print(f"[PHOTOS] Scraped {len(items)} real items from CarJet", file=sys.stderr, flush=True)
                        
                        # Extract photos from items
                        for item in items:
                            car_name = item.get('car', '').strip().lower()
                            photo_url = item.get('photo', '')
                            
                            # Clean car name (remove "ou similar" and extra spaces)
                            if ' ou similar' in car_name:
                                car_name = car_name.split(' ou similar')[0].strip()
                            
                            if car_name and photo_url and car_name not in all_photos:
                                all_photos[car_name] = photo_url
                                print(f"  üì∏ Found photo for: {car_name} -> {photo_url}", file=sys.stderr, flush=True)
                        
                        print(f"‚úÖ Search completed: {len(items)} cars found, total photos: {len(all_photos)}", file=sys.stderr, flush=True)
                        
                        # Delay entre searches (0.5-2s aleat√≥rio)
                        delay = random.uniform(0.5, 2.0)
                        print(f"‚è≥ Aguardando {delay:.1f}s antes da pr√≥xima pesquisa...", file=sys.stderr, flush=True)
                        await asyncio.sleep(delay)
                        
                    except Exception as e:
                        print(f"‚ùå Error searching {location}, {date}, {days} days: {e}", file=sys.stderr, flush=True)
                        import traceback
                        traceback.print_exc()
                        continue
        
        # SALVAR FOTOS NA BASE DE DADOS
        downloaded = 0
        skipped = 0
        
        print(f"\nüíæ Saving {len(all_photos)} photos to database...", file=sys.stderr, flush=True)
        
        for car_name, photo_url in all_photos.items():
            try:
                # Verificar se j√° existe
                with _db_lock:
                    conn = _db_connect()
                    try:
                        existing = conn.execute(
                            "SELECT id FROM vehicle_images WHERE vehicle_key = ?",
                            (car_name,)
                        ).fetchone()
                        
                        if existing:
                            skipped += 1
                            print(f"  ‚è≠Ô∏è  Skipped (already exists): {car_name}", file=sys.stderr, flush=True)
                        else:
                            # Inserir nova foto
                            query = _convert_query_for_db(
                                """INSERT INTO vehicle_images 
                                   (vehicle_key, source_url, downloaded_at, content_type) 
                                   VALUES (?, ?, datetime('now'), 'image/jpeg')""", conn)
                            
                            if conn.__class__.__module__ == 'psycopg2.extensions':
                                with conn.cursor() as cur:
                                    cur.execute(query, (car_name, photo_url))
                            else:
                                conn.execute(query, (car_name, photo_url))
                            conn.commit()
                            downloaded += 1
                            print(f"  ‚úÖ Saved: {car_name} -> {photo_url}", file=sys.stderr, flush=True)
                    finally:
                        conn.close()
            except Exception as e:
                print(f"  ‚ùå Error saving {car_name}: {e}", file=sys.stderr, flush=True)
                continue
        
        print(f"\n‚úÖ Photo import completed: {downloaded} downloaded, {skipped} skipped", file=sys.stderr, flush=True)
        
        return _no_store_json({
            "ok": True,
            "message": f"Found {len(all_photos)} car photos",
            "downloaded": downloaded,
            "skipped": skipped,
            "photos": all_photos
        })
        
    except Exception as e:
        import traceback
        return _no_store_json({"ok": False, "error": str(e), "traceback": traceback.format_exc()}, 500)

# ============================================================
# API ENDPOINTS - UNIVERSAL SETTINGS SYNC (ANTI DATA-LOSS)
# ============================================================

@app.post("/api/settings/sync")
async def sync_all_settings(request: Request):
    """Sync ALL localStorage settings to database - prevents data loss on Render"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üîÑ Syncing {len(data)} setting keys to database")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                for key, value in data.items():
                    # Store as JSON string
                    value_str = value if isinstance(value, str) else json.dumps(value)
                    
                    query = f"""
                        INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                        VALUES ({placeholder}, {placeholder}, 'json', CURRENT_TIMESTAMP)
                        ON CONFLICT (setting_key) DO UPDATE SET
                            setting_value = EXCLUDED.setting_value,
                            updated_at = CURRENT_TIMESTAMP
                    """
                    
                    conn.execute(query, (key, value_str))
                
                conn.commit()
                logging.info(f"‚úÖ Synced {len(data)} settings to database (placeholder: {placeholder})")
                return JSONResponse({"ok": True, "synced": len(data)})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error syncing settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/settings/load-all")
async def load_all_settings(request: Request):
    """Load ALL settings from database - restores data after Render restart"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("SELECT setting_key, setting_value FROM price_automation_settings")
                rows = cursor.fetchall()
                
                settings = {}
                for row in rows:
                    key, value = row[0], row[1]
                    # Try to parse as JSON, fallback to string
                    try:
                        settings[key] = json.loads(value) if value else None
                    except:
                        settings[key] = value
                
                logging.info(f"üì• Loaded {len(settings)} settings from database")
                return JSONResponse({"ok": True, "settings": settings})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error loading settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/settings/automated-reports/save")
async def save_automated_reports_settings(request: Request):
    """Save automated reports settings to PostgreSQL"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üíæ Saving automated reports settings to database")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                # Save as JSON
                settings_json = json.dumps(data)
                
                query = f"""
                    INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                    VALUES ('automatedReportsSettings', {placeholder}, 'json', CURRENT_TIMESTAMP)
                    ON CONFLICT (setting_key) DO UPDATE SET
                        setting_value = EXCLUDED.setting_value,
                        updated_at = CURRENT_TIMESTAMP
                """
                
                conn.execute(query, (settings_json,))
                conn.commit()
                logging.info(f"‚úÖ Saved automated reports settings to PostgreSQL (placeholder: {placeholder})")
                return JSONResponse({"ok": True, "message": "Settings saved successfully"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving automated reports settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/settings/automated-reports/advanced")
async def save_advanced_automated_reports(request: Request):
    """Save ADVANCED automated reports settings (m√∫ltiplos hor√°rios, mensal, etc)"""
    require_auth(request)
    
    try:
        data = await request.json()
        logging.info(f"üíæ Saving ADVANCED automated reports settings")
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                settings_json = json.dumps(data)
                
                query = f"""
                    INSERT INTO price_automation_settings (setting_key, setting_value, setting_type, updated_at)
                    VALUES ('automatedReportsAdvanced', {placeholder}, 'json', CURRENT_TIMESTAMP)
                    ON CONFLICT (setting_key) DO UPDATE SET
                        setting_value = EXCLUDED.setting_value,
                        updated_at = CURRENT_TIMESTAMP
                """
                
                conn.execute(query, (settings_json,))
                conn.commit()
                
                logging.info(f"‚úÖ Saved ADVANCED automated reports settings")
                logging.info(f"   Daily schedules: {len(data.get('daily', {}).get('schedules', []))}")
                logging.info(f"   Weekly: {data.get('weekly', {}).get('enabled', False)}")
                logging.info(f"   Monthly: {data.get('monthly', {}).get('enabled', False)}")
                
                return JSONResponse({"ok": True, "message": "Configura√ß√µes avan√ßadas guardadas com sucesso"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving advanced automated reports: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/settings/automated-reports/advanced/load")
async def load_advanced_automated_reports(request: Request):
    """Load ADVANCED automated reports settings"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsAdvanced'"
                )
                row = cursor.fetchone()
                
                if row and row[0]:
                    settings = json.loads(row[0])
                    logging.info(f"üì• Loaded ADVANCED automated reports settings")
                    return JSONResponse({"ok": True, "settings": settings})
                else:
                    logging.info(f"üì≠ No advanced settings found, returning defaults")
                    return JSONResponse({"ok": True, "settings": None})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error loading advanced settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/settings/automated-reports/load")
async def load_automated_reports_settings(request: Request):
    """Load automated reports settings from PostgreSQL"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                
                if row and row[0]:
                    settings = json.loads(row[0])
                    logging.info(f"üì• Loaded automated reports settings from database")
                    return JSONResponse({"ok": True, "settings": settings})
                else:
                    logging.info(f"üì≠ No automated reports settings found in database")
                    return JSONResponse({"ok": True, "settings": None})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error loading automated reports settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# API ENDPOINTS - OAUTH2 EMAIL INTEGRATION
# ============================================================

@app.get("/api/oauth/gmail/status")
async def check_gmail_oauth_status(request: Request):
    """Check if Gmail OAuth credentials exist and are valid"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT user_email, access_token, refresh_token, expires_at, updated_at
                    FROM oauth_tokens 
                    WHERE provider = 'google' 
                    ORDER BY updated_at DESC 
                    LIMIT 1
                    """
                )
                row = cursor.fetchone()
                
                if not row:
                    return JSONResponse({
                        "ok": False,
                        "connected": False,
                        "message": "Nenhuma conta Gmail conectada. Por favor, conecte sua conta."
                    })
                
                user_email, access_token, refresh_token, expires_at, updated_at = row
                
                # Check if has refresh token (critical!)
                has_refresh = refresh_token and refresh_token.strip() != ''
                has_access = access_token and access_token.strip() != ''
                
                status = {
                    "ok": True,
                    "connected": has_access and has_refresh,
                    "email": user_email,
                    "hasAccessToken": has_access,
                    "hasRefreshToken": has_refresh,
                    "expiresAt": expires_at,
                    "lastUpdated": updated_at,
                    "message": "‚úÖ Gmail conectado e funcional" if (has_access and has_refresh) else "‚ö†Ô∏è Credenciais incompletas - reconecte o Gmail"
                }
                
                logging.info(f"Gmail OAuth Status: connected={status['connected']}, email={user_email}")
                return JSONResponse(status)
                
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error checking Gmail status: {str(e)}")
        return JSONResponse({
            "ok": False,
            "error": str(e)
        }, status_code=500)

@app.get("/api/oauth/gmail/authorize")
async def oauth_gmail_authorize(request: Request):
    """Initiate Gmail OAuth2 flow - REAL Google OAuth"""
    require_auth(request)
    
    # Google OAuth2 configuration
    GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID', 'YOUR_CLIENT_ID_HERE')
    GOOGLE_CLIENT_SECRET = os.getenv('GOOGLE_CLIENT_SECRET', 'YOUR_CLIENT_SECRET_HERE')
    REDIRECT_URI = os.getenv('OAUTH_REDIRECT_URI', 'http://127.0.0.1:8000/api/oauth/gmail/callback')
    
    # Check if credentials are configured
    if GOOGLE_CLIENT_ID == 'YOUR_CLIENT_ID_HERE':
        # Show setup instructions if not configured
        html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Gmail OAuth</title>
        <style>
            body {
                font-family: 'Outfit', sans-serif;
                display: flex;
                align-items: center;
                justify-content: center;
                height: 100vh;
                margin: 0;
                background: #f0f9fb;
            }
            .container {
                text-align: center;
                padding: 2rem;
                background: white;
                border-radius: 8px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            }
            .icon {
                width: 64px;
                height: 64px;
                margin: 0 auto 1rem;
                color: #009cb6;
            }
            h1 {
                color: #009cb6;
                margin-bottom: 1rem;
            }
            p {
                color: #666;
                margin-bottom: 1.5rem;
            }
            button {
                background: #009cb6;
                color: white;
                border: none;
                padding: 12px 24px;
                border-radius: 6px;
                cursor: pointer;
                font-size: 16px;
            }
            button:hover {
                background: #007a91;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <svg class="icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"/>
            </svg>
            <h1>Conectar Gmail</h1>
            <p>Esta √© uma <strong>demonstra√ß√£o</strong> do fluxo OAuth2.</p>
            <p style="font-size: 14px; color: #666; margin: 1rem 0;">Para conectar ao Gmail real, √© necess√°rio:</p>
            <ul style="text-align: left; font-size: 13px; color: #666; margin: 0 auto; max-width: 400px; line-height: 1.8;">
                <li>Registar app no <a href="https://console.cloud.google.com" target="_blank" style="color: #009cb6;">Google Cloud Console</a></li>
                <li>Obter Client ID e Client Secret</li>
                <li>Configurar OAuth2 redirect URLs</li>
                <li>Implementar fluxo OAuth completo</li>
            </ul>
            <p style="font-size: 12px; color: #999; margin-top: 1rem;">Por agora, clique abaixo para simular a conex√£o:</p>
            <button onclick="simulateOAuth()">Simular Conex√£o Gmail</button>
        </div>
        <script>
            function simulateOAuth() {
                // Simulate successful OAuth
                const data = {
                    type: 'oauth-success',
                    provider: 'gmail',
                    email: 'seu-email@gmail.com',
                    token: 'mock_access_token_' + Date.now(),
                    refreshToken: 'mock_refresh_token',
                    expiresAt: Date.now() + 3600000
                };
                
                // Send message to parent window
                if (window.opener) {
                    window.opener.postMessage(data, '*');
                    window.close();
                } else {
                    alert('Erro: Janela pai n√£o encontrada');
                }
            }
        </script>
    </body>
    </html>
    """
        return HTMLResponse(content=html)
    
    # Real OAuth2 flow - redirect to Google
    import urllib.parse
    
    # OAuth2 parameters
    auth_params = {
        'client_id': GOOGLE_CLIENT_ID,
        'redirect_uri': REDIRECT_URI,
        'response_type': 'code',
        'scope': 'https://www.googleapis.com/auth/gmail.send https://www.googleapis.com/auth/userinfo.email',
        'access_type': 'offline',
        'prompt': 'consent'
    }
    
    # Build authorization URL
    auth_url = 'https://accounts.google.com/o/oauth2/v2/auth?' + urllib.parse.urlencode(auth_params)
    
    # Redirect to Google OAuth
    return RedirectResponse(url=auth_url)

@app.get("/api/oauth/gmail/callback")
async def oauth_gmail_callback(request: Request, code: str = None, error: str = None):
    """Handle Gmail OAuth2 callback"""
    require_auth(request)
    
    if error:
        return HTMLResponse(f"<h1>Error: {error}</h1><p>OAuth authorization failed.</p>")
    
    if not code:
        return HTMLResponse("<h1>Error</h1><p>No authorization code received.</p>")
    
    # Exchange code for tokens
    GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID')
    GOOGLE_CLIENT_SECRET = os.getenv('GOOGLE_CLIENT_SECRET')
    REDIRECT_URI = os.getenv('OAUTH_REDIRECT_URI', 'http://127.0.0.1:8000/api/oauth/gmail/callback')
    
    import urllib.parse
    import httpx
    import time
    
    token_params = {
        'code': code,
        'client_id': GOOGLE_CLIENT_ID,
        'client_secret': GOOGLE_CLIENT_SECRET,
        'redirect_uri': REDIRECT_URI,
        'grant_type': 'authorization_code'
    }
    
    try:
        async with httpx.AsyncClient() as client:
            # Exchange code for tokens
            token_response = await client.post(
                'https://oauth2.googleapis.com/token',
                data=token_params
            )
            token_data = token_response.json()
            
            if 'error' in token_data:
                return HTMLResponse(f"<h1>Error</h1><p>{token_data.get('error_description', 'Token exchange failed')}</p>")
            
            # Get user info (email, name, picture)
            access_token = token_data.get('access_token')
            userinfo_response = await client.get(
                'https://www.googleapis.com/oauth2/v2/userinfo',
                headers={'Authorization': f'Bearer {access_token}'}
            )
            userinfo = userinfo_response.json()
            user_email = userinfo.get('email', 'unknown@gmail.com')
            user_name = userinfo.get('name', '')
            user_picture = userinfo.get('picture', '')  # Google profile picture URL
            google_id = userinfo.get('id', '')
            
            # Save token to database IMMEDIATELY
            try:
                expires_at = int(time.time()) + token_data.get('expires_in', 3600)
                refresh_token_new = token_data.get('refresh_token')
                
                with _db_lock:
                    conn = _db_connect()
                    try:
                        # Detectar PostgreSQL vs SQLite
                        is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                        placeholder = "%s" if is_postgres else "?"
                        
                        # Se h√° um novo refresh_token, atualizar tudo
                        # Se N√ÉO h√° refresh_token (reconex√£o), manter o antigo
                        if refresh_token_new:
                            query = f"""
                                INSERT INTO oauth_tokens 
                                (provider, user_email, access_token, refresh_token, expires_at, google_id, user_name, user_picture)
                                VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                                ON CONFLICT(provider, user_email) 
                                DO UPDATE SET 
                                    access_token = excluded.access_token,
                                    refresh_token = excluded.refresh_token,
                                    expires_at = excluded.expires_at,
                                    google_id = excluded.google_id,
                                    user_name = excluded.user_name,
                                    user_picture = excluded.user_picture,
                                    updated_at = CURRENT_TIMESTAMP
                            """
                            
                            conn.execute(
                                query,
                                ('gmail', user_email, access_token, refresh_token_new, 
                                 expires_at, google_id, user_name, user_picture)
                            )
                            logging.info(f"‚úÖ Token saved with NEW refresh_token for {user_email}")
                        else:
                            # Atualizar apenas access_token e expires_at, MANTER refresh_token existente
                            query = f"""
                                INSERT INTO oauth_tokens 
                                (provider, user_email, access_token, refresh_token, expires_at, google_id, user_name, user_picture)
                                VALUES ({placeholder}, {placeholder}, {placeholder}, '', {placeholder}, {placeholder}, {placeholder}, {placeholder})
                                ON CONFLICT(provider, user_email) 
                                DO UPDATE SET 
                                    access_token = excluded.access_token,
                                    expires_at = excluded.expires_at,
                                    google_id = excluded.google_id,
                                    user_name = excluded.user_name,
                                    user_picture = excluded.user_picture,
                                    updated_at = CURRENT_TIMESTAMP
                            """
                            
                            conn.execute(
                                query,
                                ('gmail', user_email, access_token, expires_at, google_id, user_name, user_picture)
                            )
                            logging.info(f"‚úÖ Token updated WITHOUT changing refresh_token for {user_email}")
                        
                        conn.commit()
                    finally:
                        conn.close()
            except Exception as e:
                logging.error(f"‚ùå Failed to save token to database: {str(e)}")
            
            # Return success page
            html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Gmail Connected</title>
                <style>
                    body {{
                        font-family: 'Outfit', sans-serif;
                        display: flex;
                        align-items: center;
                        justify-content: center;
                        height: 100vh;
                        margin: 0;
                        background: #f0f9fb;
                    }}
                    .container {{
                        text-align: center;
                        padding: 2rem;
                        background: white;
                        border-radius: 8px;
                        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                    }}
                    .icon {{
                        width: 64px;
                        height: 64px;
                        margin: 0 auto 1rem;
                        color: #009cb6;
                    }}
                    h1 {{
                        color: #009cb6;
                        margin-bottom: 1rem;
                    }}
                </style>
            </head>
            <body>
                <div class="container">
                    <svg class="icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                    </svg>
                    <h1>Gmail Conectado!</h1>
                    <p>Conta: <strong>{user_email}</strong></p>
                    <p style="color: #666; font-size: 14px;">Esta janela vai fechar automaticamente...</p>
                </div>
                <script>
                    const data = {{
                        type: 'oauth-success',
                        provider: 'google',
                        email: '{user_email}',
                        name: '{user_name}',
                        picture: '{user_picture}',
                        googleId: '{google_id}',
                        token: '{access_token}',
                        refreshToken: '{token_data.get("refresh_token", "")}',
                        expiresAt: {int(time.time()) + token_data.get('expires_in', 3600)} * 1000
                    }};
                    
                    if (window.opener) {{
                        window.opener.postMessage(data, '*');
                        setTimeout(() => window.close(), 2000);
                    }}
                </script>
            </body>
            </html>
            """
            return HTMLResponse(content=html)
            
    except Exception as e:
        logging.error(f"OAuth callback error: {str(e)}")
        return HTMLResponse(f"<h1>Error</h1><p>Failed to complete OAuth: {str(e)}</p>")

@app.post("/api/oauth/save-token")
async def save_oauth_token(request: Request):
    """Save OAuth token to database (persists across deploys)"""
    require_auth(request)
    
    try:
        data = await request.json()
        provider = data.get('provider', 'google')
        user_email = data.get('email')
        access_token = data.get('token')
        refresh_token = data.get('refreshToken', '')
        expires_at = data.get('expiresAt', 0)
        google_id = data.get('googleId', '')
        user_name = data.get('name', '')
        user_picture = data.get('picture', '')
        
        if not user_email or not access_token:
            return JSONResponse({"ok": False, "error": "Missing email or token"})
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                query = f"""
                    INSERT INTO oauth_tokens 
                    (provider, user_email, access_token, refresh_token, expires_at, google_id, user_name, user_picture, updated_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, CURRENT_TIMESTAMP)
                    ON CONFLICT (provider, user_email) DO UPDATE SET
                        access_token = EXCLUDED.access_token,
                        refresh_token = EXCLUDED.refresh_token,
                        expires_at = EXCLUDED.expires_at,
                        google_id = EXCLUDED.google_id,
                        user_name = EXCLUDED.user_name,
                        user_picture = EXCLUDED.user_picture,
                        updated_at = CURRENT_TIMESTAMP
                """
                
                conn.execute(
                    query,
                    (provider, user_email, access_token, refresh_token, expires_at, google_id, user_name, user_picture)
                )
                conn.commit()
                logging.info(f"‚úÖ OAuth token saved to database for {user_email}")
                return JSONResponse({"ok": True, "message": "Token saved successfully"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error saving OAuth token: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/oauth/load-token")
async def load_oauth_token(request: Request):
    """Load OAuth token from database"""
    require_auth(request)
    
    try:
        provider = request.query_params.get('provider', 'google')
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                cursor = conn.execute(
                    f"""
                    SELECT user_email, access_token, refresh_token, expires_at, google_id, user_name, user_picture
                    FROM oauth_tokens
                    WHERE provider = {placeholder}
                    ORDER BY updated_at DESC
                    LIMIT 1
                    """,
                    (provider,)
                )
                row = cursor.fetchone()
                
                if row:
                    return JSONResponse({
                        "ok": True,
                        "token": {
                            "email": row[0],
                            "token": row[1],
                            "refreshToken": row[2],
                            "expiresAt": row[3],
                            "googleId": row[4],
                            "name": row[5],
                            "picture": row[6]
                        }
                    })
                else:
                    return JSONResponse({"ok": False, "error": "No token found"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error loading OAuth token: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/user/update-google-profile")
async def update_google_profile(request: Request):
    """Update user profile with Google info (picture, google_id)"""
    require_auth(request)
    
    try:
        data = await request.json()
        google_id = data.get('googleId')
        picture_url = data.get('pictureUrl')
        
        if not google_id or not picture_url:
            return JSONResponse({"ok": False, "error": "Missing googleId or pictureUrl"})
        
        # Get current user from session
        username = request.session.get('username')
        if not username:
            return JSONResponse({"ok": False, "error": "Not authenticated"})
        
        # Update user with google_id and profile picture
        with _db_lock:
            conn = _db_connect()
            try:
                # Detectar tipo de BD
                is_postgres = False
                conn_type = type(conn).__name__
                
                if 'psycopg' in conn_type.lower() or conn_type == 'connection':
                    is_postgres = True
                elif os.getenv('DATABASE_URL'):
                    is_postgres = True
                
                placeholder = "%s" if is_postgres else "?"
                
                # First, remove google_id from any other user (to avoid duplicate constraint)
                conn.execute(
                    f"UPDATE users SET google_id = NULL WHERE google_id = {placeholder} AND username != {placeholder}",
                    (google_id, username)
                )
                
                # Then update current user with google_id and profile picture
                conn.execute(
                    f"UPDATE users SET google_id = {placeholder}, profile_picture_path = {placeholder} WHERE username = {placeholder}",
                    (google_id, picture_url, username)
                )
                conn.commit()
                logging.info(f"‚úÖ Updated Google profile for user {username}")
                
                return JSONResponse({
                    "ok": True,
                    "message": "Profile updated successfully"
                })
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"Error updating Google profile: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/email/settings/save")
async def save_email_settings(request: Request):
    """Save email settings to database (persists across deploys)"""
    require_auth(request)
    
    try:
        data = await request.json()
        username = request.session.get('username', 'default')
        
        # Convert settings dict to JSON string
        import json
        settings_json = json.dumps(data)
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect PostgreSQL vs SQLite
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                # Save to user_settings table
                query = f"""
                    INSERT INTO user_settings (user_key, setting_key, setting_value, updated_at)
                    VALUES ({placeholder}, 'email_settings', {placeholder}, CURRENT_TIMESTAMP)
                    ON CONFLICT (user_key, setting_key) DO UPDATE SET
                        setting_value = EXCLUDED.setting_value,
                        updated_at = CURRENT_TIMESTAMP
                """
                
                conn.execute(query, (username, settings_json))
                conn.commit()
                logging.info(f"‚úÖ Email settings saved to database for user {username}")
                return JSONResponse({"ok": True, "message": "Settings saved successfully"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error saving email settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/email/settings/load")
async def load_email_settings(request: Request):
    """Load email settings from database"""
    require_auth(request)
    
    try:
        username = request.session.get('username', 'default')
        
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    """
                    SELECT setting_value
                    FROM user_settings
                    WHERE user_key = ? AND setting_key = 'email_settings'
                    """,
                    (username,)
                )
                row = cursor.fetchone()
                
                if row:
                    import json
                    settings = json.loads(row[0])
                    logging.info(f"‚úÖ Email settings loaded from database for user {username}")
                    return JSONResponse({"ok": True, "settings": settings})
                else:
                    return JSONResponse({"ok": False, "error": "No settings found"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Error loading email settings: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/reports/test-daily")
async def test_daily_report(request: Request):
    """Send test daily report email - 2 EMAILS SEPARADOS com NOVO TEMPLATE"""
    require_auth(request)
    
    try:
        from googleapiclient.discovery import build
        from google.oauth2.credentials import Credentials
        import base64
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        from improved_reports import generate_daily_report_html_by_location
        
        # Get complete Gmail credentials
        credentials = _get_gmail_credentials()
        
        if not credentials:
            return JSONResponse({
                "ok": False,
                "error": "Token OAuth n√£o encontrado. Por favor, conecte sua conta Gmail primeiro."
            })
        
        # Usar email de configura√ß√£o como destinat√°rio
        report_recipients = [_get_setting("report_email", "carlpac82@hotmail.com")]
        
        username = request.session.get('username')
        logging.info(f"Test daily report requested by {username} for {len(report_recipients)} recipient(s)")
        
        # Load search data - try today first, then last 7 days
        search_data = {'results': []}
        with _db_lock:
            conn = _db_connect()  
            try:
                # Try today first
                cursor = conn.execute(
                    """
                    SELECT location, start_date, days, results_data, timestamp
                    FROM recent_searches
                    WHERE DATE(timestamp) = CURRENT_DATE
                    ORDER BY timestamp DESC
                    """
                )
                rows = cursor.fetchall()
                
                # If no data today, get most recent from last 7 days
                if not rows:
                    logging.info("üì≠ No data from today, fetching most recent from last 7 days...")
                    cursor = conn.execute(
                        """
                        SELECT location, start_date, days, results_data, timestamp
                        FROM recent_searches
                        WHERE timestamp >= datetime('now', '-7 days')
                        ORDER BY timestamp DESC
                        LIMIT 50
                        """
                    )
                    rows = cursor.fetchall()
                
                all_results = []
                for row in rows:
                    location, start_date, days, results_data, timestamp = row
                    if results_data:
                        results = json.loads(results_data)
                        for r in results:
                            r['days'] = days
                            r['location'] = location
                        all_results.extend(results)
                
                search_data = {'results': all_results}
                logging.info(f"üìä Found {len(all_results)} total results for test report")
            finally:
                conn.close()
        
        if not search_data['results']:
            return JSONResponse({
                "ok": False,
                "error": "Sem dados de pesquisa dispon√≠veis. Execute uma pesquisa primeiro ou aguarde a pesquisa autom√°tica."
            })
        
        # Build Gmail service
        service = build('gmail', 'v1', credentials=credentials)
        
        # ENVIAR 2 EMAILS SEPARADOS
        locations = ['Albufeira', 'Aeroporto de Faro']
        sent_count = 0
        message_ids = []
        sent_details = []
        
        for location in locations:
            logging.info(f"\nüìç Generating test report for: {location}")
            
            # Generate HTML for this location with REAL data
            html_content = generate_daily_report_html_by_location(search_data, location)
            
            # Send to all recipients
            for recipient in report_recipients:
                try:
                    message = MIMEMultipart('alternative')
                    message['to'] = recipient
                    message['subject'] = f'üìä TESTE Relat√≥rio Di√°rio {location} - Auto Prudente ({datetime.now().strftime("%d/%m/%Y")})'
                    
                    html_part = MIMEText(html_content, 'html')
                    message.attach(html_part)
                    
                    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
                    send_message = service.users().messages().send(
                        userId='me',
                        body={'raw': raw_message}
                    ).execute()
                    
                    message_ids.append(send_message.get('id'))
                    sent_count += 1
                    sent_details.append(f"{location} ‚Üí {recipient}")
                    logging.info(f"‚úÖ Test {location} report sent to {recipient}")
                except Exception as e:
                    logging.error(f"‚ùå Failed to send {location} report to {recipient}: {str(e)}")
        
        return JSONResponse({
            "ok": True,
            "message": f"2 emails de teste enviados com sucesso! (dados reais de hoje 07h00)",
            "recipients": report_recipients,
            "sent": sent_count,
            "details": sent_details,
            "messageIds": message_ids,
            "totalResults": len(search_data['results'])
        })
        
    except Exception as e:
        logging.error(f"Test daily report error: {str(e)}")
        return JSONResponse({
            "ok": False,
            "error": f"Erro ao enviar email: {str(e)}"
        }, status_code=500)

@app.post("/api/reports/test-alert")
async def test_alert_email(request: Request):
    """Send test alert email with sample data"""
    require_auth(request)
    
    try:
        from googleapiclient.discovery import build
        from google.oauth2.credentials import Credentials
        import base64
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        
        # Get complete Gmail credentials
        credentials = _get_gmail_credentials()
        
        if not credentials:
            return JSONResponse({
                "ok": False,
                "error": "Token OAuth n√£o encontrado. Por favor, conecte sua conta Gmail primeiro."
            })
        
        # Usar email de configura√ß√£o como destinat√°rio
        report_recipients = [_get_setting("report_email", "carlpac82@hotmail.com")]
        
        # Build Gmail service with complete credentials
        service = build('gmail', 'v1', credentials=credentials)
        
        # Send to each recipient
        sent_count = 0
        
        for recipient in report_recipients:
            try:
                # Create HTML email with alert sample
                message = MIMEMultipart('alternative')
                message['to'] = recipient
                message['subject'] = f'üö® Alerta de Pre√ßos - Auto Prudente ({datetime.now().strftime("%d/%m/%Y")})'
                
                html_content = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <meta charset="UTF-8">
                </head>
                <body style="font-family: 'Segoe UI', sans-serif; background: #f8fafc; padding: 20px;">
                    <div style="max-width: 600px; margin: 0 auto; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%); padding: 30px; text-align: center;">
                            <h1 style="margin: 0; color: white; font-size: 24px;">üö® Alerta de Pre√ßos</h1>
                            <p style="margin: 8px 0 0 0; color: #fee2e2; font-size: 14px;">{datetime.now().strftime('%d/%m/%Y √†s %H:%M')}</p>
                        </div>
                        <div style="padding: 30px;">
                            <h2 style="color: #ef4444; margin: 0 0 20px 0;">‚ö†Ô∏è Mudan√ßas Significativas Detectadas</h2>
                            <p style="color: #64748b; font-size: 16px; line-height: 1.6; margin: 0 0 20px 0;">
                                Este √© um email de teste do sistema de alertas autom√°ticos.<br>
                                O sistema detecta mudan√ßas significativas de pre√ßos e envia notifica√ß√µes.
                            </p>
                            
                            <div style="background: #fef2f2; border-left: 4px solid #ef4444; padding: 15px; margin-bottom: 15px; border-radius: 4px;">
                                <div style="font-weight: bold; color: #991b1b; margin-bottom: 5px;">BMW 3 Series (Grupo J2) - Faro</div>
                                <div style="color: #7f1d1d; font-size: 14px;">
                                    ‚Ç¨78.00 ‚Üí ‚Ç¨95.00 <span style="color: #ef4444; font-weight: bold;">(+21.8%)</span>
                                </div>
                            </div>
                            
                            <div style="background: #fef2f2; border-left: 4px solid #ef4444; padding: 15px; margin-bottom: 15px; border-radius: 4px;">
                                <div style="font-weight: bold; color: #991b1b; margin-bottom: 5px;">Mercedes C-Class (Grupo J2) - Albufeira</div>
                                <div style="color: #7f1d1d; font-size: 14px;">
                                    ‚Ç¨85.00 ‚Üí ‚Ç¨105.00 <span style="color: #ef4444; font-weight: bold;">(+23.5%)</span>
                                </div>
                            </div>
                            
                            <div style="background: #f0f9fb; border-left: 4px solid #009cb6; padding: 15px; border-radius: 4px;">
                                <p style="margin: 0; color: #1e293b; font-size: 14px;">
                                    <strong style="color: #009cb6;">Informa√ß√µes:</strong><br>
                                    ‚Ä¢ Alertas configurados para mudan√ßas >10%<br>
                                    ‚Ä¢ Verifica√ß√£o autom√°tica di√°ria<br>
                                    ‚Ä¢ Configure limites em Price Validation
                                </p>
                            </div>
                        </div>
                        <div style="background: #f8fafc; padding: 20px; text-align: center; border-top: 1px solid #e2e8f0;">
                            <p style="margin: 0; font-size: 12px; color: #94a3b8;">
                                Auto Prudente ¬© {datetime.now().year} - Sistema de Monitoriza√ß√£o de Pre√ßos
                            </p>
                        </div>
                    </div>
                </body>
                </html>
                """
                
                html_part = MIMEText(html_content, 'html')
                message.attach(html_part)
                
                # Encode and send
                raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
                send_message = service.users().messages().send(
                    userId='me',
                    body={'raw': raw_message}
                ).execute()
                
                sent_count += 1
                logging.info(f"‚úÖ Test alert email sent to {recipient}")
                
            except Exception as e:
                logging.error(f"‚ùå Failed to send alert to {recipient}: {str(e)}")
        
        return JSONResponse({
            "ok": True,
            "message": f"Email de alerta enviado com sucesso para {sent_count} destinat√°rio(s)!",
            "sent": sent_count,
            "recipients": report_recipients
        })
        
    except Exception as e:
        logging.error(f"Test alert email error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/reports/run-now")
async def run_report_now(request: Request):
    """Execute search and email report IMMEDIATELY"""
    require_auth(request)
    
    try:
        username = request.session.get('username')
        logging.info(f"‚ö° IMMEDIATE REPORT RUN requested by {username}")
        
        # Step 1: Run search
        logging.info("üîç Step 1: Running search...")
        try:
            await run_daily_report_search()
            logging.info("‚úÖ Search completed")
        except Exception as e:
            logging.error(f"‚ùå Search failed: {str(e)}")
            return JSONResponse({
                "ok": False,
                "error": f"Pesquisa falhou: {str(e)}"
            }, status_code=500)
        
        # Step 2: Send email report
        logging.info("üìß Step 2: Sending email...")
        try:
            await send_automatic_daily_report()
            logging.info("‚úÖ Email sent")
        except Exception as e:
            logging.error(f"‚ùå Email failed: {str(e)}")
            return JSONResponse({
                "ok": False,
                "error": f"Email falhou: {str(e)}"
            }, status_code=500)
        
        return JSONResponse({
            "ok": True,
            "message": "Pesquisa executada e email enviado com sucesso!"
        })
        
    except Exception as e:
        logging.error(f"Run report now error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/scheduler/jobs")
async def get_scheduler_jobs(request: Request):
    """Get list of active scheduler jobs"""
    require_auth(request)
    
    try:
        from datetime import datetime
        import pytz
        
        # Get scheduler instance
        try:
            from apscheduler.schedulers.background import BackgroundScheduler
            # Scheduler is global, defined at startup
            jobs_info = []
            
            if 'scheduler' in globals():
                scheduler = globals()['scheduler']
                jobs = scheduler.get_jobs()
                
                lisbon_tz = pytz.timezone('Europe/Lisbon')
                
                for job in jobs:
                    next_run = job.next_run_time
                    if next_run:
                        # Convert to Lisbon timezone
                        next_run_lisbon = next_run.astimezone(lisbon_tz)
                        next_run_str = next_run_lisbon.strftime('%Y-%m-%d %H:%M:%S %Z')
                    else:
                        next_run_str = 'N/A'
                    
                    jobs_info.append({
                        'id': job.id,
                        'name': job.name,
                        'next_run': next_run_str,
                        'trigger': str(job.trigger)
                    })
                
                return JSONResponse({
                    "ok": True,
                    "jobs_count": len(jobs_info),
                    "jobs": jobs_info,
                    "scheduler_running": scheduler.running,
                    "current_time_lisbon": datetime.now(lisbon_tz).strftime('%Y-%m-%d %H:%M:%S %Z')
                })
            else:
                return JSONResponse({
                    "ok": False,
                    "error": "Scheduler not initialized"
                })
        except Exception as e:
            return JSONResponse({
                "ok": False,
                "error": f"Scheduler error: {str(e)}"
            })
    except Exception as e:
        logging.error(f"Get scheduler jobs error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# RENDER CRON JOB ENDPOINTS (FREE!)
# ============================================================

def verify_cron_secret(request: Request) -> bool:
    """Verify X-Cron-Secret header matches CRON_SECRET_KEY or SECRET_KEY (fallback)"""
    cron_secret = request.headers.get('X-Cron-Secret', '')
    
    # Try CRON_SECRET_KEY first, fallback to SECRET_KEY (j√° existe no environment)
    expected_secret = os.environ.get('CRON_SECRET_KEY') or os.environ.get('SECRET_KEY', '')
    
    if not expected_secret:
        logging.warning("‚ö†Ô∏è Neither CRON_SECRET_KEY nor SECRET_KEY found in environment")
        return False
    
    if not cron_secret:
        logging.warning("‚ö†Ô∏è No X-Cron-Secret header provided in request")
        return False
    
    is_valid = cron_secret == expected_secret
    if not is_valid:
        logging.error(f"‚ùå Invalid cron secret provided (expected starts with: {expected_secret[:8]}...)")
    
    return is_valid

@app.post("/api/cron/backup")
async def cron_backup(request: Request):
    """Triggered by Render Cron Job - Daily backup at 3 AM"""
    if not verify_cron_secret(request):
        logging.error("‚ùå Unauthorized cron job attempt - invalid secret")
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    print("\n" + "="*80)
    print("üîÑ CRON JOB: Daily Backup")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Run backup in background thread
    import threading
    thread = threading.Thread(target=create_automatic_backup)
    thread.daemon = True
    thread.start()
    
    return JSONResponse({"ok": True, "message": "Backup started"})

@app.post("/api/cron/daily-search")
async def cron_daily_search(request: Request):
    """Triggered by Render Cron Job - Daily report search at 7 AM"""
    if not verify_cron_secret(request):
        logging.error("‚ùå Unauthorized cron job attempt - invalid secret")
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    print("\n" + "="*80)
    print("üîÑ CRON JOB: Daily Report Search")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Run search in background thread
    import threading
    thread = threading.Thread(target=run_daily_report_search)
    thread.daemon = True
    thread.start()
    
    return JSONResponse({"ok": True, "message": "Daily search started"})

@app.post("/api/cron/daily-report")
async def cron_daily_report(request: Request):
    """Triggered by Render Cron Job - Daily report email at 9 AM"""
    if not verify_cron_secret(request):
        logging.error("‚ùå Unauthorized cron job attempt - invalid secret")
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    print("\n" + "="*80)
    print("üîÑ CRON JOB: Daily Report Email")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Run report in background thread
    import threading
    thread = threading.Thread(target=send_automatic_daily_report)
    thread.daemon = True
    thread.start()
    
    return JSONResponse({"ok": True, "message": "Daily report started"})

@app.post("/api/cron/weekly-search")
async def cron_weekly_search(request: Request):
    """Triggered by Render Cron Job - Weekly report search on Monday at 7 AM"""
    if not verify_cron_secret(request):
        logging.error("‚ùå Unauthorized cron job attempt - invalid secret")
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    print("\n" + "="*80)
    print("üîÑ CRON JOB: Weekly Report Search")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Run search in background thread
    import threading
    thread = threading.Thread(target=run_weekly_report_search)
    thread.daemon = True
    thread.start()
    
    return JSONResponse({"ok": True, "message": "Weekly search started"})

@app.post("/api/cron/weekly-report")
async def cron_weekly_report(request: Request):
    """Triggered by Render Cron Job - Weekly report email on Monday at 9 AM"""
    if not verify_cron_secret(request):
        logging.error("‚ùå Unauthorized cron job attempt - invalid secret")
        return JSONResponse({"ok": False, "error": "Unauthorized"}, status_code=401)
    
    print("\n" + "="*80)
    print("üîÑ CRON JOB: Weekly Report Email")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Run report in background thread
    import threading
    thread = threading.Thread(target=send_automatic_weekly_report)
    thread.daemon = True
    thread.start()
    
    return JSONResponse({"ok": True, "message": "Weekly report started"})

@app.post("/api/backup/create")
async def create_backup(request: Request):
    """Create system backup"""
    require_auth(request)
    
    try:
        import zipfile
        import shutil
        from datetime import datetime
        
        data = await request.json()
        logging.info(f"Backup requested with options: {data}")
        
        # Create backup directory if not exists
        backup_dir = Path("backups")
        backup_dir.mkdir(exist_ok=True)
        
        # Generate backup filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"backup_{timestamp}.zip"
        backup_path = backup_dir / backup_filename
        
        # Create ZIP file
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # 1. All Databases (SQLite local)
            if data.get('database', True):
                db_files = ["rental_tracker.db", "data.db", "car_images.db", "carrental.db"]
                for db_file in db_files:
                    db_path = Path(db_file)
                    if db_path.exists() and db_path.stat().st_size > 0:
                        zipf.write(db_path, f"database/{db_file}")
                        size_kb = db_path.stat().st_size / 1024
                        logging.info(f"‚úÖ Database {db_file} added to backup ({size_kb:.1f} KB)")
                
                # 1.1. PostgreSQL Backup (if in production)
                if _USE_NEW_DB and USE_POSTGRES:
                    try:
                        import subprocess
                        pg_backup_file = f"postgres_backup_{timestamp}.sql"
                        pg_backup_path = backup_dir / pg_backup_file
                        
                        # Get DATABASE_URL
                        db_url = os.getenv("DATABASE_URL")
                        if db_url:
                            logging.info("üêò Creating PostgreSQL backup...")
                            result = subprocess.run(
                                ["pg_dump", db_url],
                                capture_output=True,
                                text=True,
                                timeout=300  # 5 minutes timeout
                            )
                            
                            if result.returncode == 0:
                                with open(pg_backup_path, 'w') as f:
                                    f.write(result.stdout)
                                
                                zipf.write(pg_backup_path, f"database/{pg_backup_file}")
                                size_mb = pg_backup_path.stat().st_size / (1024 * 1024)
                                logging.info(f"‚úÖ PostgreSQL backup added ({size_mb:.2f} MB)")
                                
                                # Remove temp file
                                pg_backup_path.unlink()
                            else:
                                logging.error(f"‚ùå PostgreSQL backup failed: {result.stderr}")
                    except Exception as e:
                        logging.error(f"‚ùå PostgreSQL backup error: {e}")
            
            # 2. Settings (localStorage data stored in DB)
            if data.get('settings', True):
                # Settings are in localStorage, backed up with database
                pass
            
            # 3. Vehicle mappings (in database)
            if data.get('vehicles', True):
                # Vehicle data is in database
                pass
            
            # 4. Uploaded files (logos, profile pictures)
            if data.get('uploads', True):
                uploads_dir = Path("uploads")
                if uploads_dir.exists():
                    for file_path in uploads_dir.rglob("*"):
                        if file_path.is_file():
                            arcname = f"uploads/{file_path.relative_to(uploads_dir)}"
                            zipf.write(file_path, arcname)
                    logging.info("‚úÖ Uploads added to backup")
            
            # 5. ALL Static files
            static_dir = Path("static")
            if static_dir.exists():
                for file_path in static_dir.rglob("*"):
                    if file_path.is_file():
                        arcname = f"static/{file_path.relative_to(static_dir)}"
                        zipf.write(file_path, arcname)
                logging.info("‚úÖ All static files added to backup")
            
            # 6. ALL Templates
            templates_dir = Path("templates")
            if templates_dir.exists():
                for file_path in templates_dir.rglob("*"):
                    if file_path.is_file():
                        arcname = f"templates/{file_path.relative_to(templates_dir)}"
                        zipf.write(file_path, arcname)
                logging.info("‚úÖ All templates added to backup")
            
            # 7. Main.py and other Python files
            for py_file in Path(".").glob("*.py"):
                if py_file.is_file():
                    zipf.write(py_file, f"code/{py_file.name}")
            logging.info("‚úÖ Python files added to backup")
            
            # 8. Requirements and config files
            config_files = ["requirements.txt", "Procfile", "runtime.txt", ".gitignore"]
            for config_file in config_files:
                config_path = Path(config_file)
                if config_path.exists():
                    zipf.write(config_path, f"config/{config_file}")
            logging.info("‚úÖ Config files added to backup")
            
            # 6. OAuth settings (if requested - sensitive!)
            if data.get('oauth', False):
                env_path = Path(".env")
                if env_path.exists():
                    zipf.write(env_path, "config/.env")
                    logging.info("‚úÖ OAuth config added to backup")
        
        # Get file size
        file_size = backup_path.stat().st_size
        size_mb = file_size / (1024 * 1024)
        
        logging.info(f"‚úÖ Backup created: {backup_filename} ({size_mb:.2f} MB)")
        
        return JSONResponse({
            "ok": True,
            "message": f"Backup criado com sucesso ({size_mb:.2f} MB)",
            "downloadUrl": f"/api/backup/download/{backup_filename}",
            "filename": backup_filename,
            "size": f"{size_mb:.2f} MB"
        })
        
    except Exception as e:
        logging.error(f"Backup creation error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/backup/download/{filename}")
async def download_backup(request: Request, filename: str):
    """Download backup file"""
    require_auth(request)
    
    try:
        from fastapi.responses import FileResponse
        
        backup_path = Path("backups") / filename
        
        if not backup_path.exists():
            raise HTTPException(status_code=404, detail="Backup not found")
        
        return FileResponse(
            path=backup_path,
            filename=filename,
            media_type="application/zip"
        )
        
    except Exception as e:
        logging.error(f"Backup download error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/backup/restore")
async def restore_backup(request: Request):
    """Restore system from backup ZIP file"""
    require_auth(request)
    
    try:
        import zipfile
        import shutil
        from fastapi import UploadFile, File
        
        # Receber ficheiro ZIP
        form = await request.form()
        backup_file = form.get("file")
        
        if not backup_file or not isinstance(backup_file, UploadFile):
            return JSONResponse({"ok": False, "error": "Nenhum ficheiro enviado"}, status_code=400)
        
        if not backup_file.filename.endswith('.zip'):
            return JSONResponse({"ok": False, "error": "Ficheiro deve ser ZIP"}, status_code=400)
        
        logging.info(f"Restore backup from: {backup_file.filename}")
        
        # Guardar ZIP temporariamente
        temp_zip = Path("backups") / f"restore_temp_{backup_file.filename}"
        temp_zip.parent.mkdir(exist_ok=True)
        
        with open(temp_zip, "wb") as f:
            content = await backup_file.read()
            f.write(content)
        
        # Extrair e restaurar
        with zipfile.ZipFile(temp_zip, 'r') as zipf:
            # 1. Restaurar base de dados
            for db_file in zipf.namelist():
                if db_file.startswith("database/") and db_file.endswith(".db"):
                    db_name = Path(db_file).name
                    target_path = DATA_DIR / db_name
                    
                    # Backup da BD atual antes de sobrescrever
                    if target_path.exists():
                        backup_current = target_path.with_suffix('.db.backup')
                        shutil.copy2(target_path, backup_current)
                        logging.info(f"Current DB backed up to {backup_current}")
                    
                    # Extrair nova BD
                    with zipf.open(db_file) as source, open(target_path, 'wb') as target:
                        shutil.copyfileobj(source, target)
                    logging.info(f"‚úÖ Database {db_name} restored")
            
            # 2. Restaurar uploads
            for upload_file in zipf.namelist():
                if upload_file.startswith("uploads/"):
                    target_path = Path(upload_file)
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    with zipf.open(upload_file) as source, open(target_path, 'wb') as target:
                        shutil.copyfileobj(source, target)
            logging.info("‚úÖ Uploads restored")
            
            # 3. Restaurar static files (opcional - pode sobrescrever c√≥digo)
            # Comentado por seguran√ßa - descomentar se necess√°rio
            # for static_file in zipf.namelist():
            #     if static_file.startswith("static/"):
            #         target_path = Path(static_file)
            #         target_path.parent.mkdir(parents=True, exist_ok=True)
            #         with zipf.open(static_file) as source, open(target_path, 'wb') as target:
            #             shutil.copyfileobj(source, target)
        
        # Limpar ficheiro tempor√°rio
        temp_zip.unlink()
        
        logging.info("‚úÖ Backup restored successfully")
        
        return JSONResponse({
            "ok": True,
            "message": "Backup restaurado com sucesso! Recarrega a p√°gina."
        })
        
    except Exception as e:
        logging.error(f"Backup restore error: {str(e)}", exc_info=True)
        return JSONResponse({"ok": False, "error": f"Erro ao restaurar backup: {str(e)}"}, status_code=500)

@app.post("/api/fix-schema-emergency")
async def fix_schema_emergency(request: Request):
    """Emergency endpoint to fix PostgreSQL schema"""
    try:
        require_auth(request)
    except:
        pass  # Allow without auth in emergency
    
    try:
        if not _USE_NEW_DB or not USE_POSTGRES:
            return JSONResponse({"ok": False, "error": "Not using PostgreSQL"})
        
        results = []
        
        columns = [
            ("first_name", "TEXT"),
            ("last_name", "TEXT"),
            ("email", "TEXT"),
            ("mobile", "TEXT"),
            ("profile_picture_path", "TEXT"),
            ("profile_picture_data", "BYTEA"),
            ("is_admin", "BOOLEAN DEFAULT FALSE"),
            ("enabled", "BOOLEAN DEFAULT TRUE"),
            ("created_at", "TEXT"),
            ("google_id", "TEXT"),
        ]
        
        with _db_lock:
            conn = _db_connect()
            
            # Se as colunas j√° existem como INTEGER, converter para BOOLEAN
            try:
                # Verificar tipo atual
                cursor = conn.execute("""
                    SELECT column_name, data_type 
                    FROM information_schema.columns 
                    WHERE table_name='users' AND column_name IN ('is_admin', 'enabled')
                """)
                existing = {row[0]: row[1] for row in cursor.fetchall()}
                
                # Converter INTEGER para BOOLEAN se necess√°rio
                if existing.get('is_admin') == 'integer':
                    conn.execute("ALTER TABLE users ALTER COLUMN is_admin TYPE BOOLEAN USING is_admin::boolean")
                    conn.commit()
                    results.append({"column": "is_admin", "status": "converted to BOOLEAN"})
                
                if existing.get('enabled') == 'integer':
                    conn.execute("ALTER TABLE users ALTER COLUMN enabled TYPE BOOLEAN USING enabled::boolean")
                    conn.commit()
                    results.append({"column": "enabled", "status": "converted to BOOLEAN"})
            except Exception as e:
                conn.rollback()
                results.append({"column": "type_conversion", "status": "error", "error": str(e)})
            try:
                for col_name, col_type in columns:
                    try:
                        conn.execute(f"ALTER TABLE users ADD COLUMN IF NOT EXISTS {col_name} {col_type}")
                        conn.commit()
                        results.append({"column": col_name, "status": "added"})
                    except Exception as e:
                        conn.rollback()
                        if "already exists" in str(e):
                            results.append({"column": col_name, "status": "exists"})
                        else:
                            results.append({"column": col_name, "status": "error", "error": str(e)})
                
                # Verify
                cursor = conn.execute("SELECT column_name FROM information_schema.columns WHERE table_name='users' ORDER BY ordinal_position")
                cols = [row[0] for row in cursor.fetchall()]
                
                return JSONResponse({
                    "ok": True,
                    "message": "Schema fix completed",
                    "results": results,
                    "total_columns": len(cols),
                    "columns": cols,
                    "enabled_exists": "enabled" in cols
                })
            finally:
                conn.close()
                
    except Exception as e:
        import traceback
        return JSONResponse({
            "ok": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }, status_code=500)

@app.get("/api/backup/list")
async def list_backups(request: Request):
    """List available backups"""
    require_auth(request)
    
    try:
        from datetime import datetime
        
        backup_dir = Path("backups")
        backups = []
        
        if backup_dir.exists():
            for backup_file in sorted(backup_dir.glob("backup_*.zip"), reverse=True):
                stat = backup_file.stat()
                size_mb = stat.st_size / (1024 * 1024)
                
                # Parse timestamp from filename
                try:
                    timestamp_str = backup_file.stem.replace("backup_", "")
                    dt = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                    date_str = dt.strftime("%d/%m/%Y %H:%M")
                except:
                    date_str = "Data desconhecida"
                
                backups.append({
                    "name": backup_file.name,
                    "date": date_str,
                    "size": f"{size_mb:.2f} MB",
                    "downloadUrl": f"/api/backup/download/{backup_file.name}"
                })
        
        return JSONResponse({
            "ok": True,
            "backups": backups
        })
        
    except Exception as e:
        logging.error(f"Backup list error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/reports/test-weekly")
async def test_weekly_report(request: Request):
    """Send test weekly report - 2 EMAILS com NOVO TEMPLATE (M√™s ‚Üí dias ‚Üí grupos)"""
    require_auth(request)
    
    try:
        from googleapiclient.discovery import build
        from google.oauth2.credentials import Credentials
        import base64
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        from improved_reports import generate_weekly_report_html_by_location
        
        # Get complete Gmail credentials
        credentials = _get_gmail_credentials()
        
        if not credentials:
            return JSONResponse({
                "ok": False,
                "error": "Token OAuth n√£o encontrado. Por favor, conecte sua conta Gmail primeiro."
            })
        
        # Usar email de configura√ß√£o como destinat√°rio
        report_recipients = [_get_setting("report_email", "carlpac82@hotmail.com")]
        
        # Load search data - try today first, then last 7 days
        search_data = {'results': []}
        with _db_lock:
            conn = _db_connect()  
            try:
                # Try today first
                cursor = conn.execute(
                    """
                    SELECT location, start_date, days, results_data, timestamp
                    FROM recent_searches
                    WHERE DATE(timestamp) = CURRENT_DATE
                    ORDER BY timestamp DESC
                    """
                )
                rows = cursor.fetchall()
                
                # If no data today, get most recent from last 7 days
                if not rows:
                    logging.info("üì≠ No data from today, fetching most recent from last 7 days...")
                    cursor = conn.execute(
                        """
                        SELECT location, start_date, days, results_data, timestamp
                        FROM recent_searches
                        WHERE timestamp >= datetime('now', '-7 days')
                        ORDER BY timestamp DESC
                        LIMIT 50
                        """
                    )
                    rows = cursor.fetchall()
                
                all_results = []
                for row in rows:
                    location, start_date, days, results_data, timestamp = row
                    if results_data:
                        results = json.loads(results_data)
                        for r in results:
                            r['days'] = days
                            r['location'] = location
                        all_results.extend(results)
                
                search_data = {'results': all_results}
                logging.info(f"üìä Found {len(all_results)} total results for weekly test report")
            finally:
                conn.close()
        
        if not search_data['results']:
            return JSONResponse({
                "ok": False,
                "error": "Sem dados de pesquisa dispon√≠veis. Execute uma pesquisa primeiro ou aguarde a pesquisa autom√°tica."
            })
        
        # Build Gmail service with complete credentials
        service = build('gmail', 'v1', credentials=credentials)
        
        # ENVIAR 2 EMAILS SEPARADOS
        locations = ['Albufeira', 'Aeroporto de Faro']
        sent_count = 0
        message_ids = []
        sent_details = []
        
        for location in locations:
            logging.info(f"\nüìç Generating weekly test report for: {location}")
            
            # Generate HTML for this location with REAL data
            html_content = generate_weekly_report_html_by_location(search_data, location)
            
            # Send to all recipients
            for recipient in report_recipients:
                try:
                    message = MIMEMultipart('alternative')
                    message['to'] = recipient
                    message['subject'] = f'üìä TESTE Relat√≥rio Semanal {location} - Auto Prudente (Semana {datetime.now().strftime("%W/%Y")})'
                    
                    html_part = MIMEText(html_content, 'html')
                    message.attach(html_part)
                    
                    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
                    send_message = service.users().messages().send(
                        userId='me',
                        body={'raw': raw_message}
                    ).execute()
                    
                    message_ids.append(send_message.get('id'))
                    sent_count += 1
                    sent_details.append(f"{location} ‚Üí {recipient}")
                    logging.info(f"‚úÖ Test weekly {location} report sent to {recipient}")
                except Exception as e:
                    logging.error(f"‚ùå Failed to send weekly {location} report to {recipient}: {str(e)}")
        
        return JSONResponse({
            "ok": True,
            "message": f"2 emails semanais de teste enviados! (M√™s ‚Üí dias ‚Üí grupos)",
            "sent": sent_count,
            "details": sent_details,
            "messageIds": message_ids,
            "recipients": report_recipients,
            "totalResults": len(search_data['results'])
        })
        
    except Exception as e:
        logging.error(f"Test weekly report error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/reports/check-automated-searches")
async def check_automated_searches(request: Request):
    """Verificar pesquisas autom√°ticas configuradas e identificar testes"""
    require_auth(request)
    
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                
                if not row or not row[0]:
                    return JSONResponse({
                        "ok": True,
                        "message": "Nenhuma configura√ß√£o de relat√≥rios autom√°ticos encontrada",
                        "settings": None
                    })
                
                settings = json.loads(row[0])
                
                # Analisar configura√ß√µes
                analysis = {
                    "dailyEnabled": settings.get('dailyEnabled', False),
                    "weeklyEnabled": settings.get('weeklyEnabled', False),
                    "dailyTime": settings.get('dailyTime', '09:00'),
                    "weeklyDay": settings.get('weeklyDay', 'monday'),
                    "weeklyTime": settings.get('weeklyTime', '09:00'),
                    "isTest": False  # Determinar se s√£o configura√ß√µes de teste
                }
                
                # Detectar se √© teste (hor√°rios at√≠picos ou muito frequentes)
                daily_hour = int(analysis['dailyTime'].split(':')[0])
                weekly_hour = int(analysis['weeklyTime'].split(':')[0])
                
                # Considerar TESTE se:
                # - Hora di√°ria n√£o √© 9h (oficial)
                # - Hora semanal n√£o √© 9h (oficial)
                # - Dia semanal n√£o √© segunda (oficial)
                if (daily_hour != 9 and settings.get('dailyEnabled')) or \
                   (weekly_hour != 9 and settings.get('weeklyEnabled')) or \
                   (settings.get('weeklyDay', '').lower() not in ['monday', 'segunda'] and settings.get('weeklyEnabled')):
                    analysis['isTest'] = True
                    analysis['testReason'] = f"Hor√°rios at√≠picos: Daily={analysis['dailyTime']}, Weekly={analysis['weeklyTime']} ({analysis['weeklyDay']})"
                
                return JSONResponse({
                    "ok": True,
                    "settings": settings,
                    "analysis": analysis,
                    "recommendation": "Desativar testes e usar apenas hor√°rios oficiais (9h)" if analysis['isTest'] else "Configura√ß√µes parecem oficiais"
                })
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"Check automated searches error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/reports/reset-to-official")
async def reset_to_official_settings(request: Request):
    """Resetar para configura√ß√µes oficiais (apenas pesquisas programadas √†s 9h)"""
    require_auth(request)
    
    try:
        # Configura√ß√µes OFICIAIS (sem testes)
        official_settings = {
            "dailyEnabled": True,
            "weeklyEnabled": True,
            "dailyTime": "09:00",  # 9h - Oficial
            "weeklyDay": "monday",  # Segunda - Oficial
            "weeklyTime": "09:00"   # 9h - Oficial
        }
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Atualizar configura√ß√µes
                conn.execute(
                    "UPDATE price_automation_settings SET setting_value = ? WHERE setting_key = 'automatedReportsSettings'",
                    (json.dumps(official_settings),)
                )
                
                # Se n√£o existir, inserir
                if conn.total_changes == 0:
                    conn.execute(
                        "INSERT INTO price_automation_settings (setting_key, setting_value) VALUES (?, ?)",
                        ('automatedReportsSettings', json.dumps(official_settings))
                    )
                
                conn.commit()
                
                logging.info("‚úÖ Reset to official settings: Daily 9h, Weekly Monday 9h")
                
                return JSONResponse({
                    "ok": True,
                    "message": "‚úÖ Configura√ß√µes resetadas para oficiais!",
                    "settings": official_settings,
                    "schedule": {
                        "daily": "Todos os dias √†s 9h",
                        "weekly": "Segundas-feiras √†s 9h"
                    }
                })
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"Reset to official error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/email/test-oauth")
async def test_email_oauth(request: Request):
    """Send test email using OAuth token"""
    require_auth(request)
    
    try:
        from googleapiclient.discovery import build
        from google.oauth2.credentials import Credentials
        import base64
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        
        data = await request.json()
        provider = data.get('provider')
        email = data.get('email')
        recipients = data.get('recipients', '')
        
        # Get complete Gmail credentials
        credentials = _get_gmail_credentials()
        
        if not credentials:
            return JSONResponse({"ok": False, "error": "Token OAuth n√£o encontrado. Conecte Gmail primeiro."})
        
        # Parse recipients (one per line)
        recipient_list = [r.strip() for r in recipients.split('\n') if r.strip()]
        
        if not recipient_list:
            return JSONResponse({"ok": False, "error": "Nenhum destinat√°rio especificado"})
        
        # Build Gmail service with complete credentials
        service = build('gmail', 'v1', credentials=credentials)
        
        # Send to each recipient
        sent_count = 0
        errors = []
        
        for recipient in recipient_list:
            try:
                # Create HTML email
                message = MIMEMultipart('alternative')
                message['to'] = recipient
                message['subject'] = f'üìß Email de Teste - Auto Prudente ({datetime.now().strftime("%d/%m/%Y %H:%M")})'
                
                html_content = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <meta charset="UTF-8">
                </head>
                <body style="font-family: 'Segoe UI', sans-serif; background: #f8fafc; padding: 20px;">
                    <div style="max-width: 600px; margin: 0 auto; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="background: linear-gradient(135deg, #009cb6 0%, #007a91 100%); padding: 30px; text-align: center;">
                            <h1 style="margin: 0; color: white; font-size: 24px;">‚úÖ Email de Teste</h1>
                            <p style="margin: 8px 0 0 0; color: #e0f2f7; font-size: 14px;">{datetime.now().strftime('%d/%m/%Y √†s %H:%M')}</p>
                        </div>
                        <div style="padding: 30px; text-align: center;">
                            <h2 style="color: #009cb6; margin: 0 0 20px 0;">üéâ Sistema de Email Funcionando!</h2>
                            <p style="color: #64748b; font-size: 16px; line-height: 1.6; margin: 0 0 20px 0;">
                                Este √© um email de teste do sistema de notifica√ß√µes autom√°ticas da Auto Prudente.<br>
                                O sistema est√° configurado e pronto para enviar relat√≥rios e alertas.
                            </p>
                            <div style="background: #f0f9fb; border-left: 4px solid #009cb6; padding: 15px; text-align: left; border-radius: 4px;">
                                <p style="margin: 0; color: #1e293b; font-size: 14px;">
                                    <strong style="color: #009cb6;">Informa√ß√µes:</strong><br>
                                    ‚Ä¢ Enviado via Gmail OAuth<br>
                                    ‚Ä¢ Sistema de relat√≥rios autom√°ticos ativo<br>
                                    ‚Ä¢ Notifica√ß√µes de alertas configuradas
                                </p>
                            </div>
                        </div>
                        <div style="background: #f8fafc; padding: 20px; text-align: center; border-top: 1px solid #e2e8f0;">
                            <p style="margin: 0; font-size: 12px; color: #94a3b8;">
                                Auto Prudente ¬© {datetime.now().year} - Sistema de Monitoriza√ß√£o de Pre√ßos
                            </p>
                        </div>
                    </div>
                </body>
                </html>
                """
                
                html_part = MIMEText(html_content, 'html')
                message.attach(html_part)
                
                # Encode and send
                raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
                send_message = service.users().messages().send(
                    userId='me',
                    body={'raw': raw_message}
                ).execute()
                
                sent_count += 1
                logging.info(f"‚úÖ Test email sent to {recipient}")
                
            except Exception as e:
                error_msg = f"{recipient}: {str(e)}"
                errors.append(error_msg)
                logging.error(f"‚ùå Failed to send to {recipient}: {str(e)}")
        
        if sent_count > 0:
            message = f"Email enviado com sucesso para {sent_count} destinat√°rio(s)!"
            if errors:
                message += f" ({len(errors)} falhou)"
            return JSONResponse({
                "ok": True,
                "message": message,
                "sent": sent_count,
                "errors": errors if errors else None
            })
        else:
            return JSONResponse({
                "ok": False,
                "error": "Nenhum email foi enviado",
                "errors": errors
            }, status_code=500)
        
    except Exception as e:
        logging.error(f"Test email error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/reports/daily/preview")
async def preview_daily_report(request: Request):
    """Preview daily report HTML (for testing)"""
    require_auth(request)
    
    try:
        # Load recent search data from database
        search_data = None
        with _db_lock:
            conn = _db_connect()  
            try:
                cursor = conn.execute(
                    """
                    SELECT location, start_date, days, results_data
                    FROM recent_searches
                    ORDER BY timestamp DESC
                    LIMIT 1
                    """
                )
                row = cursor.fetchone()
                if row:
                    search_data = {
                        'location': row[0],
                        'date': row[1],
                        'days': row[2],
                        'results': json.loads(row[3]) if row[3] else []
                    }
            finally:
                conn.close()
        
        # Generate HTML report
        html_content = generate_daily_report_html(search_data)
        
        # Return HTML directly for preview
        from starlette.responses import HTMLResponse
        return HTMLResponse(content=html_content)
        
    except Exception as e:
        logging.error(f"Preview daily report error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/reports/weekly/preview")
async def preview_weekly_report(request: Request):
    """Preview weekly report HTML (for testing)"""
    require_auth(request)
    
    try:
        from datetime import datetime, timedelta
        
        # Analyze NEXT 3 months (same logic as automatic report)
        months_to_analyze = 3
        months_data = []
        
        with _db_lock:
            conn = _db_connect()
            try:
                today = datetime.now()
                
                # For each of the next 3 months
                for month_offset in range(1, months_to_analyze + 1):
                    target_date = today + timedelta(days=30 * month_offset)
                    month_name = target_date.strftime('%B de %Y')
                    
                    # Get most recent search for dates in this month range
                    month_start = target_date.replace(day=1).strftime('%Y-%m-%d')
                    if target_date.month == 12:
                        month_end = target_date.replace(year=target_date.year + 1, month=1, day=1).strftime('%Y-%m-%d')
                    else:
                        month_end = target_date.replace(month=target_date.month + 1, day=1).strftime('%Y-%m-%d')
                    
                    cursor = conn.execute(
                        """
                        SELECT results_data, timestamp
                        FROM recent_searches
                        WHERE start_date >= ? AND start_date < ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                        """,
                        (month_start, month_end)
                    )
                    
                    row = cursor.fetchone()
                    if not row or not row[0]:
                        months_data.append({
                            'name': month_name,
                            'best_price_count': 0,
                            'competitive_count': 0,
                            'percentage': 0,
                            'has_data': False
                        })
                        continue
                    
                    results = json.loads(row[0])
                    search_time = row[1]
                    
                    groups = {}
                    for car in results:
                        group = car.get('group', 'Unknown')
                        if group not in groups:
                            groups[group] = []
                        groups[group].append(car)
                    
                    best_price_count = 0
                    competitive_count = 0
                    total_groups = len(groups)
                    
                    for group, cars in groups.items():
                        sorted_cars = sorted(cars, key=lambda x: float(x.get('price_num', 999999)))
                        
                        ap_position = None
                        for idx, car in enumerate(sorted_cars, 1):
                            supplier = (car.get('supplier', '') or '').lower()
                            if 'autoprudente' in supplier or 'auto prudente' in supplier:
                                ap_position = idx
                                break
                        
                        if ap_position == 1:
                            best_price_count += 1
                        elif ap_position and ap_position <= 3:
                            competitive_count += 1
                    
                    # Get previous week data for comparison
                    cursor_prev = conn.execute(
                        """
                        SELECT results_data
                        FROM recent_searches
                        WHERE start_date >= ? AND start_date < ?
                        AND timestamp < ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                        """,
                        (month_start, month_end, search_time)
                    )
                    
                    prev_row = cursor_prev.fetchone()
                    prev_best = None
                    if prev_row and prev_row[0]:
                        prev_results = json.loads(prev_row[0])
                        prev_groups = {}
                        for car in prev_results:
                            g = car.get('group', 'Unknown')
                            if g not in prev_groups:
                                prev_groups[g] = []
                            prev_groups[g].append(car)
                        
                        prev_best = 0
                        for g, cars in prev_groups.items():
                            sorted_c = sorted(cars, key=lambda x: float(x.get('price_num', 999999)))
                            for idx, car in enumerate(sorted_c, 1):
                                supplier = (car.get('supplier', '') or '').lower()
                                if 'autoprudente' in supplier or 'auto prudente' in supplier:
                                    if idx == 1:
                                        prev_best += 1
                                    break
                    
                    months_data.append({
                        'name': month_name,
                        'best_price_count': best_price_count,
                        'competitive_count': competitive_count,
                        'percentage': (best_price_count / total_groups * 100) if total_groups > 0 else 0,
                        'has_data': True,
                        'prev_best': prev_best,
                        'change': best_price_count - prev_best if prev_best is not None else None
                    })
                    
            finally:
                conn.close()
        
        # Generate HTML report
        html_content = generate_weekly_report_html(months_data)
        
        # Return HTML directly for preview
        from starlette.responses import HTMLResponse
        return HTMLResponse(content=html_content)
        
    except Exception as e:
        logging.error(f"Preview weekly report error: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/oauth/outlook/authorize")
async def oauth_outlook_authorize(request: Request):
    """Initiate Outlook OAuth2 flow"""
    require_auth(request)
    
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Outlook OAuth</title>
        <style>
            body {
                font-family: 'Outfit', sans-serif;
                display: flex;
                align-items: center;
                justify-content: center;
                height: 100vh;
                margin: 0;
                background: #f0f9fb;
            }
            .container {
                text-align: center;
                padding: 2rem;
                background: white;
                border-radius: 8px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            }
            .icon {
                width: 64px;
                height: 64px;
                margin: 0 auto 1rem;
                color: #009cb6;
            }
            h1 {
                color: #009cb6;
                margin-bottom: 1rem;
            }
            p {
                color: #666;
                margin-bottom: 1.5rem;
            }
            button {
                background: #009cb6;
                color: white;
                border: none;
                padding: 12px 24px;
                border-radius: 6px;
                cursor: pointer;
                font-size: 16px;
            }
            button:hover {
                background: #007a91;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <svg class="icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"/>
            </svg>
            <h1>Conectar Outlook</h1>
            <p>Esta √© uma <strong>demonstra√ß√£o</strong> do fluxo OAuth2.</p>
            <p style="font-size: 14px; color: #666; margin: 1rem 0;">Para conectar ao Outlook real, √© necess√°rio:</p>
            <ul style="text-align: left; font-size: 13px; color: #666; margin: 0 auto; max-width: 400px; line-height: 1.8;">
                <li>Registar app no <a href="https://portal.azure.com" target="_blank" style="color: #009cb6;">Azure Portal</a></li>
                <li>Obter Application ID e Secret</li>
                <li>Configurar Microsoft OAuth2 redirect URLs</li>
                <li>Implementar fluxo OAuth completo</li>
            </ul>
            <p style="font-size: 12px; color: #999; margin-top: 1rem;">Por agora, clique abaixo para simular a conex√£o:</p>
            <button onclick="simulateOAuth()">Simular Conex√£o Outlook</button>
        </div>
        <script>
            function simulateOAuth() {
                const data = {
                    type: 'oauth-success',
                    provider: 'outlook',
                    email: 'seu-email@outlook.com',
                    token: 'mock_access_token_' + Date.now(),
                    refreshToken: 'mock_refresh_token',
                    expiresAt: Date.now() + 3600000
                };
                
                if (window.opener) {
                    window.opener.postMessage(data, '*');
                    window.close();
                } else {
                    alert('Erro: Janela pai n√£o encontrada');
                }
            }
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html)

# ============================================================
# AUTOMATIC BACKUPS SCHEDULER
# ============================================================

def create_automatic_backup():
    """Criar backup autom√°tico do sistema"""
    try:
        import zipfile
        from pathlib import Path
        from datetime import datetime
        
        # Criar diret√≥rio de backups
        backup_dir = Path("backups")
        backup_dir.mkdir(exist_ok=True)
        
        # Nome do backup com timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"auto_backup_{timestamp}.zip"
        backup_path = backup_dir / backup_filename
        
        # Criar ZIP
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Databases
            db_files = ["rental_tracker.db", "data.db", "car_images.db", "carrental.db"]
            for db_file in db_files:
                db_path = Path(db_file)
                if db_path.exists() and db_path.stat().st_size > 0:
                    zipf.write(db_path, f"database/{db_file}")
            
            # Settings
            if Path("app_settings.json").exists():
                zipf.write("app_settings.json", "settings/app_settings.json")
        
        file_size = backup_path.stat().st_size / (1024 * 1024)
        log_to_db("INFO", f"Automatic backup created: {backup_filename} ({file_size:.2f} MB)", "main", "create_automatic_backup")
        
        # Limpar backups antigos (manter √∫ltimos 7)
        cleanup_old_backups(backup_dir, keep_last=7)
        
        return True
    except Exception as e:
        log_to_db("ERROR", f"Automatic backup failed: {str(e)}", "main", "create_automatic_backup")
        return False

def cleanup_old_backups(backup_dir: Path, keep_last: int = 7):
    """Limpar backups antigos, mantendo apenas os √∫ltimos N"""
    try:
        backups = sorted(backup_dir.glob("auto_backup_*.zip"), key=lambda p: p.stat().st_mtime, reverse=True)
        for old_backup in backups[keep_last:]:
            old_backup.unlink()
            log_to_db("INFO", f"Old backup deleted: {old_backup.name}", "main", "cleanup_old_backups")
    except Exception as e:
        log_to_db("ERROR", f"Cleanup old backups failed: {str(e)}", "main", "cleanup_old_backups")

# ============================================================
# AUTOMATED REPORTS FUNCTIONS
# ============================================================

def generate_daily_report_html(search_data):
    """Generate visual HTML report with car pricing analysis"""
    from datetime import datetime
    
    # SVG Icons (monocrom√°ticos)
    icon_chart = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M3 13h2v8H3v-8zm4-6h2v14H7V7zm4-4h2v18h-2V3zm4 8h2v10h-2V11z"/></svg>'
    icon_trophy = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M20 7h-2V5c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v3c0 2.5 1.5 4.7 3.8 5.7.5 1.7 1.8 3 3.5 3.7V23h5v-1.6c1.7-.7 3-2 3.5-3.7 2.3-1 3.8-3.2 3.8-5.7V9c0-1.1-.9-2-2-2zm0 5c0 1.9-1.2 3.5-2.9 4.1-.2-1.3-.8-2.4-1.7-3.3l-1.4 1.4c.6.6 1 1.5 1 2.4 0 1.9-1.6 3.5-3.5 3.5S8 18.5 8 16.6c0-.9.4-1.8 1-2.4L7.6 12.8c-.9.9-1.5 2-1.7 3.3C4.2 15.5 3 13.9 3 12V9h3V5h12v4h3v3z"/></svg>'
    icon_car = '<svg width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M18.92 6.01C18.72 5.42 18.16 5 17.5 5h-11c-.66 0-1.21.42-1.42 1.01L3 12v8c0 .55.45 1 1 1h1c.55 0 1-.45 1-1v-1h12v1c0 .55.45 1 1 1h1c.55 0 1-.45 1-1v-8l-2.08-5.99zM6.5 16c-.83 0-1.5-.67-1.5-1.5S5.67 13 6.5 13s1.5.67 1.5 1.5S7.33 16 6.5 16zm11 0c-.83 0-1.5-.67-1.5-1.5s.67-1.5 1.5-1.5 1.5.67 1.5 1.5-.67 1.5-1.5 1.5zM5 11l1.5-4.5h11L19 11H5z"/></svg>'
    icon_location = '<svg width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M12 2C8.13 2 5 5.13 5 9c0 5.25 7 13 7 13s7-7.75 7-13c0-3.87-3.13-7-7-7zm0 9.5c-1.38 0-2.5-1.12-2.5-2.5s1.12-2.5 2.5-2.5 2.5 1.12 2.5 2.5-1.12 2.5-2.5 2.5z"/></svg>'
    icon_warning = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M1 21h22L12 2 1 21zm12-3h-2v-2h2v2zm0-4h-2v-4h2v4z"/></svg>'
    
    if not search_data or not search_data.get('results'):
        return f"""
        <!DOCTYPE html>
        <html>
        <head><meta charset="UTF-8"><title>Relat√≥rio Di√°rio</title></head>
        <body style="margin: 0; padding: 0; background-color: #f8fafc; font-family: 'Outfit', 'Segoe UI', sans-serif;">
            <table width="100%" cellpadding="0" cellspacing="0" style="background-color: #f8fafc; padding: 20px 0;">
                <tr><td align="center">
                    <table width="700" cellpadding="0" cellspacing="0" style="background-color: #ffffff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <tr><td style="background: #009cb6; padding: 30px; text-align: center;">
                            <img src="https://carrental-api-5r6g.onrender.com/static/logo.svg" alt="Auto Prudente" style="height: 60px; margin-bottom: 15px;" />
                            <h1 style="margin: 10px 0 0 0; color: #ffffff; font-size: 26px; font-weight: 600;">Relat√≥rio Di√°rio de Pre√ßos</h1>
                            <p style="margin: 5px 0 0 0; color: #e0f7fa; font-size: 15px;">{datetime.now().strftime('%d de %B de %Y')}</p>
                        </td></tr>
                        <tr><td style="padding: 40px; text-align: center;">
                            <div style="display: inline-block; padding: 20px; background: #fef3c7; border-left: 4px solid #f59e0b;">
                                {icon_warning}
                                <p style="color: #92400e; font-size: 16px; margin: 10px 0 0 0; font-weight: 500;">Sem dados de pesquisa dispon√≠veis</p>
                            </div>
                            <p style="color: #94a3b8; font-size: 14px; margin-top: 15px;">Execute uma pesquisa para gerar relat√≥rios com an√°lise de pre√ßos</p>
                        </td></tr>
                    </table>
                </td></tr>
            </table>
        </body>
        </html>
        """
    
    # Analyze car data by groups
    results = search_data['results']
    groups = {}
    
    for car in results:
        group = car.get('group', 'Unknown')
        if group not in groups:
            groups[group] = []
        groups[group].append(car)
    
    # Generate car cards HTML
    car_cards_html = ""
    total_groups = 0
    ap_competitive = 0
    ap_best_price = 0
    
    for group, cars in sorted(groups.items()):
        total_groups += 1
        
        # Sort cars by price
        sorted_cars = sorted(cars, key=lambda x: float(x.get('price_num', 999999)))
        
        # Find Auto Prudente position
        ap_car = None
        ap_position = None
        for idx, car in enumerate(sorted_cars, 1):
            supplier = (car.get('supplier', '') or '').lower()
            if 'autoprudente' in supplier or 'auto prudente' in supplier:
                ap_car = car
                ap_position = idx
                break
        
        if ap_position == 1:
            ap_best_price += 1
            position_color = "#10b981"
            position_badge = f'<span style="display:inline-flex;align-items:center;gap:4px;">{icon_trophy} 1¬∫ Lugar</span>'
        elif ap_position and ap_position <= 3:
            ap_competitive += 1
            position_color = "#f59e0b"
            position_badge = f'<span style="display:inline-flex;align-items:center;gap:4px;">{icon_chart} #{ap_position} Posi√ß√£o</span>'
        elif ap_position:
            position_color = "#ef4444"
            position_badge = f'<span style="display:inline-flex;align-items:center;gap:4px;">{icon_warning} #{ap_position} Posi√ß√£o</span>'
        else:
            position_color = "#94a3b8"
            position_badge = f'<span style="display:inline-flex;align-items:center;gap:4px;">{icon_warning} Indispon√≠vel</span>'
        
        # Get top 3 competitors for comparison
        competitors_html = ""
        for idx, car in enumerate(sorted_cars[:3], 1):
            supplier = car.get('supplier', 'Unknown')
            price = float(car.get('price_num', 0))
            is_ap = 'autoprudente' in supplier.lower()
            
            bg_color = "#e0f7fa" if is_ap else "#f8fafc"
            border = "2px solid #009cb6" if is_ap else "1px solid #e2e8f0"
            
            competitors_html += f"""
            <div style="background: {bg_color}; padding: 12px; border: {border}; border-radius: 6px; margin-bottom: 8px;">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <div>
                        <span style="font-weight: 600; color: #1e293b; font-size: 14px;">{idx}¬∫ {supplier}</span>
                        <br><span style="font-size: 12px; color: #64748b;">{car.get('car', 'N/A')}</span>
                    </div>
                    <div style="text-align: right;">
                        <div style="font-size: 20px; font-weight: 700; color: #009cb6;">{price:.2f}‚Ç¨</div>
                        <div style="font-size: 11px; color: #64748b;">por dia</div>
                    </div>
                </div>
            </div>
            """
        
        car_cards_html += f"""
        <tr><td style="padding: 0 30px 20px 30px;">
            <div style="background: #ffffff; border: 2px solid #e2e8f0; padding: 20px; box-shadow: 0 1px 3px rgba(0,0,0,0.1);">
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
                    <h3 style="margin: 0; color: #1e293b; font-size: 18px; display: flex; align-items: center; gap: 8px;">{icon_car} Grupo {group}</h3>
                    <span style="background: {position_color}; color: white; padding: 8px 14px; font-size: 12px; font-weight: 600;">{position_badge}</span>
                </div>
                {competitors_html}
                <div style="margin-top: 12px; padding-top: 12px; border-top: 1px solid #e2e8f0; font-size: 12px; color: #64748b;">
                    Total de ofertas neste grupo: <strong>{len(sorted_cars)}</strong>
                </div>
            </div>
        </td></tr>
        """
    
    # Summary stats
    ap_percentage = (ap_best_price / total_groups * 100) if total_groups > 0 else 0
    
    return f"""
    <!DOCTYPE html>
    <html>
    <head><meta charset="UTF-8"><title>Relat√≥rio Di√°rio</title></head>
    <body style="margin: 0; padding: 0; background-color: #f8fafc; font-family: 'Outfit', 'Segoe UI', sans-serif;">
        <table width="100%" cellpadding="0" cellspacing="0" style="background-color: #f8fafc; padding: 20px 0;">
            <tr><td align="center">
                <table width="700" cellpadding="0" cellspacing="0" style="background-color: #ffffff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                    <!-- Header -->
                    <tr><td style="background: #009cb6; padding: 30px; text-align: center;">
                        <img src="https://carrental-api-5r6g.onrender.com/static/logo.svg" alt="Auto Prudente" style="height: 60px; margin-bottom: 15px;" />
                        <h1 style="margin: 10px 0 0 0; color: #ffffff; font-size: 26px; font-weight: 600;">Relat√≥rio Di√°rio de Pre√ßos</h1>
                        <p style="margin: 8px 0 0 0; color: #e0f7fa; font-size: 15px;">{datetime.now().strftime('%d de %B de %Y')}</p>
                        <p style="margin: 5px 0 0 0; color: #ffffff; font-size: 14px; display: inline-flex; align-items: center; gap: 4px; justify-content: center;">{icon_location} {search_data['location']} ‚Ä¢ {search_data['days']} dias</p>
                    </td></tr>
                    
                    <!-- Summary Stats -->
                    <tr><td style="padding: 25px 30px; background: #f0f9fb; border-bottom: 1px solid #e2e8f0;">
                        <table width="100%" cellpadding="0" cellspacing="0">
                            <tr>
                                <td width="33%" style="text-align: center; padding: 10px;">
                                    <div style="font-size: 32px; font-weight: 700; color: #10b981;">{ap_best_price}</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">Melhores Pre√ßos</div>
                                </td>
                                <td width="33%" style="text-align: center; padding: 10px; border-left: 1px solid #cbd5e1; border-right: 1px solid #cbd5e1;">
                                    <div style="font-size: 32px; font-weight: 700; color: #f59e0b;">{ap_competitive}</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">Competitivos</div>
                                </td>
                                <td width="33%" style="text-align: center; padding: 10px;">
                                    <div style="font-size: 32px; font-weight: 700; color: #009cb6;">{ap_percentage:.0f}%</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">Taxa de Lideran√ßa</div>
                                </td>
                            </tr>
                        </table>
                    </td></tr>
                    
                    <!-- Car Groups -->
                    {car_cards_html}
                    
                    <!-- Footer -->
                    <tr><td style="background: #f8fafc; padding: 25px; text-align: center; border-top: 1px solid #e2e8f0;">
                        <p style="margin: 0; font-size: 12px; color: #94a3b8;">
                            Auto Prudente ¬© {datetime.now().year} ‚Ä¢ Sistema de Monitoriza√ß√£o de Pre√ßos
                        </p>
                        <p style="margin: 8px 0 0 0; font-size: 11px; color: #cbd5e1;">
                            Dados baseados na √∫ltima pesquisa ‚Ä¢ Atualizado automaticamente
                        </p>
                    </td></tr>
                </table>
            </td></tr>
        </table>
    </body>
    </html>
    """

def run_daily_report_search():
    """Run automated search 2 hours before daily report (stores results for comparison)"""
    from datetime import datetime, timedelta
    import random
    
    try:
        print("\n" + "="*80)
        print("üîç DAILY REPORT SEARCH STARTED")
        print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80 + "\n")
        logging.info("üîç Starting daily report search (2h before email)...")
        
        # Load settings to get search location (CORRIGIDO - tabela certa)
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                if not row or not row[0]:
                    logging.warning("‚ö†Ô∏è No automated reports settings - skipping search")
                    return
                
                settings = json.loads(row[0])
                if not settings.get('dailyEnabled'):
                    logging.info("‚ÑπÔ∏è Daily reports disabled - skipping search")
                    return
            finally:
                conn.close()
        
        today = datetime.now()
        days_ahead = random.randint(2, 4)  # Random 2-4 days
        search_date = (today + timedelta(days=days_ahead)).strftime('%Y-%m-%d')
        
        # Pesquisar m√∫ltiplas dura√ß√µes
        days_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 14]  # Removido 22 e 28 (causam timeout)
        
        # PESQUISAR AMBOS OS LOCAIS
        locations = ["Aeroporto de Faro", "Albufeira"]
        
        import requests
        import time
        import os
        
        # URL do pr√≥prio servidor (detecta Render ou local)
        # Render define RENDER_EXTERNAL_HOSTNAME automaticamente
        render_host = os.environ.get('RENDER_EXTERNAL_HOSTNAME')
        if render_host:
            base_url = f"https://{render_host}"  # Render usa HTTPS
        else:
            base_url = "http://localhost:8000"  # Local development
        
        logging.info(f"üåê Using base URL: {base_url}")
        
        # Iniciar pesquisas em background (fire-and-forget) para n√£o bloquear scheduler
        import threading
        
        def execute_searches_in_background():
            """Executa todas as pesquisas em background thread"""
            for idx, location in enumerate(locations, 1):
                for days in days_list:
                    try:
                        logging.info(f"üìç Search {idx}: {location} | {days} dias")
                        
                        # Fire HTTP request (n√£o aguarda resposta)
                        response = requests.post(
                            f"{base_url}/api/track-by-params",
                            json={
                                "location": location,
                                "start_date": search_date,
                                "days": days,
                                "lang": "pt",
                                "currency": "EUR"
                            },
                            timeout=180,
                            headers={"X-Internal-Request": "scheduler"}
                        )
                        
                        if response.ok:
                            result = response.json()
                            if result.get('ok'):
                                items = result.get('items', [])
                                # Contar carros com fotos
                                with_photos = sum(1 for car in items if car.get('photo') and 'loading-car' not in car.get('photo', ''))
                                photo_pct = (with_photos / len(items) * 100) if items else 0
                                logging.info(f"‚úÖ {location} ({days}d): {len(items)} carros | Fotos: {with_photos}/{len(items)} ({photo_pct:.1f}%)")
                                
                                # Salvar resultados
                                with _db_lock:
                                    conn = _db_connect()
                                    try:
                                        # Save items array directly (not nested in "cars" object)
                                        results_json = json.dumps(items)
                                        
                                        try:
                                            import psycopg2
                                            is_postgres = isinstance(conn, psycopg2.extensions.connection)
                                        except:
                                            is_postgres = False
                                        
                                        logging.info(f"[SAVE-DEBUG] Attempting to save: location={location}, days={days}, source=automated")
                                        
                                        if is_postgres:
                                            with conn.cursor() as cur:
                                                cur.execute(
                                                    'INSERT INTO recent_searches (location, start_date, days, results_data, timestamp, "user", source) VALUES (%s, %s, %s, %s, %s, %s, %s)',
                                                    (location, search_date, days, results_json, datetime.now().isoformat(), 'admin', 'automated')
                                                )
                                        else:
                                            conn.execute(
                                                'INSERT INTO recent_searches (location, start_date, days, results_data, timestamp, user, source) VALUES (?, ?, ?, ?, ?, ?, ?)',
                                                (location, search_date, days, results_json, datetime.now().isoformat(), 'admin', 'automated')
                                            )
                                        conn.commit()
                                        logging.info(f"‚úÖ SAVED to recent_searches: {location} ({days}d) [AUTOMATED] - source column OK!")
                                    finally:
                                        conn.close()
                    except Exception as e:
                        logging.error(f"‚ùå Search error: {e}")
        
        # Iniciar thread em background e RETORNAR IMEDIATAMENTE
        search_thread = threading.Thread(target=execute_searches_in_background, daemon=True)
        search_thread.start()
        logging.info(f"üöÄ Started background searches (non-blocking)")
        
    except Exception as e:
        logging.error(f"‚ùå Daily report search preparation failed: {str(e)}")

def run_weekly_report_search():
    """Run automated search for next 3 months (2h before weekly report)"""
    from datetime import datetime, timedelta
    import random
    
    try:
        print("\n" + "="*80)
        print("üîç WEEKLY REPORT SEARCH STARTED")
        print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80 + "\n")
        logging.info("üîç Starting weekly report search for next 3 months (2h before email)...")
        
        # Load settings (CORRIGIDO - tabela certa)
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                if not row or not row[0]:
                    logging.warning("‚ö†Ô∏è No automated reports settings - skipping weekly search")
                    return
                
                settings = json.loads(row[0])
                if not settings.get('weeklyEnabled'):
                    logging.info("‚ÑπÔ∏è Weekly reports disabled - skipping search")
                    return
            finally:
                conn.close()
        
        import requests
        import time
        import os
        
        today = datetime.now()
        # PESQUISAR AMBOS OS LOCAIS
        locations = ["Aeroporto de Faro", "Albufeira"]
        # Pesquisar m√∫ltiplas dura√ß√µes
        days_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 14]  # Removido 22 e 28 (causam timeout)
        
        # URL do pr√≥prio servidor (detecta Render ou local)
        render_host = os.environ.get('RENDER_EXTERNAL_HOSTNAME')
        if render_host:
            base_url = f"https://{render_host}"  # Render usa HTTPS
        else:
            base_url = "http://localhost:8000"  # Local development
        
        logging.info(f"üåê Using base URL for weekly: {base_url}")
        
        # Background thread para pesquisas semanais
        import threading
        
        def execute_weekly_searches_in_background():
            """Executa pesquisas semanais em background"""
            for month_offset in range(1, 4):  # Months 1, 2, 3
                search_date = (today + timedelta(days=30 * month_offset)).strftime('%Y-%m-%d')
                
                for location in locations:
                    for days in days_list:
                        try:
                            logging.info(f"üìç Weekly month {month_offset}: {location} | {days} dias")
                            
                            response = requests.post(
                                f"{base_url}/api/track-by-params",
                                json={
                                    "location": location,
                                    "start_date": search_date,
                                    "days": days,
                                    "lang": "pt",
                                    "currency": "EUR"
                                },
                                timeout=180,
                                headers={"X-Internal-Request": "scheduler-weekly"}
                            )
                            
                            if response.ok:
                                result = response.json()
                                if result.get('ok'):
                                    items = result.get('items', [])
                                    with_photos = sum(1 for car in items if car.get('photo') and 'loading-car' not in car.get('photo', ''))
                                    photo_pct = (with_photos / len(items) * 100) if items else 0
                                    logging.info(f"‚úÖ Weekly M{month_offset} {location} ({days}d): {len(items)} carros | Fotos: {with_photos}/{len(items)} ({photo_pct:.1f}%)")
                                    
                                    with _db_lock:
                                        conn = _db_connect()
                                        try:
                                            results_json = json.dumps(items)
                                            
                                            try:
                                                import psycopg2
                                                is_postgres = isinstance(conn, psycopg2.extensions.connection)
                                            except:
                                                is_postgres = False
                                            
                                            if is_postgres:
                                                with conn.cursor() as cur:
                                                    cur.execute(
                                                        'INSERT INTO recent_searches (location, start_date, days, results_data, timestamp, "user", source) VALUES (%s, %s, %s, %s, %s, %s, %s)',
                                                        (location, search_date, days, results_json, datetime.now().isoformat(), 'admin', 'automated')
                                                    )
                                            else:
                                                conn.execute(
                                                    'INSERT INTO recent_searches (location, start_date, days, results_data, timestamp, user, source) VALUES (?, ?, ?, ?, ?, ?, ?)',
                                                    (location, search_date, days, results_json, datetime.now().isoformat(), 'admin', 'automated')
                                                )
                                            conn.commit()
                                            logging.info(f"‚úÖ Saved to recent_searches: Weekly M{month_offset} {location} ({days}d) [AUTOMATED]")
                                        finally:
                                            conn.close()
                        except Exception as e:
                            logging.error(f"‚ùå Weekly search error: {e}")
        
        # Iniciar thread em background e RETORNAR IMEDIATAMENTE
        weekly_thread = threading.Thread(target=execute_weekly_searches_in_background, daemon=True)
        weekly_thread.start()
        logging.info(f"üöÄ Started weekly background searches (non-blocking)")
        
    except Exception as e:
        logging.error(f"‚ùå Weekly report search preparation failed: {str(e)}")

def save_automated_prices_from_searches():
    """Save automated prices from recent automated searches to history"""
    from datetime import datetime, timedelta
    import json
    
    try:
        logging.info("üíæ Saving automated prices from searches...")
        
        # Get recent automated searches (last 24 hours)
        cutoff = (datetime.now() - timedelta(days=1)).isoformat()
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Detect if PostgreSQL
                try:
                    import psycopg2
                    is_postgres = isinstance(conn, psycopg2.extensions.connection)
                except:
                    is_postgres = False
                
                user_col = '"user"' if is_postgres else 'user'
                placeholder = "%s" if is_postgres else "?"
                
                query = f"""
                    SELECT location, start_date, days, results_data, timestamp
                    FROM recent_searches
                    WHERE source = {placeholder} AND timestamp > {placeholder}
                    ORDER BY timestamp DESC
                """
                cursor = conn.execute(query, ('automated', cutoff))
                searches = cursor.fetchall()
                
                if not searches:
                    logging.info("‚ÑπÔ∏è No automated searches found to process")
                    return
                
                logging.info(f"üìä Processing {len(searches)} automated searches")
                
                # Process each search
                entries_saved = 0
                for search in searches:
                    location, start_date, days, results_json, timestamp = search
                    
                    try:
                        results = json.loads(results_json) if results_json else []
                    except:
                        continue
                    
                    if not results:
                        continue
                    
                    # Group cars by ACRISS group
                    groups = {}
                    for car in results:
                        grupo = car.get('grupo', car.get('group', 'Unknown'))
                        price_str = str(car.get('price', '0'))
                        
                        # Extract numeric price
                        try:
                            price = float(price_str.replace('‚Ç¨', '').replace(',', '.').strip())
                        except:
                            continue
                        
                        if grupo not in groups:
                            groups[grupo] = []
                        groups[grupo].append(price)
                    
                    # Save cheapest price per group
                    for grupo, prices in groups.items():
                        if not prices:
                            continue
                        
                        cheapest = min(prices)
                        
                        # Insert into automated_prices_history
                        placeholders_insert = ", ".join([placeholder] * 10)
                        insert_query = f"""
                            INSERT INTO automated_prices_history
                            (location, grupo, pickup_date, auto_price, real_price,
                             strategy_used, strategy_details, min_price_applied, created_by, source)
                            VALUES ({placeholders_insert})
                        """
                        conn.execute(
                            insert_query,
                            (
                                location,
                                grupo,
                                start_date,
                                cheapest,  # auto_price = real_price (no markup)
                                cheapest,  # real_price
                                'automated_search',
                                json.dumps({'search_timestamp': timestamp, 'days': days}),
                                cheapest,  # min_price_applied
                                'system',
                                'automated'
                            )
                        )
                        entries_saved += 1
                
                conn.commit()
                logging.info(f"‚úÖ Saved {entries_saved} automated price entries")
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error saving automated prices: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())

def save_automated_searches_to_history():
    """Process recent automated searches and save to automated_search_history table"""
    from datetime import datetime, timedelta
    import json
    
    try:
        logging.info("üíæ Processing automated searches for history...")
        
        # Get recent automated searches (last 24 hours)
        cutoff = (datetime.now() - timedelta(hours=24)).isoformat()
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                user_col = '"user"' if is_postgres else 'user'
                
                # Get all automated searches from last 24h
                query = f"""
                    SELECT location, start_date, days, results_data, timestamp
                    FROM recent_searches
                    WHERE source = {placeholder} AND timestamp > {placeholder}
                    ORDER BY timestamp DESC, location, days
                """
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute(query, ('automated', cutoff))
                        searches = cur.fetchall()
                else:
                    searches = conn.execute(query, ('automated', cutoff)).fetchall()
                
                if not searches:
                    logging.info("‚ÑπÔ∏è No automated searches found to process")
                    return
                
                logging.info(f"üìä Found {len(searches)} automated searches to process")
                
                # Group searches by location to create a single history entry per location
                searches_by_location = {}
                for search in searches:
                    location, start_date, days, results_json, timestamp = search
                    
                    if location not in searches_by_location:
                        searches_by_location[location] = {
                            'searches': [],
                            'latest_timestamp': timestamp
                        }
                    
                    try:
                        results = json.loads(results_json) if results_json else []
                        searches_by_location[location]['searches'].append({
                            'days': days,
                            'results': results,
                            'timestamp': timestamp
                        })
                    except Exception as e:
                        logging.error(f"‚ùå Error parsing results for {location}: {e}")
                        continue
                
                # Process each location
                saved_count = 0
                for location, data in searches_by_location.items():
                    try:
                        # Aggregate prices by group and days
                        prices_by_group = {}  # { "B1": { "1": 25.50, "3": 23.00 }, ... }
                        dias_set = set()
                        
                        for search in data['searches']:
                            days = search['days']
                            dias_set.add(days)
                            
                            for car in search['results']:
                                grupo = car.get('grupo', car.get('group', 'Unknown'))
                                price_str = str(car.get('price', '0'))
                                
                                # Extract numeric price
                                try:
                                    price = float(price_str.replace('‚Ç¨', '').replace(',', '.').strip())
                                except:
                                    continue
                                
                                if grupo not in prices_by_group:
                                    prices_by_group[grupo] = {}
                                
                                # Keep cheapest price for each group/days combination
                                day_key = str(days)
                                if day_key not in prices_by_group[grupo]:
                                    prices_by_group[grupo][day_key] = price
                                else:
                                    prices_by_group[grupo][day_key] = min(prices_by_group[grupo][day_key], price)
                        
                        # Prepare data for saving
                        dias_list = sorted(list(dias_set))
                        price_count = sum(len(group_prices) for group_prices in prices_by_group.values())
                        
                        if not prices_by_group or price_count == 0:
                            logging.warning(f"‚ö†Ô∏è No valid prices found for {location}")
                            continue
                        
                        # Generate month_key
                        now = datetime.now()
                        month_key = f"{now.year}-{str(now.month).zfill(2)}"
                        
                        # Save to automated_search_history
                        prices_json = json.dumps(prices_by_group)
                        dias_json = json.dumps(dias_list)
                        search_type = 'automated'
                        user_email = 'system_scheduler'
                        
                        if is_postgres:
                            with conn.cursor() as cur:
                                try:
                                    cur.execute("""
                                        INSERT INTO automated_search_history 
                                        (location, search_type, month_key, prices_data, dias, price_count, user_email)
                                        VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s)
                                        RETURNING id
                                    """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email))
                                    search_id = cur.fetchone()[0]
                                    conn.commit()
                                    logging.info(f"‚úÖ Saved {location} to history: ID={search_id}, Groups={len(prices_by_group)}, Dias={dias_list}, Prices={price_count}")
                                    saved_count += 1
                                except Exception as e:
                                    logging.error(f"‚ùå Error saving {location} to PostgreSQL: {e}")
                                    conn.rollback()
                        else:
                            try:
                                cursor = conn.execute("""
                                    INSERT INTO automated_search_history 
                                    (location, search_type, month_key, prices_data, dias, price_count, user_email)
                                    VALUES (?, ?, ?, ?, ?, ?, ?)
                                """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email))
                                search_id = cursor.lastrowid
                                conn.commit()
                                logging.info(f"‚úÖ Saved {location} to history: ID={search_id}, Groups={len(prices_by_group)}, Dias={dias_list}, Prices={price_count}")
                                saved_count += 1
                            except Exception as e:
                                logging.error(f"‚ùå Error saving {location} to SQLite: {e}")
                                conn.rollback()
                        
                    except Exception as e:
                        logging.error(f"‚ùå Error processing {location}: {e}")
                        import traceback
                        logging.error(traceback.format_exc())
                
                logging.info(f"üéâ Saved {saved_count}/{len(searches_by_location)} locations to automated_search_history")
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error in save_automated_searches_to_history: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())

def send_automatic_daily_report():
    """
    Send daily report automatically - 2 EMAILS SEPARADOS
    1 email para Albufeira, 1 email para Aeroporto
    """
    from datetime import datetime, timedelta
    from improved_reports import generate_daily_report_html_by_location
    
    try:
        print("\n" + "="*80)
        print("üìß DAILY REPORT EMAIL STARTED")
        print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80 + "\n")
        logging.info("üîÑ Starting automatic daily report...")
        
        # Load settings from database
        with _db_lock:
            conn = _db_connect()
            try:
                # Check automation settings (daily enabled, etc)
                logging.info("[EMAIL-DEBUG] Checking price_automation_settings table...")
                
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                
                if not row or not row[0]:
                    logging.warning("‚ö†Ô∏è No automated reports settings found")
                    return
                
                automation_settings = json.loads(row[0])
                logging.info(f"[EMAIL-DEBUG] Automation settings: dailyEnabled={automation_settings.get('dailyEnabled')}")
                
                if not automation_settings.get('dailyEnabled'):
                    logging.info("‚ÑπÔ∏è Daily reports are disabled - skipping")
                    return
                
                # Get recipients
                cursor = conn.execute(
                    "SELECT setting_value FROM user_settings WHERE setting_key = 'email_settings'"
                )
                email_row = cursor.fetchone()
                
                if not email_row or not email_row[0]:
                    logging.warning("‚ö†Ô∏è No email settings found")
                    return
                
                email_settings = json.loads(email_row[0])
                recipients_text = email_settings.get('recipients', '')
                recipients = [email.strip() for email in recipients_text.split('\n') if email.strip()]
                
                if not recipients:
                    logging.warning("‚ö†Ô∏è No recipients configured")
                    return
                
                logging.info(f"üìß Sending daily report to {len(recipients)} recipient(s): {recipients}")
            finally:
                conn.close()
        
        # Get Gmail credentials
        from googleapiclient.discovery import build
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        import base64
        
        credentials = _get_gmail_credentials()
        
        if not credentials:
            logging.error("‚ùå No Gmail credentials found")
            return
        
        # Load ALL recent search data (TODOS os dias e locais)
        search_data = {'results': []}
        with _db_lock:
            conn = _db_connect()  
            try:
                cursor = conn.execute(
                    """
                    SELECT location, start_date, days, results_data, timestamp
                    FROM recent_searches
                    WHERE DATE(timestamp) = CURRENT_DATE
                    ORDER BY timestamp DESC
                    """
                )
                rows = cursor.fetchall()
                
                all_results = []
                for row in rows:
                    location, start_date, days, results_data, timestamp = row
                    if results_data:
                        results = json.loads(results_data)
                        # Adicionar info de days a cada resultado
                        for r in results:
                            r['days'] = days
                            r['location'] = location
                        all_results.extend(results)
                
                search_data = {'results': all_results}
                logging.info(f"üìä Found {len(all_results)} total results from today's searches")
            finally:
                conn.close()
        
        if not search_data['results']:
            logging.warning("‚ö†Ô∏è No search data found for today")
            return
        
        # Build Gmail service
        service = build('gmail', 'v1', credentials=credentials)
        
        # ENVIAR 2 EMAILS SEPARADOS
        locations = ['Albufeira', 'Aeroporto de Faro']
        total_sent = 0
        
        for location in locations:
            logging.info(f"\nüìç Generating report for: {location}")
            
            # Generate HTML for this location
            html_content = generate_daily_report_html_by_location(search_data, location)
            
            # Send to all recipients
            for recipient in recipients:
                try:
                    message = MIMEMultipart('alternative')
                    message['to'] = recipient  
                    message['subject'] = f'üìä Relat√≥rio Di√°rio {location} - Auto Prudente ({datetime.now().strftime("%d/%m/%Y")})'  
                    message.attach(MIMEText(html_content, 'html'))
                    
                    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')
                    service.users().messages().send(userId='me', body={'raw': raw_message}).execute()
                    
                    total_sent += 1
                    logging.info(f"‚úÖ {location} report sent to {recipient}")
                except Exception as e:
                    logging.error(f"‚ùå Failed to send {location} report to {recipient}: {str(e)}")
        
        logging.info(f"üéâ Daily reports completed: {total_sent} emails sent (2 locations √ó {len(recipients)} recipients)")
        
        # Save history
        try:
            save_automated_searches_to_history()
        except Exception as e:
            logging.error(f"‚ö†Ô∏è Failed to save automated searches to history: {str(e)}")
        
        try:
            save_automated_prices_from_searches()
        except Exception as e:
            logging.error(f"‚ö†Ô∏è Failed to save automated prices: {str(e)}")
        
    except Exception as e:
        logging.error(f"‚ùå Automatic daily report failed: {str(e)}")

def generate_weekly_report_html(months_data):
    """Generate visual HTML weekly report with monthly analysis"""
    from datetime import datetime
    
    # SVG Icons (monocrom√°ticos)
    icon_chart = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M3 13h2v8H3v-8zm4-6h2v14H7V7zm4-4h2v18h-2V3zm4 8h2v10h-2V11z"/></svg>'
    icon_trophy = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M20 7h-2V5c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v3c0 2.5 1.5 4.7 3.8 5.7.5 1.7 1.8 3 3.5 3.7V23h5v-1.6c1.7-.7 3-2 3.5-3.7 2.3-1 3.8-3.2 3.8-5.7V9c0-1.1-.9-2-2-2zm0 5c0 1.9-1.2 3.5-2.9 4.1-.2-1.3-.8-2.4-1.7-3.3l-1.4 1.4c.6.6 1 1.5 1 2.4 0 1.9-1.6 3.5-3.5 3.5S8 18.5 8 16.6c0-.9.4-1.8 1-2.4L7.6 12.8c-.9.9-1.5 2-1.7 3.3C4.2 15.5 3 13.9 3 12V9h3V5h12v4h3v3z"/></svg>'
    icon_calendar = '<svg width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M19 4h-1V2h-2v2H8V2H6v2H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 16H5V10h14v10zM5 8V6h14v2H5z"/></svg>'
    icon_trend_up = '<svg width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M16 6l2.29 2.29-4.88 4.88-4-4L2 16.59 3.41 18l6-6 4 4 6.3-6.29L22 12V6h-6z"/></svg>'
    icon_trend_down = '<svg width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M16 18l2.29-2.29-4.88-4.88-4 4L2 7.41 3.41 6l6 6 4-4 6.3 6.29L22 12v6h-6z"/></svg>'
    icon_warning = '<svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M1 21h22L12 2 1 21zm12-3h-2v-2h2v2zm0-4h-2v-4h2v4z"/></svg>'
    
    if not months_data or len(months_data) == 0:
        return f"""
        <!DOCTYPE html>
        <html>
        <head><meta charset="UTF-8"><title>Relat√≥rio Semanal</title></head>
        <body style="margin: 0; padding: 0; background-color: #f8fafc; font-family: 'Outfit', 'Segoe UI', sans-serif;">
            <table width="100%" cellpadding="0" cellspacing="0" style="background-color: #f8fafc; padding: 20px 0;">
                <tr><td align="center">
                    <table width="700" cellpadding="0" cellspacing="0" style="background-color: #ffffff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <tr><td style="background: #7ec6e0; padding: 30px; text-align: center;">
                            <img src="https://carrental-api-5r6g.onrender.com/static/logo.svg" alt="Auto Prudente" style="height: 40px; margin-bottom: 15px;" />
                            <h1 style="margin: 10px 0 0 0; color: #1e293b; font-size: 26px; font-weight: 600;">Relat√≥rio Semanal</h1>
                            <p style="margin: 5px 0 0 0; color: #475569; font-size: 15px;">Semana {datetime.now().strftime('%W/%Y')}</p>
                        </td></tr>
                        <tr><td style="padding: 40px; text-align: center;">
                            <div style="display: inline-block; padding: 20px; background: #fef3c7; border-left: 4px solid #f59e0b;">
                                {icon_warning}
                                <p style="color: #92400e; font-size: 16px; margin: 10px 0 0 0; font-weight: 500;">Sem dados de an√°lise dispon√≠veis</p>
                            </div>
                            <p style="color: #94a3b8; font-size: 14px; margin-top: 15px;">Configure o per√≠odo de an√°lise (1-6 meses) nas defini√ß√µes</p>
                        </td></tr>
                    </table>
                </td></tr>
            </table>
        </body>
        </html>
        """
    
    # Build monthly comparison HTML
    months_html = ""
    for month in months_data:
        month_name = month['name']
        best = month['best_price_count']
        competitive = month['competitive_count']
        has_data = month.get('has_data', True)
        change = month.get('change')
        
        # Trend indicator
        trend_html = ""
        if change is not None and change != 0:
            if change > 0:
                trend_html = f'<span style="color: #10b981; font-size: 12px; margin-left: 5px;">{icon_trend_up} +{change}</span>'
            else:
                trend_html = f'<span style="color: #ef4444; font-size: 12px; margin-left: 5px;">{icon_trend_down} {change}</span>'
        
        if not has_data:
            months_html += f"""
            <tr><td style="padding: 15px; border-bottom: 1px solid #e2e8f0;">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <div style="display: flex; align-items: center; gap: 10px;">
                        {icon_calendar}
                        <span style="font-weight: 600; color: #1e293b; font-size: 16px;">{month_name}</span>
                    </div>
                    <div>
                        <span style="color: #94a3b8; font-size: 13px;">{icon_warning} Sem dados</span>
                    </div>
                </div>
            </td></tr>
            """
        else:
            months_html += f"""
            <tr><td style="padding: 15px; border-bottom: 1px solid #e2e8f0;">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <div style="display: flex; align-items: center; gap: 10px;">
                        {icon_calendar}
                        <span style="font-weight: 600; color: #1e293b; font-size: 16px;">{month_name}</span>
                    </div>
                    <div style="display: flex; gap: 20px; align-items: center;">
                        <div style="text-align: center;">
                            <div style="font-size: 24px; font-weight: 700; color: #10b981; display: flex; align-items: center; justify-content: center;">
                                {best}{trend_html}
                            </div>
                            <div style="font-size: 11px; color: #64748b;">1¬∫ Lugar</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 24px; font-weight: 700; color: #f59e0b;">{competitive}</div>
                            <div style="font-size: 11px; color: #64748b;">Top 3</div>
                        </div>
                    </div>
                </div>
            </td></tr>
            """
    
    # Calculate totals
    total_best = sum(m['best_price_count'] for m in months_data)
    total_competitive = sum(m['competitive_count'] for m in months_data)
    avg_percentage = sum(m['percentage'] for m in months_data) / len(months_data) if months_data else 0
    
    return f"""
    <!DOCTYPE html>
    <html>
    <head><meta charset="UTF-8"><title>Relat√≥rio Semanal</title></head>
    <body style="margin: 0; padding: 0; background-color: #f8fafc; font-family: 'Outfit', 'Segoe UI', sans-serif;">
        <table width="100%" cellpadding="0" cellspacing="0" style="background-color: #f8fafc; padding: 20px 0;">
            <tr><td align="center">
                <table width="700" cellpadding="0" cellspacing="0" style="background-color: #ffffff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                    <!-- Header -->
                    <tr><td style="background: #7ec6e0; padding: 30px; text-align: center;">
                        <img src="https://carrental-api-5r6g.onrender.com/static/logo.svg" alt="Auto Prudente" style="height: 40px; margin-bottom: 15px;" />
                        <h1 style="margin: 10px 0 0 0; color: #1e293b; font-size: 26px; font-weight: 600;">Relat√≥rio Semanal</h1>
                        <p style="margin: 8px 0 0 0; color: #475569; font-size: 15px;">Semana {datetime.now().strftime('%W de %Y')}</p>
                        <p style="margin: 5px 0 0 0; color: #64748b; font-size: 14px;">Pr√≥ximos {len(months_data)} {('m√™s' if len(months_data) == 1 else 'meses')}</p>
                    </td></tr>
                    
                    <!-- Summary Stats -->
                    <tr><td style="padding: 25px 30px; background: #f0f9fb; border-bottom: 1px solid #e2e8f0;">
                        <table width="100%" cellpadding="0" cellspacing="0">
                            <tr>
                                <td width="33%" style="text-align: center; padding: 10px;">
                                    <div style="font-size: 32px; font-weight: 700; color: #10b981;">{total_best}</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">Total Melhores Pre√ßos</div>
                                </td>
                                <td width="33%" style="text-align: center; padding: 10px; border-left: 1px solid #cbd5e1; border-right: 1px solid #cbd5e1;">
                                    <div style="font-size: 32px; font-weight: 700; color: #f59e0b;">{total_competitive}</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">Total Competitivos</div>
                                </td>
                                <td width="33%" style="text-align: center; padding: 10px;">
                                    <div style="font-size: 32px; font-weight: 700; color: #009cb6;">{avg_percentage:.0f}%</div>
                                    <div style="font-size: 13px; color: #64748b; margin-top: 5px;">M√©dia de Lideran√ßa</div>
                                </td>
                            </tr>
                        </table>
                    </td></tr>
                    
                    <!-- Monthly Breakdown -->
                    <tr><td style="padding: 20px 30px;">
                        <h2 style="margin: 0 0 15px 0; color: #1e293b; font-size: 18px; font-weight: 600;">Desempenho Mensal</h2>
                        <table width="100%" cellpadding="0" cellspacing="0" style="background: #ffffff; border: 1px solid #e2e8f0;">
                            {months_html}
                        </table>
                    </td></tr>
                    
                    <!-- Footer -->
                    <tr><td style="background: #f8fafc; padding: 25px; text-align: center; border-top: 1px solid #e2e8f0;">
                        <p style="margin: 0; font-size: 12px; color: #94a3b8;">
                            Auto Prudente ¬© {datetime.now().year} ‚Ä¢ Sistema de Monitoriza√ß√£o de Pre√ßos
                        </p>
                        <p style="margin: 8px 0 0 0; font-size: 11px; color: #cbd5e1;">
                            Relat√≥rio semanal autom√°tico ‚Ä¢ An√°lise hist√≥rica de desempenho
                        </p>
                    </td></tr>
                </table>
            </td></tr>
        </table>
    </body>
    </html>
    """

def send_automatic_weekly_report():
    """Send weekly report automatically based on saved settings"""
    from datetime import datetime, timedelta
    
    try:
        print("\n" + "="*80)
        print("üìß WEEKLY REPORT EMAIL STARTED")
        print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80 + "\n")
        logging.info("üîÑ Starting automatic weekly report...")
        
        # Load settings from database (CORRIGIDO - tabela certa)
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute(
                    "SELECT setting_value FROM price_automation_settings WHERE setting_key = 'automatedReportsSettings'"
                )
                row = cursor.fetchone()
                if not row or not row[0]:
                    logging.warning("‚ö†Ô∏è No automated reports settings found - skipping weekly report")
                    return
                
                settings = json.loads(row[0])
                if not settings.get('weeklyEnabled'):
                    logging.info("‚ÑπÔ∏è Weekly reports are disabled - skipping")
                    return
                
                recipients = settings.get('recipients', [])
                if not recipients:
                    logging.warning("‚ö†Ô∏è No recipients configured - skipping weekly report")
                    return
                
                logging.info(f"üìß Sending weekly report to {len(recipients)} recipient(s)")
            finally:
                conn.close()
        
        # Get complete Gmail credentials
        from googleapiclient.discovery import build
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime, timedelta
        import base64
        
        credentials = _get_gmail_credentials()
        
        if not credentials:
            logging.error("‚ùå No Gmail credentials found - cannot send weekly report")
            return
        
        months_to_analyze = settings.get('weeklyMonths', 3)  # Default: 3 months
        months_data = []
        
        # Get most recent search data for next N months
        with _db_lock:
            conn = _db_connect()
            try:
                today = datetime.now()
                
                # For each of the next N months
                for month_offset in range(1, months_to_analyze + 1):
                    # Calculate target month (e.g., +1 month, +2 months, +3 months)
                    target_date = today + timedelta(days=30 * month_offset)
                    month_name = target_date.strftime('%B de %Y')
                    
                    # Get most recent search for dates in this month range
                    month_start = target_date.replace(day=1).strftime('%Y-%m-%d')
                    if target_date.month == 12:
                        month_end = target_date.replace(year=target_date.year + 1, month=1, day=1).strftime('%Y-%m-%d')
                    else:
                        month_end = target_date.replace(month=target_date.month + 1, day=1).strftime('%Y-%m-%d')
                    
                    cursor = conn.execute(
                        """
                        SELECT results_data, timestamp
                        FROM recent_searches
                        WHERE start_date >= ? AND start_date < ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                        """,
                        (month_start, month_end)
                    )
                    
                    row = cursor.fetchone()
                    if not row or not row[0]:
                        # No data for this month
                        months_data.append({
                            'name': month_name,
                            'best_price_count': 0,
                            'competitive_count': 0,
                            'percentage': 0,
                            'has_data': False
                        })
                        continue
                    
                    results = json.loads(row[0])
                    search_time = row[1]
                    
                    # Group by car groups and analyze
                    groups = {}
                    for car in results:
                        group = car.get('group', 'Unknown')
                        if group not in groups:
                            groups[group] = []
                        groups[group].append(car)
                    
                    best_price_count = 0
                    competitive_count = 0
                    total_groups = len(groups)
                    
                    for group, cars in groups.items():
                        sorted_cars = sorted(cars, key=lambda x: float(x.get('price_num', 999999)))
                        
                        # Find Auto Prudente position
                        ap_position = None
                        for idx, car in enumerate(sorted_cars, 1):
                            supplier = (car.get('supplier', '') or '').lower()
                            if 'autoprudente' in supplier or 'auto prudente' in supplier:
                                ap_position = idx
                                break
                        
                        if ap_position == 1:
                            best_price_count += 1
                        elif ap_position and ap_position <= 3:
                            competitive_count += 1
                    
                    # Get previous week data for comparison (if exists)
                    cursor_prev = conn.execute(
                        """
                        SELECT results_data
                        FROM recent_searches
                        WHERE start_date >= ? AND start_date < ?
                        AND timestamp < ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                        """,
                        (month_start, month_end, search_time)
                    )
                    
                    prev_row = cursor_prev.fetchone()
                    prev_best = None
                    if prev_row and prev_row[0]:
                        prev_results = json.loads(prev_row[0])
                        prev_groups = {}
                        for car in prev_results:
                            g = car.get('group', 'Unknown')
                            if g not in prev_groups:
                                prev_groups[g] = []
                            prev_groups[g].append(car)
                        
                        prev_best = 0
                        for g, cars in prev_groups.items():
                            sorted_c = sorted(cars, key=lambda x: float(x.get('price_num', 999999)))
                            for idx, car in enumerate(sorted_c, 1):
                                supplier = (car.get('supplier', '') or '').lower()
                                if 'autoprudente' in supplier or 'auto prudente' in supplier:
                                    if idx == 1:
                                        prev_best += 1
                                    break
                    
                    months_data.append({
                        'name': month_name,
                        'best_price_count': best_price_count,
                        'competitive_count': competitive_count,
                        'percentage': (best_price_count / total_groups * 100) if total_groups > 0 else 0,
                        'has_data': True,
                        'prev_best': prev_best,
                        'change': best_price_count - prev_best if prev_best is not None else None
                    })
                    
            finally:
                conn.close()
        
        # Generate HTML report
        html_content = generate_weekly_report_html(months_data)
        
        # Build Gmail service with complete credentials (already loaded)
        service = build('gmail', 'v1', credentials=credentials)
        
        sent_count = 0
        for recipient in recipients:
            try:
                message = MIMEMultipart('alternative')
                message['to'] = recipient
                message['subject'] = f'Relat√≥rio Semanal - Auto Prudente (Semana {datetime.now().strftime("%W/%Y")})'
                message.attach(MIMEText(html_content, 'html'))
                
                raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')
                service.users().messages().send(userId='me', body={'raw': raw_message}).execute()
                
                sent_count += 1
                logging.info(f"‚úÖ Weekly report sent to {recipient}")
            except Exception as e:
                logging.error(f"‚ùå Failed to send weekly report to {recipient}: {str(e)}")
        
        logging.info(f"üéâ Weekly report completed: {sent_count}/{len(recipients)} sent successfully")
        
        # Save automated searches to history (for Pre√ßos Automatizados page)
        try:
            save_automated_searches_to_history()
        except Exception as e:
            logging.error(f"‚ö†Ô∏è Failed to save automated searches to history: {str(e)}")
        
    except Exception as e:
        logging.error(f"‚ùå Automatic weekly report failed: {str(e)}")

# ============================================================
# SCHEDULERS - BACKUPS & REPORTS
# ============================================================

# Global scheduler variable
scheduler = None

try:
    from apscheduler.schedulers.background import BackgroundScheduler
    from apscheduler.triggers.cron import CronTrigger
    import pytz
    
    # Create single scheduler for all jobs with Portugal timezone
    lisbon_tz = pytz.timezone('Europe/Lisbon')
    scheduler = BackgroundScheduler(timezone=lisbon_tz)
    
    print("=" * 80)
    print("üöÄ INITIALIZING APSCHEDULER")
    print(f"üìç Timezone: Europe/Lisbon (UTC+0/+1)")
    print(f"‚è∞ Current Lisbon time: {datetime.now(lisbon_tz).strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # === BACKUP JOB ===
    # Daily backup at 3 AM
    scheduler.add_job(
        create_automatic_backup,
        CronTrigger(hour=3, minute=0),
        id='daily_backup',
        name='Daily Automatic Backup',
        replace_existing=True
    )
    log_to_db("INFO", "‚úÖ Backup scheduler configured (daily at 3 AM)", "main", "scheduler")
    
    # === REPORTS JOBS ===
    # Daily search at 7 AM (2h before report)
    scheduler.add_job(
        run_daily_report_search,
        CronTrigger(hour=7, minute=0),
        id='daily_report_search',
        name='Daily Report Search',
        replace_existing=True
    )
    log_to_db("INFO", "‚úÖ Daily report search scheduler configured (daily at 7 AM)", "main", "scheduler")
    
    # Daily report at 9 AM (default time)
    scheduler.add_job(
        send_automatic_daily_report,
        CronTrigger(hour=9, minute=0),
        id='daily_report',
        name='Daily Automatic Report',
        replace_existing=True
    )
    log_to_db("INFO", "‚úÖ Daily report scheduler configured (daily at 9 AM)", "main", "scheduler")
    
    # Weekly search on Monday at 7 AM (2h before report) - searches next 3 months
    scheduler.add_job(
        run_weekly_report_search,
        CronTrigger(day_of_week='mon', hour=7, minute=0),
        id='weekly_report_search',
        replace_existing=True
    )
    log_to_db("INFO", "‚úÖ Weekly report search scheduler configured (Monday at 7 AM - next 3 months)", "main", "scheduler")
    
    # Weekly report on Monday at 9 AM (default time)
    scheduler.add_job(
        send_automatic_weekly_report,
        CronTrigger(day_of_week='mon', hour=9, minute=0),
        id='weekly_report',
        name='Weekly Automatic Report',
        replace_existing=True
    )
    log_to_db("INFO", "‚úÖ Weekly report scheduler configured (Monday at 9 AM)", "main", "scheduler")
    
    # Start scheduler
    scheduler.start()
    log_to_db("INFO", "üöÄ Scheduler started successfully with official jobs (backup + daily + weekly)", "main", "scheduler")
    
    # Print all configured jobs with next run times
    print("\n" + "=" * 80)
    print("‚úÖ SCHEDULER STARTED SUCCESSFULLY")
    print("=" * 80)
    print(f"Total jobs: {len(scheduler.get_jobs())}")
    print("\nScheduled jobs:")
    for job in scheduler.get_jobs():
        next_run = job.next_run_time.astimezone(lisbon_tz).strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else 'N/A'
        print(f"  ‚Ä¢ {job.name} ({job.id})")
        print(f"    ‚è∞ Next run: {next_run}")
        print(f"    üîÑ Trigger: {job.trigger}")
    print("=" * 80 + "\n")
    
except ImportError:
    print("‚ö†Ô∏è  WARNING: APScheduler not installed - automatic backups and reports disabled")
    log_to_db("WARNING", "APScheduler not installed - automatic backups and reports disabled", "main", "scheduler")
except Exception as e:
    print(f"‚ùå ERROR: Failed to start scheduler: {str(e)}")
    log_to_db("ERROR", f"Failed to start scheduler: {str(e)}", "main", "scheduler")

# ============================================================
# EMAIL QUEUE SYSTEM
# ============================================================

import queue
import threading

email_queue = queue.Queue()
email_queue_running = False

def email_worker():
    """Worker thread para processar fila de emails"""
    global email_queue_running
    email_queue_running = True
    
    while email_queue_running:
        try:
            # Aguardar email na fila (timeout 1s)
            email_data = email_queue.get(timeout=1)
            
            # Enviar email
            try:
                _send_notification_email(
                    email_data['to'],
                    email_data['subject'],
                    email_data['message']
                )
                log_to_db("INFO", f"Email sent from queue: {email_data['to']}", "main", "email_worker")
            except Exception as e:
                log_to_db("ERROR", f"Failed to send queued email: {str(e)}", "main", "email_worker")
                # Retry at√© 3 vezes
                if email_data.get('retry_count', 0) < 3:
                    email_data['retry_count'] = email_data.get('retry_count', 0) + 1
                    email_queue.put(email_data)
            
            email_queue.task_done()
        except queue.Empty:
            continue
        except Exception as e:
            log_to_db("ERROR", f"Email worker error: {str(e)}", "main", "email_worker")

def queue_email(to: str, subject: str, message: str):
    """Adicionar email √† fila para envio ass√≠ncrono"""
    email_queue.put({
        'to': to,
        'subject': subject,
        'message': message,
        'retry_count': 0
    })

# Iniciar worker thread para emails
email_thread = threading.Thread(target=email_worker, daemon=True)
email_thread.start()
log_to_db("INFO", "Email worker thread started", "main", "startup")

# ============================================================
# SEARCH HISTORY & NOTIFICATIONS ENDPOINTS
# ============================================================

@app.post("/api/search-history/save")
async def save_search_history_endpoint(request: Request):
    """Salva pesquisa no hist√≥rico"""
    require_auth(request)
    try:
        data = await request.json()
        
        save_search_to_history(
            location=data.get('location', ''),
            start_date=data.get('start_date', ''),
            end_date=data.get('end_date', ''),
            days=data.get('days', 0),
            results_count=data.get('results_count', 0),
            min_price=data.get('min_price'),
            max_price=data.get('max_price'),
            avg_price=data.get('avg_price'),
            user=request.state.user.get('username', 'admin') if hasattr(request.state, 'user') else 'admin',
            search_params=json.dumps(data)
        )
        
        return _no_store_json({"ok": True, "message": "Search saved to history"})
    except Exception as e:
        logging.error(f"Save search history error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

@app.get("/api/search-history/list")
async def list_search_history(request: Request, limit: int = 50):
    """Lista hist√≥rico de pesquisas"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT * FROM search_history 
                    ORDER BY search_timestamp DESC 
                    LIMIT ?
                """, (limit,))
                
                rows = cursor.fetchall()
                history = []
                for row in rows:
                    history.append({
                        'id': row[0],
                        'location': row[1],
                        'start_date': row[2],
                        'end_date': row[3],
                        'days': row[4],
                        'results_count': row[5],
                        'min_price': row[6],
                        'max_price': row[7],
                        'avg_price': row[8],
                        'user': row[9],
                        'search_timestamp': row[10],
                        'search_params': row[11]
                    })
                
                return _no_store_json({"ok": True, "history": history})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"List search history error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

@app.post("/api/notifications/rules/create")
async def create_notification_rule(request: Request):
    """Cria regra de notifica√ß√£o"""
    require_auth(request)
    try:
        data = await request.json()
        
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute("""
                    INSERT INTO notification_rules 
                    (rule_name, notification_type, recipient, trigger_condition, 
                     trigger_value, message_template, enabled)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    data.get('rule_name'),
                    data.get('notification_type', 'email'),
                    data.get('recipient'),
                    data.get('trigger_condition'),
                    data.get('trigger_value'),
                    data.get('message_template'),
                    True
                ))
                conn.commit()
                return _no_store_json({"ok": True, "message": "Notification rule created"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Create notification rule error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

@app.get("/api/notifications/rules/list")
async def list_notification_rules(request: Request):
    """Lista regras de notifica√ß√£o"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT * FROM notification_rules 
                    ORDER BY created_at DESC
                """)
                
                rows = cursor.fetchall()
                rules = []
                for row in rows:
                    rules.append({
                        'id': row[0],
                        'rule_name': row[1],
                        'notification_type': row[2],
                        'recipient': row[3],
                        'trigger_condition': row[4],
                        'trigger_value': row[5],
                        'message_template': row[6],
                        'enabled': row[7],
                        'created_at': row[8]
                    })
                
                return _no_store_json({"ok": True, "rules": rules})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"List notification rules error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

@app.delete("/api/notifications/rules/{rule_id}")
async def delete_notification_rule(request: Request, rule_id: int):
    """Deleta regra de notifica√ß√£o"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                conn.execute("DELETE FROM notification_rules WHERE id = ?", (rule_id,))
                conn.commit()
                return _no_store_json({"ok": True, "message": "Rule deleted"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"Delete notification rule error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

@app.get("/api/notifications/history")
async def list_notification_history(request: Request, limit: int = 50):
    """Lista hist√≥rico de notifica√ß√µes"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT * FROM notification_history 
                    ORDER BY sent_at DESC 
                    LIMIT ?
                """, (limit,))
                
                rows = cursor.fetchall()
                history = []
                for row in rows:
                    history.append({
                        'id': row[0],
                        'rule_id': row[1],
                        'notification_type': row[2],
                        'recipient': row[3],
                        'subject': row[4],
                        'message': row[5],
                        'status': row[6],
                        'sent_at': row[7]
                    })
                
                return _no_store_json({"ok": True, "history": history})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"List notification history error: {e}")
        return _no_store_json({"ok": False, "error": str(e)}, 500)

# ============================================================
# AI LEARNING DATA ENDPOINTS
# ============================================================

@app.post("/api/ai/learning/save")
async def save_ai_learning_data(request: Request):
    """Salvar dados de aprendizagem AI no PostgreSQL"""
    require_auth(request)
    try:
        data = await request.json()
        adjustments = data.get('adjustments', [])
        patterns = data.get('patterns', {})
        suggestions = data.get('suggestions', [])
        
        logging.info(f"üíæ Saving AI learning data: {len(adjustments)} adjustments")
        
        with _db_lock:
            conn = _db_connect()
            try:
                # Limpar dados antigos
                conn.execute("DELETE FROM ai_learning_data")
                
                # Salvar novos ajustes
                for adj in adjustments:
                    conn.execute("""
                        INSERT INTO ai_learning_data 
                        (grupo, days, supplier, original_price, adjusted_price, 
                         adjustment_type, adjustment_value, reason, context, 
                         timestamp, success_score)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        adj.get('grupo'),
                        adj.get('days'),
                        adj.get('supplier'),
                        adj.get('originalPrice'),
                        adj.get('adjustedPrice'),
                        adj.get('adjustmentType'),
                        adj.get('adjustmentValue'),
                        adj.get('reason'),
                        json.dumps(adj.get('context', {})),
                        adj.get('timestamp'),
                        adj.get('successScore')
                    ))
                
                conn.commit()
                logging.info(f"‚úÖ AI learning data saved: {len(adjustments)} adjustments")
                return JSONResponse({"ok": True, "message": f"Saved {len(adjustments)} adjustments"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving AI learning data: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/ai/learning/load")
async def load_ai_learning_data(request: Request):
    """Carregar dados de aprendizagem AI do PostgreSQL"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                cursor = conn.execute("""
                    SELECT grupo, days, supplier, original_price, adjusted_price,
                           adjustment_type, adjustment_value, reason, context,
                           timestamp, success_score
                    FROM ai_learning_data
                    ORDER BY timestamp DESC
                    LIMIT 1000
                """)
                
                rows = cursor.fetchall()
                adjustments = []
                
                for row in rows:
                    adjustments.append({
                        'grupo': row[0],
                        'days': row[1],
                        'supplier': row[2],
                        'originalPrice': row[3],
                        'adjustedPrice': row[4],
                        'adjustmentType': row[5],
                        'adjustmentValue': row[6],
                        'reason': row[7],
                        'context': json.loads(row[8]) if row[8] else {},
                        'timestamp': row[9],
                        'successScore': row[10]
                    })
                
                logging.info(f"‚úÖ AI learning data loaded: {len(adjustments)} adjustments")
                return JSONResponse({
                    "ok": True,
                    "data": {
                        "adjustments": adjustments,
                        "patterns": {},
                        "suggestions": []
                    }
                })
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error loading AI learning data: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# PRICE SNAPSHOTS - Save automatically when scraping
# ============================================================

@app.post("/api/price-snapshots/save")
async def save_price_snapshots(request: Request):
    """Salvar snapshots de pre√ßos ap√≥s scraping"""
    require_auth(request)
    try:
        data = await request.json()
        snapshots = data.get('snapshots', [])
        
        logging.info(f"üíæ Saving price snapshots: {len(snapshots)} items")
        
        with _db_lock:
            conn = _db_connect()
            try:
                for snapshot in snapshots:
                    conn.execute("""
                        INSERT INTO price_snapshots 
                        (ts, location, grupo, days, supplier, car_name, price, 
                         currency, url, search_params)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        snapshot.get('timestamp', datetime.now().isoformat()),
                        snapshot.get('location'),
                        snapshot.get('grupo'),
                        snapshot.get('days'),
                        snapshot.get('supplier'),
                        snapshot.get('car_name'),
                        snapshot.get('price'),
                        snapshot.get('currency', 'EUR'),
                        snapshot.get('url'),
                        json.dumps(snapshot.get('search_params', {}))
                    ))
                
                conn.commit()
                logging.info(f"‚úÖ Price snapshots saved: {len(snapshots)} items")
                return JSONResponse({"ok": True, "message": f"Saved {len(snapshots)} snapshots"})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving price snapshots: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# AUTOMATED SEARCH HISTORY ENDPOINTS
# ============================================================

@app.post("/api/automated-search/save")
async def save_automated_search_history(request: Request):
    """Save automated search results to PostgreSQL"""
    try:
        data = await request.json()
        
        location = data.get('location', '')
        search_type = data.get('searchType', 'automated')  # 'automated' or 'current'
        prices_data = data.get('prices', {})  # { "B1": { "31": 25.50, "60": 23.00 }, ... }
        dias = data.get('dias', [])
        price_count = data.get('priceCount', 0)
        supplier_data = data.get('supplierData', {})  # allCarsByDay - dados individuais dos suppliers
        pickup_date = data.get('pickupDate', '')  # Date of search (not current date)
        
        logging.info(f"üì• Received save request: Location={location}, Type={search_type}, PickupDate={pickup_date}, Dias={dias}, PriceCount={price_count}, Groups={list(prices_data.keys())}")
        
        if not prices_data:
            logging.warning("‚ö†Ô∏è No prices data provided in save request")
            return JSONResponse({"ok": False, "error": "No prices data provided"}, status_code=400)
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                # Generate month_key based on pickup_date (search date), not current date
                from datetime import datetime
                if pickup_date:
                    try:
                        search_date = datetime.strptime(pickup_date, '%Y-%m-%d')
                        month_key = f"{search_date.year}-{str(search_date.month).zfill(2)}"
                    except:
                        # Fallback to current date if parsing fails
                        now = datetime.now()
                        month_key = f"{now.year}-{str(now.month).zfill(2)}"
                else:
                    # If no pickup_date provided, use current date
                    now = datetime.now()
                    month_key = f"{now.year}-{str(now.month).zfill(2)}"
                
                # Save search
                import json
                prices_json = json.dumps(prices_data)
                dias_json = json.dumps(dias)
                supplier_data_json = json.dumps(supplier_data) if supplier_data else None
                user_email = request.session.get('user_email', 'unknown')
                
                if is_postgres:
                    with conn.cursor() as cur:
                        try:
                            # Try new schema with supplier_data
                            cur.execute("""
                                INSERT INTO automated_search_history 
                                (location, search_type, month_key, prices_data, dias, price_count, user_email, supplier_data)
                                VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s, %s::jsonb)
                                RETURNING id
                            """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email, supplier_data_json))
                            search_id = cur.fetchone()[0]
                        except Exception as e:
                            # Column doesn't exist - rollback and use old schema
                            conn.rollback()
                            logging.warning(f"supplier_data column not found on save, using old schema: {e}")
                            cur.execute("""
                                INSERT INTO automated_search_history 
                                (location, search_type, month_key, prices_data, dias, price_count, user_email)
                                VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s)
                                RETURNING id
                            """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email))
                            search_id = cur.fetchone()[0]
                    conn.commit()
                else:
                    try:
                        cursor = conn.execute("""
                            INSERT INTO automated_search_history 
                            (location, search_type, month_key, prices_data, dias, price_count, user_email, supplier_data)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email, supplier_data_json))
                        search_id = cursor.lastrowid
                    except Exception as e:
                        logging.warning(f"supplier_data column not found on save, using old schema: {e}")
                        cursor = conn.execute("""
                            INSERT INTO automated_search_history 
                            (location, search_type, month_key, prices_data, dias, price_count, user_email)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, (location, search_type, month_key, prices_json, dias_json, price_count, user_email))
                        search_id = cursor.lastrowid
                    conn.commit()
                
                logging.info(f"‚úÖ Automated search saved: ID={search_id}, Type={search_type}, Prices={price_count}, Month={month_key}")
                return JSONResponse({
                    "ok": True, 
                    "message": f"Search saved successfully",
                    "searchId": search_id,
                    "monthKey": month_key
                })
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error saving automated search: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/automated-search/history")
async def get_automated_search_history(request: Request, months: int = 24, location: str = None):
    """Get automated search history for the last N months, optionally filtered by location"""
    try:
        from datetime import datetime, timedelta
        
        # Generate month keys for last N months
        month_keys = []
        now = datetime.now()
        for i in range(months):
            date = datetime(now.year, now.month, 1) - timedelta(days=i*30)
            month_key = f"{date.year}-{str(date.month).zfill(2)}"
            if month_key not in month_keys:
                month_keys.append(month_key)
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                history = {}
                
                # Build WHERE clause for location filter
                location_filter = ""
                if location:
                    location_filter = " AND location = %s" if is_postgres else " AND location = ?"
                
                if is_postgres:
                    with conn.cursor() as cur:
                        # Try to get all searches with supplier_data (new schema)
                        try:
                            query = f"""
                                SELECT id, location, search_type, search_date, month_key, 
                                       prices_data, dias, price_count, supplier_data
                                FROM automated_search_history
                                WHERE month_key = ANY(%s){location_filter}
                                ORDER BY search_date DESC
                            """
                            params = (month_keys, location) if location else (month_keys,)
                            cur.execute(query, params)
                            rows = cur.fetchall()
                            has_supplier_data = True
                        except Exception as e:
                            # Column doesn't exist - rollback transaction and use old schema
                            logging.warning(f"supplier_data column not found, using old schema: {e}")
                            conn.rollback()
                            query = f"""
                                SELECT id, location, search_type, search_date, month_key, 
                                       prices_data, dias, price_count
                                FROM automated_search_history
                                WHERE month_key = ANY(%s){location_filter}
                                ORDER BY search_date DESC
                            """
                            params = (month_keys, location) if location else (month_keys,)
                            cur.execute(query, params)
                            rows = cur.fetchall()
                            has_supplier_data = False
                else:
                    placeholders = ','.join(['?' for _ in month_keys])
                    try:
                        query = f"""
                            SELECT id, location, search_type, search_date, month_key, 
                                   prices_data, dias, price_count, supplier_data
                            FROM automated_search_history
                            WHERE month_key IN ({placeholders}){location_filter}
                            ORDER BY search_date DESC
                        """
                        params = (*month_keys, location) if location else month_keys
                        rows = conn.execute(query, params).fetchall()
                        has_supplier_data = True
                    except Exception as e:
                        logging.warning(f"supplier_data column not found, using old schema: {e}")
                        conn.rollback()
                        query = f"""
                            SELECT id, location, search_type, search_date, month_key, 
                                   prices_data, dias, price_count
                            FROM automated_search_history
                            WHERE month_key IN ({placeholders}){location_filter}
                            ORDER BY search_date DESC
                        """
                        params = (*month_keys, location) if location else month_keys
                        rows = conn.execute(query, params).fetchall()
                        has_supplier_data = False
                
                # Group by month_key and search_type
                import json
                for row in rows:
                    if has_supplier_data:
                        search_id, location, search_type, search_date, month_key, prices_data, dias, price_count, supplier_data = row
                    else:
                        search_id, location, search_type, search_date, month_key, prices_data, dias, price_count = row
                        supplier_data = None
                    
                    if month_key not in history:
                        history[month_key] = {
                            'current': [],
                            'automated': []
                        }
                    
                    entry = {
                        'id': search_id,
                        'location': location,
                        'date': search_date,
                        'timestamp': search_date,  # Same as date for compatibility
                        'search_type': search_type,  # 'automated' from scheduler or 'current' from manual
                        'prices': json.loads(prices_data) if isinstance(prices_data, str) else prices_data,
                        'dias': json.loads(dias) if isinstance(dias, str) else dias,
                        'priceCount': price_count,
                        'supplierData': json.loads(supplier_data) if supplier_data and isinstance(supplier_data, str) else (supplier_data if supplier_data else {})
                    }
                    
                    history[month_key][search_type].append(entry)
                    
                    # NO LIMIT - Keep ALL versions (user requested to save all history)
                
                return JSONResponse({
                    "ok": True,
                    "history": history,
                    "monthKeys": month_keys
                })
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error loading search history: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.delete("/api/automated-search/{search_id}")
async def delete_automated_search(request: Request, search_id: int):
    """Delete a specific search from history"""
    require_auth(request)
    
    try:
        logging.info(f"üóëÔ∏è Attempting to delete search ID: {search_id}")
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                # Verificar se o registro existe antes de deletar
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("SELECT id FROM automated_search_history WHERE id = %s", (search_id,))
                        exists = cur.fetchone()
                        
                        if not exists:
                            logging.warning(f"‚ö†Ô∏è Search ID {search_id} not found in database")
                            return JSONResponse({"ok": False, "error": f"Search ID {search_id} not found"}, status_code=404)
                        
                        cur.execute(
                            "DELETE FROM automated_search_history WHERE id = %s",
                            (search_id,)
                        )
                    conn.commit()
                else:
                    cursor = conn.execute("SELECT id FROM automated_search_history WHERE id = ?", (search_id,))
                    exists = cursor.fetchone()
                    
                    if not exists:
                        logging.warning(f"‚ö†Ô∏è Search ID {search_id} not found in database")
                        return JSONResponse({"ok": False, "error": f"Search ID {search_id} not found"}, status_code=404)
                    
                    conn.execute(
                        "DELETE FROM automated_search_history WHERE id = ?",
                        (search_id,)
                    )
                    conn.commit()
                
                logging.info(f"‚úÖ Successfully deleted search ID: {search_id}")
                return JSONResponse({"ok": True, "message": "Search deleted successfully"})
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error deleting search ID {search_id}: {str(e)}", exc_info=True)
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# ============================================================
# AI PRICE LEARNING ENDPOINTS
# ============================================================

@app.post("/api/ai/save-adjustment")
async def save_ai_adjustment(request: Request):
    """Save AI price adjustment to PostgreSQL for learning"""
    require_auth(request)
    try:
        body = await request.json()
        grupo = body.get('grupo')
        days = body.get('days')
        location = body.get('location')
        original_price = body.get('original_price')
        new_price = body.get('new_price')
        
        if not all([grupo, days, location, new_price]):
            return JSONResponse({"ok": False, "error": "Missing required fields"}, status_code=400)
        
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute(
                            f"""INSERT INTO ai_learning_data 
                                (grupo, days, location, original_price, new_price, timestamp, "user")
                                VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})""",
                            (grupo, days, location, original_price, new_price, datetime.utcnow().isoformat(), 'admin')
                        )
                else:
                    conn.execute(
                        f"""INSERT INTO ai_learning_data 
                            (grupo, days, location, original_price, new_price, timestamp, user)
                            VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})""",
                        (grupo, days, location, original_price, new_price, datetime.utcnow().isoformat(), 'admin')
                    )
                conn.commit()
                logging.info(f"‚úÖ AI adjustment saved: {grupo}/{days}d/{location}: {original_price}‚Ç¨ ‚Üí {new_price}‚Ç¨")
                return JSONResponse({"ok": True})
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error saving AI adjustment: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/api/ai/get-price")
async def get_ai_price(request: Request, grupo: str, days: int, location: str):
    """Get AI-suggested price based on historical adjustments"""
    require_auth(request)
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                placeholder = "%s" if is_postgres else "?"
                
                # Get recent adjustments for this grupo/days/location (last 50)
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute(
                            f"""SELECT new_price, original_price, timestamp 
                                FROM ai_learning_data 
                                WHERE grupo = {placeholder} AND days = {placeholder} AND location = {placeholder}
                                ORDER BY timestamp DESC 
                                LIMIT 50""",
                            (grupo, days, location)
                        )
                        rows = cur.fetchall()
                else:
                    cursor = conn.execute(
                        f"""SELECT new_price, original_price, timestamp 
                            FROM ai_learning_data 
                            WHERE grupo = {placeholder} AND days = {placeholder} AND location = {placeholder}
                            ORDER BY timestamp DESC 
                            LIMIT 50""",
                        (grupo, days, location)
                    )
                    rows = cursor.fetchall()
                
                if not rows:
                    return JSONResponse({"ok": True, "price": None, "confidence": 0})
                
                # Calculate average of recent prices (weighted by recency)
                total_weight = 0
                weighted_sum = 0
                
                for i, row in enumerate(rows):
                    new_price = float(row[0])
                    # More recent = higher weight (exponential decay)
                    weight = pow(0.95, i)  # 95% weight for each step back
                    weighted_sum += new_price * weight
                    total_weight += weight
                
                suggested_price = round(weighted_sum / total_weight, 2) if total_weight > 0 else None
                confidence = min(len(rows) / 10.0, 1.0)  # Confidence grows with data (max at 10+ adjustments)
                
                logging.info(f"ü§ñ AI price for {grupo}/{days}d/{location}: {suggested_price}‚Ç¨ (confidence: {confidence:.0%}, samples: {len(rows)})")
                
                return JSONResponse({
                    "ok": True,
                    "price": suggested_price,
                    "confidence": confidence,
                    "sample_count": len(rows)
                })
            finally:
                conn.close()
    except Exception as e:
        logging.error(f"‚ùå Error getting AI price: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/clean-automated-history")
async def clean_automated_history():
    """Delete all automated search history entries - NO AUTH REQUIRED"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                deleted_count = 0
                
                if is_postgres:
                    with conn.cursor() as cur:
                        cur.execute("SELECT COUNT(*) FROM automated_search_history")
                        deleted_count = cur.fetchone()[0]
                        
                        cur.execute("DELETE FROM automated_search_history")
                    conn.commit()
                else:
                    cursor = conn.execute("SELECT COUNT(*) FROM automated_search_history")
                    deleted_count = cursor.fetchone()[0]
                    
                    conn.execute("DELETE FROM automated_search_history")
                    conn.commit()
                
                logging.info(f"‚úÖ Deleted {deleted_count} entries from automated_search_history")
                return JSONResponse({
                    "ok": True,
                    "message": f"Deleted {deleted_count} history entries",
                    "count": deleted_count
                })
                
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error cleaning history: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.get("/migrate-supplier-data-column")
async def migrate_supplier_data_column():
    """Add supplier_data column to automated_search_history table - NO AUTH REQUIRED"""
    try:
        with _db_lock:
            conn = _db_connect()
            try:
                is_postgres = conn.__class__.__module__ == 'psycopg2.extensions'
                
                if is_postgres:
                    with conn.cursor() as cur:
                        # Check if column exists
                        cur.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name='automated_search_history' 
                            AND column_name='supplier_data'
                        """)
                        exists = cur.fetchone()
                        
                        if not exists:
                            logging.info("Adding supplier_data column to automated_search_history...")
                            cur.execute("""
                                ALTER TABLE automated_search_history 
                                ADD COLUMN supplier_data JSONB
                            """)
                            conn.commit()
                            logging.info("‚úÖ Column supplier_data added successfully")
                            return JSONResponse({
                                "ok": True,
                                "message": "Column supplier_data added to automated_search_history"
                            })
                        else:
                            logging.info("Column supplier_data already exists")
                            return JSONResponse({
                                "ok": True,
                                "message": "Column supplier_data already exists"
                            })
                else:
                    # SQLite - column already in schema
                    return JSONResponse({
                        "ok": True,
                        "message": "SQLite - column in schema"
                    })
                    
            finally:
                conn.close()
                
    except Exception as e:
        logging.error(f"‚ùå Error in migration: {str(e)}")
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
